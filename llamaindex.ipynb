{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "d497e57e",
   "metadata": {},
   "source": [
    "# 1. Setup Asyncio"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "0b7c49a4",
   "metadata": {},
   "outputs": [],
   "source": [
    "import nest_asyncio\n",
    "\n",
    "nest_asyncio.apply()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8d74b235",
   "metadata": {},
   "source": [
    "# 2. Setup the Qdrant vector database"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "afc6a005",
   "metadata": {},
   "source": [
    "Let's now connect to out qdrant database to store the collection of documents we will use for RAG. \n",
    "We will use the `qdrant_client` library to interact with the Qdrant database."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "bc42de2b",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/fc/experiments/rag-project/.venv/lib/python3.9/site-packages/urllib3/__init__.py:35: NotOpenSSLWarning: urllib3 v2 only supports OpenSSL 1.1.1+, currently the 'ssl' module is compiled with 'LibreSSL 2.8.3'. See: https://github.com/urllib3/urllib3/issues/3020\n",
      "  warnings.warn(\n"
     ]
    }
   ],
   "source": [
    "import qdrant_client\n",
    "\n",
    "collection_name = \"chat_with_docs\"\n",
    "\n",
    "client = qdrant_client.QdrantClient(\n",
    "    host=\"localhost\",\n",
    "    port=6333,\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "08dcc9f8",
   "metadata": {},
   "source": [
    "# 3. Read the documents"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "84a323f6",
   "metadata": {},
   "source": [
    "We are now reading the documents using the docling library. For each document in the `docs` folder, we extract images and tables, in addition to its text."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "82745a5d",
   "metadata": {},
   "outputs": [],
   "source": [
    "from llama_index.core import SimpleDirectoryReader\n",
    "\n",
    "input_dir_path = \"./docs\"\n",
    "\n",
    "loader = SimpleDirectoryReader(\n",
    "    input_dir=input_dir_path,\n",
    "    required_exts=[\".pdf\"],\n",
    "    recursive=True\n",
    ")\n",
    "\n",
    "docs = loader.load_data()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "3f43cb9d",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(llama_index.core.schema.Document, 41)"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "type(docs[0]), len(docs)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f2f54557",
   "metadata": {},
   "source": [
    "## 4. A function to index data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "f6682847",
   "metadata": {},
   "outputs": [],
   "source": [
    "from llama_index.vector_stores.qdrant import QdrantVectorStore\n",
    "from llama_index.core import VectorStoreIndex, StorageContext\n",
    "\n",
    "def create_index(documents):\n",
    "\n",
    "    vector_store = QdrantVectorStore(client=client,\n",
    "                                     collection_name=collection_name)\n",
    "    \n",
    "    storage_context = StorageContext.from_defaults(vector_store=vector_store)\n",
    "    \n",
    "    index = VectorStoreIndex.from_documents(documents,\n",
    "                                            storage_context=storage_context)\n",
    "    \n",
    "    return index"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6189b8fe",
   "metadata": {},
   "source": [
    "# 5. Load the embedding model and index data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "4cbe8452",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/fc/experiments/rag-project/.venv/lib/python3.9/site-packages/tqdm/auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    }
   ],
   "source": [
    "from llama_index.embeddings.huggingface import HuggingFaceEmbedding\n",
    "from llama_index.core import Settings\n",
    "\n",
    "embed_model = HuggingFaceEmbedding(model_name=\"BAAI/bge-large-en-v1.5\",\n",
    "                                   trust_remote_code=True)\n",
    "\n",
    "Settings.embed_model = embed_model\n",
    "\n",
    "index = create_index(docs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "cd5373f1",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "llama_index.core.indices.vector_store.base.VectorStoreIndex"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "type(index)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7c43420f",
   "metadata": {},
   "source": [
    "## 6. Load the LLM"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "422711e5",
   "metadata": {},
   "source": [
    "Now, it's time to define the LLM model we will use for querying the index. We are using Ollama as the LLM provider, but you can replace it with any other LLM provider supported by LlamaIndex.\n",
    "\n",
    "Please, make sure to have available the intended model locally. To do so, you can use the pull command. \n",
    "\n",
    "For this task, we will use a _small_ model. In a separate terminal, execute:\n",
    "```bash\n",
    "ollama pull llama3.2:1\n",
    "```\n",
    "and wait for the model to download. Once ready, continue with the next cell!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "0df5efb3",
   "metadata": {},
   "outputs": [],
   "source": [
    "from llama_index.llms.ollama import Ollama\n",
    "\n",
    "llm = Ollama(model=\"llama3.2:1b\", request_timeout=120.0)\n",
    "\n",
    "Settings.llm = llm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "7dd2a7ea",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "llama_index.core.settings._Settings"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "type(Settings)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7b2f8e3f",
   "metadata": {},
   "source": [
    "# 7. Define the prompt template"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "89ac4eb5",
   "metadata": {},
   "outputs": [],
   "source": [
    "from llama_index.core import PromptTemplate\n",
    "\n",
    "template = \"\"\"Context information is below:\n",
    "              ---------------------\n",
    "              {context_str}\n",
    "              ---------------------\n",
    "              Given the context information above I want you to think\n",
    "              step by step to answer the query in a crisp manner,\n",
    "              incase you don't know the answer say 'I don't know!'\n",
    "            \n",
    "              Query: {query_str}\n",
    "        \n",
    "              Answer:\"\"\"\n",
    "\n",
    "qa_prompt_tmpl = PromptTemplate(template)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7c6c1af9",
   "metadata": {},
   "source": [
    "# 8. Reranking"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "eee93a22",
   "metadata": {},
   "source": [
    "Here, we use a cross-encoder to re-rank the document chunks. Also, we limit the output to the top 3 most relevant chunks based on the model’s scoring."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "a04ba8a9",
   "metadata": {},
   "outputs": [],
   "source": [
    "from llama_index.core.postprocessor import SentenceTransformerRerank\n",
    "\n",
    "rerank = SentenceTransformerRerank(\n",
    "    model=\"cross-encoder/ms-marco-MiniLM-L-2-v2\", \n",
    "    top_n=3\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "0739096d",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "SentenceTransformerRerank(callback_manager=<llama_index.core.callbacks.base.CallbackManager object at 0x391c33ac0>, model='cross-encoder/ms-marco-MiniLM-L-2-v2', top_n=3, device='mps', keep_retrieval_score=False, trust_remote_code=False)"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "rerank"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bb166d08",
   "metadata": {},
   "source": [
    "# 9. Query the document"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "ad1dcfe2",
   "metadata": {},
   "outputs": [],
   "source": [
    "query_engine = index.as_query_engine(similarity_top_k=10,\n",
    "                                     node_postprocessors=[rerank])\n",
    "\n",
    "query_engine.update_prompts(\n",
    "    {\"response_synthesizer:text_qa_template\": qa_prompt_tmpl}\n",
    ")\n",
    "\n",
    "# response = query_engine.query(\"What exactly is DSPy?\")\n",
    "response = query_engine.query(\"How is DSPy pronounced?\")\n",
    "# response = query_engine.query(\"What is the github repo for docling?\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c7a639b8",
   "metadata": {},
   "source": [
    "Answer:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "2d1a01d1",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/markdown": [
       "DSPy is pronounced \"dee-ess-pie\"."
      ],
      "text/plain": [
       "<IPython.core.display.Markdown object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "from IPython.display import Markdown, display\n",
    "\n",
    "display(Markdown(str(response)))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dfb4482a",
   "metadata": {},
   "source": [
    "Interestingly, in the metadata field of the response, you can find the document from which the answer was extracted. This is useful for tracking the source of the information provided by the model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "8275c694",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'ca39e45f-b95d-49ec-b226-37507d7c4b95': {'page_label': '4',\n",
       "  'file_name': 'dspy.pdf',\n",
       "  'file_path': '/Users/fc/experiments/rag-project/docs/dspy.pdf',\n",
       "  'file_type': 'application/pdf',\n",
       "  'file_size': 460814,\n",
       "  'creation_date': '2025-06-23',\n",
       "  'last_modified_date': '2024-11-02'},\n",
       " '30beb7a2-b5eb-490b-9ed7-a86219f294b0': {'page_label': '2',\n",
       "  'file_name': 'dspy.pdf',\n",
       "  'file_path': '/Users/fc/experiments/rag-project/docs/dspy.pdf',\n",
       "  'file_type': 'application/pdf',\n",
       "  'file_size': 460814,\n",
       "  'creation_date': '2025-06-23',\n",
       "  'last_modified_date': '2024-11-02'},\n",
       " '061b7cb5-52ef-44da-984f-10c3fa2aa94b': {'page_label': '27',\n",
       "  'file_name': 'dspy.pdf',\n",
       "  'file_path': '/Users/fc/experiments/rag-project/docs/dspy.pdf',\n",
       "  'file_type': 'application/pdf',\n",
       "  'file_size': 460814,\n",
       "  'creation_date': '2025-06-23',\n",
       "  'last_modified_date': '2024-11-02'}}"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "response.metadata"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f868b30e",
   "metadata": {},
   "source": [
    "## Bonus: Visualize relevant text in sources"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "3dfb2f1d",
   "metadata": {},
   "outputs": [],
   "source": [
    "from IPython.display import Markdown, display\n",
    "import re\n",
    "\n",
    "def highlight(text, query, color):\n",
    "        # Case-insensitive highlight\n",
    "        pattern = re.compile(re.escape(query), re.IGNORECASE)\n",
    "        return pattern.sub(f\"<mark style='background-color:{color};'>{query}</mark>\", text)\n",
    "\n",
    "def display_sources_with_highlight(response, docs, query, highlight_color=\"#ffff00\"):\n",
    "    \"\"\"\n",
    "    Display source documents for the response, highlighting the query in the text.\n",
    "    \"\"\"\n",
    "    \n",
    "    for source in response.metadata.values():\n",
    "        source_document = source.get(\"file_name\")\n",
    "        source_page = source.get(\"page_label\")\n",
    "        if source_page:\n",
    "            # We need also to filter per document file_name\n",
    "            d = next((doc for doc in docs if doc.metadata.get(\"file_name\") == source_document and doc.metadata.get(\"page_label\") == source_page), None)\n",
    "            if d:\n",
    "                highlighted = highlight(d.text, query, highlight_color)\n",
    "                display(Markdown(f\"### Source Document (page_label: {source_page})\\n\\n{highlighted}\"))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "693169c2",
   "metadata": {},
   "source": [
    "Usage:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "d5622e39",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/markdown": [
       "### Source Document (page_label: 4)\n",
       "\n",
       "Preprint\n",
       "3.1 N ATURAL LANGUAGE SIGNATURES CAN ABSTRACT PROMPTING & FINETUNING\n",
       "Instead of free-form string prompts, DSPy programs use natural language signatures to assign work\n",
       "to the LM. A DSPy signature isnatural-language typed declaration of a function: a short declarative\n",
       "spec that tells DSPy what a text transformation needs to do (e.g., “consume questions and return\n",
       "answers”), rather than how a specific LM should be prompted to implement that behavior. More\n",
       "formally, a DSPy signature is a tuple of input fields and output fields (and an optional instruction).\n",
       "A field consists offield name and optional metadata.4 In typical usage, the roles of fields are inferred\n",
       "by DSPy as a function of field names. For instance, the DSPy compiler will use in-context learning\n",
       "to interpret questiondifferently from answer and will iteratively refine its usage of these fields.\n",
       "Signatures offer two benefits over prompts: they can be compiled into self-improving and pipeline-\n",
       "adaptive prompts or finetunes. This is primarily done by bootstrapping (Sec 4) useful demonstrating\n",
       "examples for each signature. Additionally, they handle structured formatting and parsing logic to\n",
       "reduce (or, ideally, avoid) brittle string manipulation in user programs.\n",
       "In practice, DSPy signatures can be expressed with a shorthand notation likequestion -> answer,\n",
       "so that line 1 in the following is a complete DSPy program for a basic question-answering system\n",
       "(with line 2 illustrating usage and line 3 the response when GPT-3.5 is the LM):\n",
       "1 qa = dspy.Predict(\"question -> answer\")\n",
       "2 qa(question=\"Where is Guaran ´ı spoken?\")\n",
       "3 # Out: Prediction(answer=’Guaran ´ı is spoken mainly in South America.’)\n",
       "In the shorthand notation, each field’s name indicates the semantic role that the input (or output)\n",
       "field plays in the transformation. DSPy will parse this notation and expand the field names into\n",
       "meaningful instructions for the LM, so that english document -> french translation would\n",
       "prompt for English to French translation. When needed, DSPy offers more advanced programming\n",
       "interfaces for expressing more explicit constraints on signatures (Appendix A).\n",
       "3.2 P ARAMETERIZED & TEMPLATED MODULES CAN ABSTRACT PROMPTING TECHNIQUES\n",
       "Akin to type signatures in programming languages, DSPy signatures simply define an interface and\n",
       "provide type-like hints on the expected behavior. To use a signature, we must declare amodule with\n",
       "that signature, like we instantiated a Predict module above. A module declaration like this returns\n",
       "a function having that signature.\n",
       "The Predict Module The core module for working with signatures in DSPy isPredict(simplified\n",
       "pseudocode in Appendix D.1). Internally, Predict stores the supplied signature, an optional LM to\n",
       "use (initially None, but otherwise overrides the default LM for this module), and a list of demon-\n",
       "strations for prompting (initially empty). Like layers in PyTorch, the instantiated module behaves as\n",
       "a callable function: it takes in keyword arguments corresponding to the signature input fields (e.g.,\n",
       "question), formats a prompt to implement the signature and includes the appropriate demonstra-\n",
       "tions, calls the LM, and parses the output fields. When Predict detects it’s being used in compile\n",
       "mode, it will also internally track input/output traces to assist the teleprompter at bootstrapping the\n",
       "demonstrations.\n",
       "Other Built-in ModulesDSPy modules translate prompting techniques into modular functions that\n",
       "support any signature, contrasting with the standard approach of prompting LMs with task-specific\n",
       "details (e.g., hand-written few-shot examples). To this end, DSPy includes a number of more sophis-\n",
       "ticated modules like ChainOfThought, ProgramOfThought, MultiChainComparison, and ReAct.5\n",
       "These can all be used interchangeably to implement a DSPy signature. For instance, simply chang-\n",
       "4String descriptions of the task and the fields are also optional and usually omitted. Fields can carry optional\n",
       "field prefix and description. By default, fields are assumed to hold free-form strings; we are actively exploring\n",
       "optional data type as a way to specify constraints on valid values (e.g.,boolor int) and more gracefully handle\n",
       "formatting and parsing logic, though this feature is not core to DSPy at the time of writing.\n",
       "5These modules generalize prompting techniques from the literature, respectively, by Wei et al. (2022),\n",
       "Chen et al. (2022), Yoran et al. (2023), and Yao et al. (2022) and, in doing so, generalize the ideas on zero-shot\n",
       "prompting and rationale self-generation from Kojima et al. (2022), Zelikman et al. (2022), Zhang et al. (2022),\n",
       "and Huang et al. (2022) to parameterized modules that can bootstrap arbitrary multi-stage pipelines.\n",
       "4"
      ],
      "text/plain": [
       "<IPython.core.display.Markdown object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/markdown": [
       "### Source Document (page_label: 2)\n",
       "\n",
       "Preprint\n",
       "calls in existing LM pipelines and in popular developer frameworks are generally implemented using\n",
       "hard-coded ‘prompt templates’, that is, long strings of instructions and demonstrations that are hand\n",
       "crafted through manual trial and error. We argue that this approach, while pervasive, can be brittle\n",
       "and unscalable—conceptually akin to hand-tuning the weights for a classifier. A given string prompt\n",
       "might not generalize to different pipelines or across different LMs, data domains, or even inputs.\n",
       "Toward a more systematic approach to designing AI pipelines, we introduce theDSPy programming\n",
       "model.1 DSPy pushes building new LM pipelines away from manipulating free-form strings and\n",
       "closer to programming (composing modular operators to build text transformation graphs) where a\n",
       "compiler automatically generates optimized LM invocation strategies and prompts from a program.\n",
       "We draw inspiration from the consensus that emerged around neural network abstractions (Bergstra\n",
       "et al., 2013), where (1) many general-purpose layers can be modularly composed in any complex\n",
       "architecture and (2) the model weights can be trained using optimizers instead of being hand-tuned.\n",
       "To this end, we propose the DSPy programming model(Sec 3). We first translate string-based\n",
       "prompting techniques, including complex and task-dependent ones like Chain of Thought (Wei et al.,\n",
       "2022) and ReAct (Yao et al., 2022), into declarative modules that carrynatural-language typed sig-\n",
       "natures. DSPy modules are task-adaptive components—akin to neural network layers—that abstract\n",
       "any particular text transformation, like answering a question or summarizing a paper. We then pa-\n",
       "rameterize each module so that it can learn its desired behavior by iteratively bootstrapping useful\n",
       "demonstrations within the pipeline. Inspired directly by PyTorch abstractions (Paszke et al., 2019),\n",
       "DSPy modules are used via expressive define-by-run computational graphs. Pipelines are expressed\n",
       "by (1) declaring the modules needed and (2) using these modules in any logical control flow (e.g.,\n",
       "ifstatements, for loops, exceptions, etc.) to logically connect the modules.\n",
       "We then develop theDSPy compiler(Sec 4), which optimizes any DSPy program to improve quality\n",
       "or cost. The compiler inputs are the program, a few training inputs with optional labels, and a valida-\n",
       "tion metric. The compiler simulates versions of the program on the inputs and bootstraps example\n",
       "traces of each module for self-improvement, using them to construct effective few-shot prompts\n",
       "or finetuning small LMs for steps of the pipeline. Optimization in DSPy is highly modular: it is\n",
       "conducted by teleprompters,2 which are general-purpose optimization strategies that determine how\n",
       "the modules should learn from data. In this way, the compiler automatically maps the declarative\n",
       "modules to high-quality compositions of prompting, finetuning, reasoning, and augmentation.\n",
       "Programming models like DSPy could be assessed along many dimensions, but we focus on the role\n",
       "of expert-crafted prompts in shaping system performance. We are seeking to reduce or even remove\n",
       "their role through DSPy modules (e.g., versions of popular techniques like Chain of Thought) and\n",
       "teleprompters. We report on two expansive case studies: math word problems (GMS8K; Cobbe et al.\n",
       "2021) and multi-hop question answering (HotPotQA; Yang et al. 2018) with explorations of chain\n",
       "of thought, multi-chain reflection, multi-hop retrieval, retrieval-augmented question answering, and\n",
       "agent loops. Our evaluations use a number of different compiling strategies effectively and show\n",
       "that straightforward DSPy programs outperform systems using hand-crafted prompts, while also\n",
       "allowing our programs to use much smaller and hence more efficient LMs effectively.\n",
       "Overall, this work proposes the first programming model that translates prompting techniques into\n",
       "parameterized declarative modules and introduces an effective compiler with general optimiza-\n",
       "tion strategies (teleprompters) to optimize arbitrary pipelines of these modules. Our main contri-\n",
       "butions are empirical and algorithmic: with DSPy, we have found that we can implement very\n",
       "short programs that can bootstrap self-improving multi-stage NLP systems using LMs as small as\n",
       "llama2-13b-chat and T5-Large (770M parameters). Without hand-crafted prompts and within\n",
       "minutes to tens of minutes of compiling, compositions of DSPy modules can raise the quality of\n",
       "simple programs from 33% to 82% (Sec 6) and from 32% to 46% (Sec 7) for GPT-3.5 and, simi-\n",
       "larly, from 9% to 47% (Sec 6) and from 22% to 41% (Sec 7) for llama2-13b-chat.\n",
       "1DSPy is pronounced <mark style='background-color:#ffff00;'>dee-ess-pie</mark>. It’s the second iteration of our earlier Demonstrate–Search–Predict\n",
       "framework (DSP; Khattab et al. 2022). This paper introduces the key concepts in DSPy. For more extensive and\n",
       "up-to-date documentation of the framework, we refer readers to https://github.com/stanfordnlp/dspy.\n",
       "2We derive the name tele-prompters from the notion of abstracting and automating the task of prompting,\n",
       "in particular, such that it happens at a distance, without manual intervention.\n",
       "2"
      ],
      "text/plain": [
       "<IPython.core.display.Markdown object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/markdown": [
       "### Source Document (page_label: 27)\n",
       "\n",
       "Preprint\n",
       "D M ODULES\n",
       "D.1 P REDICT\n",
       "1 class Predict(dspy.Module):\n",
       "2 def __init__(self, signature, **config):\n",
       "3 self.signature = dspy.Signature(signature)\n",
       "4 self.config = config\n",
       "5\n",
       "6 # Module Parameters.\n",
       "7 self.lm = dspy.ParameterLM(None) # use the default LM\n",
       "8 self.demonstrations = dspy.ParameterDemonstrations([])\n",
       "9\n",
       "10 def forward(self, **kwargs):\n",
       "11 lm = get_the_right_lm(self.lm, kwargs)\n",
       "12 signature = get_the_right_signature(self.signature, kwargs)\n",
       "13 demonstrations = get_the_right_demonstrations(self.demonstrations, kwargs)\n",
       "14\n",
       "15 prompt = signature(demos=self.demos, **kwargs)\n",
       "16 completions = lm.generate(prompt, **self.config)\n",
       "17 prediction = Prediction.from_completions(completions, signature=signature)\n",
       "18\n",
       "19 if dsp.settings.compiling is not None:\n",
       "20 trace = dict(predictor=self, inputs=kwargs, outputs=prediction)\n",
       "21 dspy.settings.traces.append(trace)\n",
       "22\n",
       "23 return prediction\n",
       "D.2 C HAIN OF THOUGHT\n",
       "1 class ChainOfThought(dspy.Module):\n",
       "2 def __init__(self, signature):\n",
       "3\n",
       "4 # Modify signature from ‘*inputs -> *outputs‘ to ‘*inputs -> rationale, *outputs‘.\n",
       "5 rationale_field = dspy.OutputField(prefix=\"Reasoning: Let’s think step by step.\")\n",
       "6 signature = dspy.Signature(signature).prepend_output_field(rationale_field)\n",
       "7\n",
       "8 # Declare a sub-module with the modified signature.\n",
       "9 self.predict = dspy.Predict(self.signature)\n",
       "10\n",
       "11 def forward(self, **kwargs):\n",
       "12 # Just forward the inputs to the sub-module.\n",
       "13 return self.predict(**kwargs)\n",
       "27"
      ],
      "text/plain": [
       "<IPython.core.display.Markdown object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "word = \"dee-ess-pie\"\n",
    "display_sources_with_highlight(response, docs, word)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
