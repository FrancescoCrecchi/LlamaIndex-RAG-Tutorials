{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "d497e57e",
   "metadata": {},
   "source": [
    "# 1. Setup Asyncio"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "0b7c49a4",
   "metadata": {},
   "outputs": [],
   "source": [
    "import nest_asyncio\n",
    "\n",
    "nest_asyncio.apply()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8d74b235",
   "metadata": {},
   "source": [
    "# 2. Setup the Qdrant vector database"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8bc09349",
   "metadata": {},
   "source": [
    "Let's now connect to out qdrant database to store the collection of documents we will use for RAG. \n",
    "We will use the `qdrant_client` library to interact with the Qdrant database."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "bc42de2b",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/fc/experiments/rag-project/.venv/lib/python3.9/site-packages/urllib3/__init__.py:35: NotOpenSSLWarning: urllib3 v2 only supports OpenSSL 1.1.1+, currently the 'ssl' module is compiled with 'LibreSSL 2.8.3'. See: https://github.com/urllib3/urllib3/issues/3020\n",
      "  warnings.warn(\n"
     ]
    }
   ],
   "source": [
    "import qdrant_client\n",
    "\n",
    "collection_name = \"chat_with_docs_chonkie\"\n",
    "\n",
    "client = qdrant_client.QdrantClient(\n",
    "    host=\"localhost\",\n",
    "    port=6333,\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "08dcc9f8",
   "metadata": {},
   "source": [
    "# 3. Read the documents"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fc8f851b",
   "metadata": {},
   "source": [
    "We are now reading the documents using the LlamaIndex library. We are using the `SimpleDirectoryReader` to do so:\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "82745a5d",
   "metadata": {},
   "outputs": [],
   "source": [
    "from llama_index.core import SimpleDirectoryReader\n",
    "\n",
    "input_dir_path = \"./docs\"\n",
    "\n",
    "loader = SimpleDirectoryReader(\n",
    "    input_dir=input_dir_path,\n",
    "    required_exts=[\".pdf\"],\n",
    "    recursive=True\n",
    ")\n",
    "\n",
    "docs = loader.load_data()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "aacc0201",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'id_': 'cc8f750d-e4af-4f98-8c11-18d0f1bafe03',\n",
       " 'embedding': None,\n",
       " 'metadata': {'page_label': '1',\n",
       "  'file_name': 'docling.pdf',\n",
       "  'file_path': '/Users/fc/experiments/rag-project/docs/docling.pdf',\n",
       "  'file_type': 'application/pdf',\n",
       "  'file_size': 5566575,\n",
       "  'creation_date': '2025-06-13',\n",
       "  'last_modified_date': '2025-06-13'},\n",
       " 'excluded_embed_metadata_keys': ['file_name',\n",
       "  'file_type',\n",
       "  'file_size',\n",
       "  'creation_date',\n",
       "  'last_modified_date',\n",
       "  'last_accessed_date'],\n",
       " 'excluded_llm_metadata_keys': ['file_name',\n",
       "  'file_type',\n",
       "  'file_size',\n",
       "  'creation_date',\n",
       "  'last_modified_date',\n",
       "  'last_accessed_date'],\n",
       " 'relationships': {},\n",
       " 'metadata_template': '{key}: {value}',\n",
       " 'metadata_separator': '\\n',\n",
       " 'text_resource': {'embeddings': None,\n",
       "  'text': 'Docling Technical Report\\nVersion 1.0\\nChristoph Auer Maksym Lysak Ahmed Nassar Michele Dolfi Nikolaos Livathinos\\nPanos Vagenas Cesar Berrospi Ramis Matteo Omenetti Fabian Lindlbauer\\nKasper Dinkla Lokesh Mishra Yusik Kim Shubham Gupta Rafael Teixeira de Lima\\nValery Weber Lucas Morin Ingmar Meijer Viktor Kuropiatnyk Peter W. J. Staar\\nAI4K Group, IBM Research\\nR¨uschlikon, Switzerland\\nAbstract\\nThis technical report introduces Docling, an easy to use, self-contained, MIT-\\nlicensed open-source package for PDF document conversion. It is powered by\\nstate-of-the-art specialized AI models for layout analysis (DocLayNet) and table\\nstructure recognition (TableFormer), and runs efficiently on commodity hardware\\nin a small resource budget. The code interface allows for easy extensibility and\\naddition of new features and models.\\n1 Introduction\\nConverting PDF documents back into a machine-processable format has been a major challenge\\nfor decades due to their huge variability in formats, weak standardization and printing-optimized\\ncharacteristic, which discards most structural features and metadata. With the advent of LLMs\\nand popular application patterns such as retrieval-augmented generation (RAG), leveraging the rich\\ncontent embedded in PDFs has become ever more relevant. In the past decade, several powerful\\ndocument understanding solutions have emerged on the market, most of which are commercial soft-\\nware, cloud offerings [3] and most recently, multi-modal vision-language models. As of today, only\\na handful of open-source tools cover PDF conversion, leaving a significant feature and quality gap\\nto proprietary solutions.\\nWith Docling, we open-source a very capable and efficient document conversion tool which builds\\non the powerful, specialized AI models and datasets for layout analysis and table structure recog-\\nnition we developed and presented in the recent past [12, 13, 9]. Docling is designed as a simple,\\nself-contained python library with permissive license, running entirely locally on commodity hard-\\nware. Its code architecture allows for easy extensibility and addition of new features and models.\\nDocling Technical Report\\n1\\narXiv:2408.09869v5  [cs.CL]  9 Dec 2024',\n",
       "  'path': None,\n",
       "  'url': None,\n",
       "  'mimetype': None},\n",
       " 'image_resource': None,\n",
       " 'audio_resource': None,\n",
       " 'video_resource': None,\n",
       " 'text_template': '{metadata_str}\\n\\n{content}',\n",
       " 'class_name': 'Document',\n",
       " 'text': 'Docling Technical Report\\nVersion 1.0\\nChristoph Auer Maksym Lysak Ahmed Nassar Michele Dolfi Nikolaos Livathinos\\nPanos Vagenas Cesar Berrospi Ramis Matteo Omenetti Fabian Lindlbauer\\nKasper Dinkla Lokesh Mishra Yusik Kim Shubham Gupta Rafael Teixeira de Lima\\nValery Weber Lucas Morin Ingmar Meijer Viktor Kuropiatnyk Peter W. J. Staar\\nAI4K Group, IBM Research\\nR¨uschlikon, Switzerland\\nAbstract\\nThis technical report introduces Docling, an easy to use, self-contained, MIT-\\nlicensed open-source package for PDF document conversion. It is powered by\\nstate-of-the-art specialized AI models for layout analysis (DocLayNet) and table\\nstructure recognition (TableFormer), and runs efficiently on commodity hardware\\nin a small resource budget. The code interface allows for easy extensibility and\\naddition of new features and models.\\n1 Introduction\\nConverting PDF documents back into a machine-processable format has been a major challenge\\nfor decades due to their huge variability in formats, weak standardization and printing-optimized\\ncharacteristic, which discards most structural features and metadata. With the advent of LLMs\\nand popular application patterns such as retrieval-augmented generation (RAG), leveraging the rich\\ncontent embedded in PDFs has become ever more relevant. In the past decade, several powerful\\ndocument understanding solutions have emerged on the market, most of which are commercial soft-\\nware, cloud offerings [3] and most recently, multi-modal vision-language models. As of today, only\\na handful of open-source tools cover PDF conversion, leaving a significant feature and quality gap\\nto proprietary solutions.\\nWith Docling, we open-source a very capable and efficient document conversion tool which builds\\non the powerful, specialized AI models and datasets for layout analysis and table structure recog-\\nnition we developed and presented in the recent past [12, 13, 9]. Docling is designed as a simple,\\nself-contained python library with permissive license, running entirely locally on commodity hard-\\nware. Its code architecture allows for easy extensibility and addition of new features and models.\\nDocling Technical Report\\n1\\narXiv:2408.09869v5  [cs.CL]  9 Dec 2024'}"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "docs[0].dict()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f9028b52",
   "metadata": {},
   "source": [
    "## 4. Use Chonkie to chunk the documents"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "98559f22",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/fc/experiments/rag-project/.venv/lib/python3.9/site-packages/tqdm/auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    }
   ],
   "source": [
    "from chonkie import SemanticChunker\n",
    "from llama_index.core.schema import Document\n",
    "from llama_index.core import Settings\n",
    "from llama_index.embeddings.huggingface import HuggingFaceEmbedding\n",
    "\n",
    "\n",
    "semantic_chunker = SemanticChunker(\n",
    "    embedding_model=\"BAAI/bge-large-en-v1.5\",\n",
    "    threshold=0.5,\n",
    "    chunk_size=512,\n",
    "    min_sentences=1\n",
    ")\n",
    "\n",
    "embed_model = HuggingFaceEmbedding(model_name=\"BAAI/bge-large-en-v1.5\",\n",
    "                                   trust_remote_code=True)\n",
    "\n",
    "Settings.embed_model = embed_model\n",
    "\n",
    "all_chunks = []\n",
    "for doc in docs:\n",
    "    chunks = semantic_chunker.chunk(doc.text)\n",
    "    for chunk in chunks:\n",
    "        # Use LlamaIndex's embedding model to embed the chunk text\n",
    "        chunk_embedding = Settings.embed_model.get_text_embedding(chunk.text)\n",
    "        all_chunks.append(\n",
    "            Document(\n",
    "                text=chunk.text,\n",
    "                metadata=doc.metadata,\n",
    "                embedding=chunk_embedding\n",
    "            )\n",
    "        )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "a53758e8",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "107"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(all_chunks)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "e220cfd7",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "llama_index.core.schema.Document"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "type(all_chunks[0])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b993507d",
   "metadata": {
    "vscode": {
     "languageId": "markdown"
    }
   },
   "source": [
    "Chunks are llama_index `Document`s with their own metadata and embeddings. Those are the actual documents we will index in Qdrant. "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6189b8fe",
   "metadata": {},
   "source": [
    "## 5. Load the embedding model and index data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "398d3707",
   "metadata": {},
   "outputs": [],
   "source": [
    "from llama_index.vector_stores.qdrant import QdrantVectorStore\n",
    "from llama_index.core import VectorStoreIndex, StorageContext\n",
    "\n",
    "def create_index(documents):\n",
    "\n",
    "    vector_store = QdrantVectorStore(client=client,\n",
    "                                     collection_name=collection_name)\n",
    "    \n",
    "    storage_context = StorageContext.from_defaults(vector_store=vector_store)\n",
    "    \n",
    "    index = VectorStoreIndex.from_documents(documents,\n",
    "                                            storage_context=storage_context)\n",
    "    \n",
    "    return index"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "4cbe8452",
   "metadata": {},
   "outputs": [],
   "source": [
    "from llama_index.core import Settings\n",
    "\n",
    "index = create_index(all_chunks)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "cd5373f1",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "llama_index.core.indices.vector_store.base.VectorStoreIndex"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "type(index)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7c43420f",
   "metadata": {},
   "source": [
    "## 6. Load the LLM"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1d425538",
   "metadata": {},
   "source": [
    "Now, it's time to define the LLM model we will use for querying the index. We are using Ollama as the LLM provider, but you can replace it with any other LLM provider supported by LlamaIndex.\n",
    "\n",
    "Please, make sure to have available the intended model locally. To do so, you can use the pull command. \n",
    "\n",
    "For this task, we will use a _small_ model. In a separate terminal, execute:\n",
    "```bash\n",
    "ollama pull llama3.2:1\n",
    "```\n",
    "and wait for the model to download. Once ready, continue with the next cell!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "0df5efb3",
   "metadata": {},
   "outputs": [],
   "source": [
    "from llama_index.llms.ollama import Ollama\n",
    "from llama_index.core import Settings\n",
    "\n",
    "\n",
    "llm = Ollama(model=\"llama3.2:1b\", request_timeout=120.0)\n",
    "\n",
    "Settings.llm = llm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "7dd2a7ea",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(llama_index.core.settings._Settings,\n",
       " Ollama(callback_manager=<llama_index.core.callbacks.base.CallbackManager object at 0x1146f40d0>, system_prompt=None, messages_to_prompt=<function messages_to_prompt at 0x10ca7d8b0>, completion_to_prompt=<function default_completion_to_prompt at 0x10d0beca0>, output_parser=None, pydantic_program_mode=<PydanticProgramMode.DEFAULT: 'default'>, query_wrapper_prompt=None, base_url='http://localhost:11434', model='llama3.2:1b', temperature=None, context_window=-1, request_timeout=120.0, prompt_key='prompt', json_mode=False, additional_kwargs={}, is_function_calling_model=True, keep_alive=None, thinking=None),\n",
       " HuggingFaceEmbedding(model_name='BAAI/bge-large-en-v1.5', embed_batch_size=10, callback_manager=<llama_index.core.callbacks.base.CallbackManager object at 0x1146f40d0>, num_workers=None, embeddings_cache=None, max_length=512, normalize=True, query_instruction=None, text_instruction=None, cache_folder=None, show_progress_bar=False))"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "type(Settings), Settings.llm, Settings.embed_model"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7b2f8e3f",
   "metadata": {},
   "source": [
    "## 7. Define the prompt template"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "89ac4eb5",
   "metadata": {},
   "outputs": [],
   "source": [
    "from llama_index.core import PromptTemplate\n",
    "\n",
    "template = \"\"\"Context information is below:\n",
    "              ---------------------\n",
    "              {context_str}\n",
    "              ---------------------\n",
    "              Given the context information above I want you to think\n",
    "              step by step to answer the query in a crisp manner,\n",
    "              incase you don't know the answer say 'I don't know!'\n",
    "            \n",
    "              Query: {query_str}\n",
    "        \n",
    "              Answer:\"\"\"\n",
    "\n",
    "qa_prompt_tmpl = PromptTemplate(template)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7c6c1af9",
   "metadata": {},
   "source": [
    "## 8. Reranking"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "eee93a22",
   "metadata": {},
   "source": [
    "Here, we use a cross-encoder to re-rank the document chunks. Also, we limit the output to the top 3 most relevant chunks based on the model’s scoring."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "a04ba8a9",
   "metadata": {},
   "outputs": [],
   "source": [
    "from llama_index.core.postprocessor import SentenceTransformerRerank\n",
    "\n",
    "rerank = SentenceTransformerRerank(\n",
    "    model=\"cross-encoder/ms-marco-MiniLM-L-2-v2\", \n",
    "    top_n=3\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "0739096d",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "SentenceTransformerRerank(callback_manager=<llama_index.core.callbacks.base.CallbackManager object at 0x42b9068b0>, model='cross-encoder/ms-marco-MiniLM-L-2-v2', top_n=3, device='mps', keep_retrieval_score=False, trust_remote_code=False)"
      ]
     },
     "execution_count": 23,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "rerank"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bb166d08",
   "metadata": {},
   "source": [
    "## 9. Query the index"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2647f0cb",
   "metadata": {},
   "source": [
    "It's time to query the index! Let's ask a question about the content of the documents we indexed."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "ad1dcfe2",
   "metadata": {},
   "outputs": [],
   "source": [
    "query_engine = index.as_query_engine(similarity_top_k=10,\n",
    "                                     node_postprocessors=[rerank])\n",
    "\n",
    "query_engine.update_prompts(\n",
    "    {\"response_synthesizer:text_qa_template\": qa_prompt_tmpl}\n",
    ")\n",
    "\n",
    "# response = query_engine.query(\"What exactly is DSPy?\")\n",
    "response = query_engine.query(\"How is DSPy pronounced?\")\n",
    "# response = query_engine.query(\"What is the github repo for docling?\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "11502764",
   "metadata": {},
   "source": [
    "Answer:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "2d1a01d1",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/markdown": [
       "DSPy is pronounced \"dee-ess-pie\". It's the second iteration of our earlier Demonstrate–Search–Predict (DSP) framework, introduced in a paper by Khattab et al. 2022."
      ],
      "text/plain": [
       "<IPython.core.display.Markdown object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "from IPython.display import Markdown, display\n",
    "\n",
    "display(Markdown(str(response)))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7be8a686",
   "metadata": {},
   "source": [
    "Interestingly, in the metadata field of the response, you can find the document from which the answer was extracted. This is useful for tracking the source of the information provided by the model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "8275c694",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'9345dd84-9b62-44e6-99fd-57d475ce36c3': {'page_label': '8',\n",
       "  'file_name': 'dspy.pdf',\n",
       "  'file_path': '/Users/fc/experiments/rag-project/docs/dspy.pdf',\n",
       "  'file_type': 'application/pdf',\n",
       "  'file_size': 460814,\n",
       "  'creation_date': '2025-06-23',\n",
       "  'last_modified_date': '2024-11-02'},\n",
       " 'e8b407d8-2938-4340-ad4d-437440b62032': {'page_label': '2',\n",
       "  'file_name': 'dspy.pdf',\n",
       "  'file_path': '/Users/fc/experiments/rag-project/docs/dspy.pdf',\n",
       "  'file_type': 'application/pdf',\n",
       "  'file_size': 460814,\n",
       "  'creation_date': '2025-06-23',\n",
       "  'last_modified_date': '2024-11-02'},\n",
       " '756ca569-188b-4587-8aab-efdf68c23510': {'page_label': '6',\n",
       "  'file_name': 'dspy.pdf',\n",
       "  'file_path': '/Users/fc/experiments/rag-project/docs/dspy.pdf',\n",
       "  'file_type': 'application/pdf',\n",
       "  'file_size': 460814,\n",
       "  'creation_date': '2025-06-23',\n",
       "  'last_modified_date': '2024-11-02'}}"
      ]
     },
     "execution_count": 26,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "response.metadata"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "41bfbad3",
   "metadata": {},
   "source": [
    "## Bonus: Visualize relevant text in sources"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "479a1b99",
   "metadata": {},
   "outputs": [],
   "source": [
    "from IPython.display import Markdown, display\n",
    "import re\n",
    "\n",
    "def highlight(text, query, color):\n",
    "        # Case-insensitive highlight\n",
    "        pattern = re.compile(re.escape(query), re.IGNORECASE)\n",
    "        return pattern.sub(f\"<mark style='background-color:{color};'>{query}</mark>\", text)\n",
    "\n",
    "def display_sources_with_highlight(response, docs, query, highlight_color=\"#ffff00\"):\n",
    "    \"\"\"\n",
    "    Display source documents for the response, highlighting the query in the text.\n",
    "    \"\"\"\n",
    "    \n",
    "    for source in response.metadata.values():\n",
    "        source_document = source.get(\"file_name\")\n",
    "        source_page = source.get(\"page_label\")\n",
    "        if source_page:\n",
    "            # We need also to filter per document file_name\n",
    "            d = next((doc for doc in docs if doc.metadata.get(\"file_name\") == source_document and doc.metadata.get(\"page_label\") == source_page), None)\n",
    "            if d:\n",
    "                highlighted = highlight(d.text, query, highlight_color)\n",
    "                display(Markdown(f\"### Source Document (page_label: {source_page})\\n\\n{highlighted}\"))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f8835ca6",
   "metadata": {},
   "source": [
    "Usage:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "e71a1326",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/markdown": [
       "### Source Document (page_label: 8)\n",
       "\n",
       "Preprint\n",
       "Table 1: Results with in-context learning on GSM8K math word problems. Each row represents\n",
       "a separate pipeline: the module in the Program column is compiled against the examples in the\n",
       "Training set. The programs, compilers, and (small) training sets are defined in Section 6. Rows with\n",
       "ensemblebuild on the immediately preceding row. Notably, all programs in this table are expressed\n",
       "by composing two to four DSPy modules and teleprompters. Compiling the correctmodules, instead\n",
       "of string prompts, improves different LMs from 4–20% accuracy to 49–88% accuracy.\n",
       "GPT-3.5 Llama2-13b-chat\n",
       "Program Compilation Training Dev Test Dev Test\n",
       "vanilla\n",
       "none n/a 24.0 25.2 7.0 9.4\n",
       "fewshot trainset 33.1 – 4.3 –\n",
       "bootstrap trainset 44.0 – 28.0 –\n",
       "bootstrap×2 trainset 64.7 61.7 37.3 36.5\n",
       "+ensemble trainset 62.7 61.9 39.0 34.6\n",
       "CoT\n",
       "none n/a 50.0 – 26.7 –\n",
       "fewshot trainset 63.0 – 27.3 –\n",
       "fewshot +humanCoT 78.6 72.4 34.3 33.7\n",
       "bootstrap trainset 80.3 72.9 43.3 –\n",
       "+ensemble trainset 88.3 81.6 43.7 –\n",
       "reflection\n",
       "none n/a 65.0 – 36.7 –\n",
       "fewshot trainset 71.7 – 36.3 –\n",
       "bootstrap trainset 83.0 76.0 44.3 40.2\n",
       "+ensemble trainset 86.7 – 49.0 46.9\n",
       "6 C ASE STUDY: M ATH WORD PROBLEMS\n",
       "We evaluate on the popular GSM8K dataset with grade school math questions (Cobbe et al., 2021).\n",
       "We sample 200 and 300 question–answer pairs from the official training set for training and develop-\n",
       "ment, respectively. Our final evaluations use the 1.3k official test set examples. We report extensive\n",
       "comparisons on the development set to avoid overfitting on test. Following prior work on GSM8K,\n",
       "we evaluate the accuracy of the final numerical value that appears in the LM output.\n",
       "Programs Considered For this task, we consider three simple DSPy programs: a one-step Pre-\n",
       "dict module (vanilla), a two-step ChainOfThought module ( CoT), and finally a multi-stage Com-\n",
       "parerOfThoughts module (ThoughtReflection). These are fully defined by the following code:\n",
       "1 vanilla = dspy.Predict(\"question -> answer\") # GSM8K Program ‘vanilla‘\n",
       "2\n",
       "3 CoT = dspy.ChainOfThought(\"question -> answer\") # GSM8K Program ‘CoT‘\n",
       "1 class ThoughtReflection(dspy.Module):\n",
       "2 def __init__(self, num_attempts):\n",
       "3 self.predict = dspy.ChainOfThought(\"question -> answer\", n=num_attempts)\n",
       "4 self.compare = dspy.MultiChainComparison(’question -> answer’, M=num_attempts)\n",
       "5\n",
       "6 def forward(self, question):\n",
       "7 completions = self.predict(question=question).completions\n",
       "8 return self.compare(question=question, completions=completions)\n",
       "9\n",
       "10 reflection = ThoughtReflection(num_attempts=5) # GSM8K Program ‘reflection‘\n",
       "In reflection, five reasoning chains are sampled from the LM (alongside their answers) and they\n",
       "are compared in parallel by a built-in MultiChainComparison module, which generalizes Yoran\n",
       "et al. (2023). This generates a new answer taking into account the patterns from the five attempts.\n",
       "Critically, the modules used are all generic, none is specific math problems or particular LM.\n",
       "Compiling As we discussed in Section 4, DSPy programs can be compiled into new, optimized\n",
       "programs. In our experiments, we evaluate the programs zero-shot (no compiling) as well as a\n",
       "number of strategies for compiling. Our simplest compiler is LabeledFewShot:\n",
       "1 fewshot = dspy.LabeledFewShot(k=8).compile(program, trainset=trainset)\n",
       "Here, programcan be any DSPy module. This simply samplesk=8random demonstrations from the\n",
       "trainsetfor the fields common to the training examples and the signature(s), in this case,question\n",
       "and answer, but not the reasoning for instance. We report the average of 3–5 runs (depending on the\n",
       "setting) when applying such random sampling.\n",
       "8"
      ],
      "text/plain": [
       "<IPython.core.display.Markdown object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/markdown": [
       "### Source Document (page_label: 2)\n",
       "\n",
       "Preprint\n",
       "calls in existing LM pipelines and in popular developer frameworks are generally implemented using\n",
       "hard-coded ‘prompt templates’, that is, long strings of instructions and demonstrations that are hand\n",
       "crafted through manual trial and error. We argue that this approach, while pervasive, can be brittle\n",
       "and unscalable—conceptually akin to hand-tuning the weights for a classifier. A given string prompt\n",
       "might not generalize to different pipelines or across different LMs, data domains, or even inputs.\n",
       "Toward a more systematic approach to designing AI pipelines, we introduce theDSPy programming\n",
       "model.1 DSPy pushes building new LM pipelines away from manipulating free-form strings and\n",
       "closer to programming (composing modular operators to build text transformation graphs) where a\n",
       "compiler automatically generates optimized LM invocation strategies and prompts from a program.\n",
       "We draw inspiration from the consensus that emerged around neural network abstractions (Bergstra\n",
       "et al., 2013), where (1) many general-purpose layers can be modularly composed in any complex\n",
       "architecture and (2) the model weights can be trained using optimizers instead of being hand-tuned.\n",
       "To this end, we propose the DSPy programming model(Sec 3). We first translate string-based\n",
       "prompting techniques, including complex and task-dependent ones like Chain of Thought (Wei et al.,\n",
       "2022) and ReAct (Yao et al., 2022), into declarative modules that carrynatural-language typed sig-\n",
       "natures. DSPy modules are task-adaptive components—akin to neural network layers—that abstract\n",
       "any particular text transformation, like answering a question or summarizing a paper. We then pa-\n",
       "rameterize each module so that it can learn its desired behavior by iteratively bootstrapping useful\n",
       "demonstrations within the pipeline. Inspired directly by PyTorch abstractions (Paszke et al., 2019),\n",
       "DSPy modules are used via expressive define-by-run computational graphs. Pipelines are expressed\n",
       "by (1) declaring the modules needed and (2) using these modules in any logical control flow (e.g.,\n",
       "ifstatements, for loops, exceptions, etc.) to logically connect the modules.\n",
       "We then develop theDSPy compiler(Sec 4), which optimizes any DSPy program to improve quality\n",
       "or cost. The compiler inputs are the program, a few training inputs with optional labels, and a valida-\n",
       "tion metric. The compiler simulates versions of the program on the inputs and bootstraps example\n",
       "traces of each module for self-improvement, using them to construct effective few-shot prompts\n",
       "or finetuning small LMs for steps of the pipeline. Optimization in DSPy is highly modular: it is\n",
       "conducted by teleprompters,2 which are general-purpose optimization strategies that determine how\n",
       "the modules should learn from data. In this way, the compiler automatically maps the declarative\n",
       "modules to high-quality compositions of prompting, finetuning, reasoning, and augmentation.\n",
       "Programming models like DSPy could be assessed along many dimensions, but we focus on the role\n",
       "of expert-crafted prompts in shaping system performance. We are seeking to reduce or even remove\n",
       "their role through DSPy modules (e.g., versions of popular techniques like Chain of Thought) and\n",
       "teleprompters. We report on two expansive case studies: math word problems (GMS8K; Cobbe et al.\n",
       "2021) and multi-hop question answering (HotPotQA; Yang et al. 2018) with explorations of chain\n",
       "of thought, multi-chain reflection, multi-hop retrieval, retrieval-augmented question answering, and\n",
       "agent loops. Our evaluations use a number of different compiling strategies effectively and show\n",
       "that straightforward DSPy programs outperform systems using hand-crafted prompts, while also\n",
       "allowing our programs to use much smaller and hence more efficient LMs effectively.\n",
       "Overall, this work proposes the first programming model that translates prompting techniques into\n",
       "parameterized declarative modules and introduces an effective compiler with general optimiza-\n",
       "tion strategies (teleprompters) to optimize arbitrary pipelines of these modules. Our main contri-\n",
       "butions are empirical and algorithmic: with DSPy, we have found that we can implement very\n",
       "short programs that can bootstrap self-improving multi-stage NLP systems using LMs as small as\n",
       "llama2-13b-chat and T5-Large (770M parameters). Without hand-crafted prompts and within\n",
       "minutes to tens of minutes of compiling, compositions of DSPy modules can raise the quality of\n",
       "simple programs from 33% to 82% (Sec 6) and from 32% to 46% (Sec 7) for GPT-3.5 and, simi-\n",
       "larly, from 9% to 47% (Sec 6) and from 22% to 41% (Sec 7) for llama2-13b-chat.\n",
       "1DSPy is pronounced <mark style='background-color:#ffff00;'>dee-ess-pie</mark>. It’s the second iteration of our earlier Demonstrate–Search–Predict\n",
       "framework (DSP; Khattab et al. 2022). This paper introduces the key concepts in DSPy. For more extensive and\n",
       "up-to-date documentation of the framework, we refer readers to https://github.com/stanfordnlp/dspy.\n",
       "2We derive the name tele-prompters from the notion of abstracting and automating the task of prompting,\n",
       "in particular, such that it happens at a distance, without manual intervention.\n",
       "2"
      ],
      "text/plain": [
       "<IPython.core.display.Markdown object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/markdown": [
       "### Source Document (page_label: 6)\n",
       "\n",
       "Preprint\n",
       "In DSPy, training sets may be small, potentially a handful of examples, though larger data enables\n",
       "more powerful optimization. Training examples may be incomplete, i.e., only input values are nec-\n",
       "essary. Labels for the pipeline steps are not required, unless they need to be used in the metric. In\n",
       "practice, we typically assume labels only for (at most) the program’s final output, not the intermedi-\n",
       "ate steps. This label-efficiency is critical for modularity: building a new pipeline in DSPy requires\n",
       "simply recompiling the new pipeline’s code, not annotating data specific to the new pipeline.\n",
       "Metrics can be simple notions like exact match (EM) or F1, but they can be entire DSPy programs\n",
       "that balance multiple concerns. For example, we may compile the RAG module above against a\n",
       "dataset of question–answer pairs qa trainset and the metric EM. The goal of optimization here is\n",
       "to effectively bootstrap few-shot demonstrations. The following code achieves this:\n",
       "1 # Small training set with only questions and final answers.\n",
       "2 qa_trainset = [dspy.Example(question=\"What is the capital of France?\", answer=\"Paris\")]\n",
       "3\n",
       "4 # The teleprompter will bootstrap missing labels: reasoning chains and retrieval contexts.\n",
       "5 teleprompter = dspy.BootstrapFewShot(metric=dspy.evaluate.answer_exact_match)\n",
       "6 compiled_rag = teleprompter.compile(RAG(), trainset=qa_trainset)\n",
       "In this example, the BootstrapFewShot teleprompter (Sec 4, Appendix E.1) simulates RAG on the\n",
       "training example(s). It will collect demonstrations of each module (i.e., examples of its input–output\n",
       "behavior) that collectively lead to valid output (i.e., respecting the signatures and the metric).\n",
       "If one wanted to push the compiled program to be extractive given its retrieved contexts, one could\n",
       "define a custom metric to use in place of dspy.evaluate.answer exact match:\n",
       "1 def answer_and_context_match(example, pred, trace=None):\n",
       "2 answer_match = dspy.evaluate.answer_exact_match(example, pred)\n",
       "3\n",
       "4 # Is the prediction a substring of some passage?\n",
       "5 context_match = any((pred.answer.lower() in c) for c in pred.context)\n",
       "6\n",
       "7 return answer_match and context_match\n",
       "Notice that behavior like this might be more accurately checked by another DSPy program that\n",
       "checks for faithful grounding of answers. Such metrics are fully supported and encouraged in DSPy.\n",
       "Teleprompters can be composed by specifying a teacher program. DSPy will sample demonstra-\n",
       "tions from this program for prompt optimization. This composition can enable very rich pipelines,\n",
       "where expensive programs (e.g., complex expensive ensembles using large LMs) supervise cheap\n",
       "programs (e.g., simple pipelines using smaller LMs). One may start withcompiled ragfrom above\n",
       "(say, compiled to use a large Llama2-13b-chat LM) but now fine-tune Flan-T5-large to create an\n",
       "efficient program:\n",
       "1 # Larger set of questions with *no labels*. Labels for all steps will be bootstrapped.\n",
       "2 unlabeled_questions = [dspy.Example(question=\"What is the capital of Germany?\"), ...]\n",
       "3\n",
       "4 # As we assumes no answer, we use ‘answer_passage_match‘ to filter ungrounded answers.\n",
       "5 finetuning_teleprompter = BootstrapFinetune(metric=dspy.evaluate.answer_passage_match)\n",
       "6\n",
       "7 # We set ‘teacher=compiled_rag‘ to compose. Bootstrapping will now use ‘compiled_rag ‘.\n",
       "8 compiled_rag_via_finetune = finetuning_teleprompter.compile(RAG(), teacher=compiled_rag,\n",
       "trainset=unlabeled_questions, target=’google/flan-t5-large’)\n",
       "4 T HE DSP Y COMPILER\n",
       "A key source of DSPy’s expressive power is its ability to compile—or automatically optimize—any\n",
       "program in this programming model. Compiling relies on a teleprompter, which is an optimizer for\n",
       "DSPy programs that improves the quality (or cost) of modules via prompting or finetuning, which\n",
       "are unified in DSPy. While DSPy does not enforce this when creating new teleprompters, typical\n",
       "teleprompters go through three stages.\n",
       "Stage 1: Candidate GenerationThe compiler first (recursively) finds all uniquePredictmodules\n",
       "(predictors) in a program, including those nested under other modules. For each unique predictor\n",
       "p, the teleprompter may generate candidate values for the parameters of p: the instructions, field\n",
       "descriptions, or—most importantly—demonstrations (i.e., example input–output pairs). In this iter-\n",
       "6"
      ],
      "text/plain": [
       "<IPython.core.display.Markdown object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "word = \"dee-ess-pie\"\n",
    "display_sources_with_highlight(response, docs, word)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
