{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "d497e57e",
   "metadata": {},
   "source": [
    "# 1. Setup Asyncio"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "0b7c49a4",
   "metadata": {},
   "outputs": [],
   "source": [
    "import nest_asyncio\n",
    "\n",
    "nest_asyncio.apply()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8d74b235",
   "metadata": {},
   "source": [
    "# 2. Setup the Qdrant vector database"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "bc42de2b",
   "metadata": {},
   "outputs": [],
   "source": [
    "import qdrant_client\n",
    "\n",
    "collection_name = \"chat_with_docs_docling\"\n",
    "\n",
    "client = qdrant_client.QdrantClient(\n",
    "    host=\"localhost\",\n",
    "    port=6333,\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "08dcc9f8",
   "metadata": {},
   "source": [
    "# 3. Read the documents"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 98,
   "id": "82745a5d",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/fc/experiments/rag-project/.venv/lib/python3.9/site-packages/torch/utils/data/dataloader.py:683: UserWarning: 'pin_memory' argument is set as true but not supported on MPS now, then device pinned memory won't be used.\n",
      "  warnings.warn(warn_msg)\n",
      "/Users/fc/experiments/rag-project/.venv/lib/python3.9/site-packages/torch/utils/data/dataloader.py:683: UserWarning: 'pin_memory' argument is set as true but not supported on MPS now, then device pinned memory won't be used.\n",
      "  warnings.warn(warn_msg)\n",
      "/Users/fc/experiments/rag-project/.venv/lib/python3.9/site-packages/torch/utils/data/dataloader.py:683: UserWarning: 'pin_memory' argument is set as true but not supported on MPS now, then device pinned memory won't be used.\n",
      "  warnings.warn(warn_msg)\n",
      "/Users/fc/experiments/rag-project/.venv/lib/python3.9/site-packages/torch/utils/data/dataloader.py:683: UserWarning: 'pin_memory' argument is set as true but not supported on MPS now, then device pinned memory won't be used.\n",
      "  warnings.warn(warn_msg)\n",
      "/Users/fc/experiments/rag-project/.venv/lib/python3.9/site-packages/torch/utils/data/dataloader.py:683: UserWarning: 'pin_memory' argument is set as true but not supported on MPS now, then device pinned memory won't be used.\n",
      "  warnings.warn(warn_msg)\n",
      "/Users/fc/experiments/rag-project/.venv/lib/python3.9/site-packages/torch/utils/data/dataloader.py:683: UserWarning: 'pin_memory' argument is set as true but not supported on MPS now, then device pinned memory won't be used.\n",
      "  warnings.warn(warn_msg)\n",
      "/Users/fc/experiments/rag-project/.venv/lib/python3.9/site-packages/torch/utils/data/dataloader.py:683: UserWarning: 'pin_memory' argument is set as true but not supported on MPS now, then device pinned memory won't be used.\n",
      "  warnings.warn(warn_msg)\n",
      "/Users/fc/experiments/rag-project/.venv/lib/python3.9/site-packages/torch/utils/data/dataloader.py:683: UserWarning: 'pin_memory' argument is set as true but not supported on MPS now, then device pinned memory won't be used.\n",
      "  warnings.warn(warn_msg)\n",
      "/Users/fc/experiments/rag-project/.venv/lib/python3.9/site-packages/torch/utils/data/dataloader.py:683: UserWarning: 'pin_memory' argument is set as true but not supported on MPS now, then device pinned memory won't be used.\n",
      "  warnings.warn(warn_msg)\n",
      "/Users/fc/experiments/rag-project/.venv/lib/python3.9/site-packages/torch/utils/data/dataloader.py:683: UserWarning: 'pin_memory' argument is set as true but not supported on MPS now, then device pinned memory won't be used.\n",
      "  warnings.warn(warn_msg)\n",
      "/Users/fc/experiments/rag-project/.venv/lib/python3.9/site-packages/torch/utils/data/dataloader.py:683: UserWarning: 'pin_memory' argument is set as true but not supported on MPS now, then device pinned memory won't be used.\n",
      "  warnings.warn(warn_msg)\n",
      "/Users/fc/experiments/rag-project/.venv/lib/python3.9/site-packages/torch/utils/data/dataloader.py:683: UserWarning: 'pin_memory' argument is set as true but not supported on MPS now, then device pinned memory won't be used.\n",
      "  warnings.warn(warn_msg)\n",
      "/Users/fc/experiments/rag-project/.venv/lib/python3.9/site-packages/torch/utils/data/dataloader.py:683: UserWarning: 'pin_memory' argument is set as true but not supported on MPS now, then device pinned memory won't be used.\n",
      "  warnings.warn(warn_msg)\n",
      "/Users/fc/experiments/rag-project/.venv/lib/python3.9/site-packages/torch/utils/data/dataloader.py:683: UserWarning: 'pin_memory' argument is set as true but not supported on MPS now, then device pinned memory won't be used.\n",
      "  warnings.warn(warn_msg)\n",
      "/Users/fc/experiments/rag-project/.venv/lib/python3.9/site-packages/torch/utils/data/dataloader.py:683: UserWarning: 'pin_memory' argument is set as true but not supported on MPS now, then device pinned memory won't be used.\n",
      "  warnings.warn(warn_msg)\n",
      "/Users/fc/experiments/rag-project/.venv/lib/python3.9/site-packages/torch/utils/data/dataloader.py:683: UserWarning: 'pin_memory' argument is set as true but not supported on MPS now, then device pinned memory won't be used.\n",
      "  warnings.warn(warn_msg)\n",
      "/Users/fc/experiments/rag-project/.venv/lib/python3.9/site-packages/torch/utils/data/dataloader.py:683: UserWarning: 'pin_memory' argument is set as true but not supported on MPS now, then device pinned memory won't be used.\n",
      "  warnings.warn(warn_msg)\n",
      "/Users/fc/experiments/rag-project/.venv/lib/python3.9/site-packages/torch/utils/data/dataloader.py:683: UserWarning: 'pin_memory' argument is set as true but not supported on MPS now, then device pinned memory won't be used.\n",
      "  warnings.warn(warn_msg)\n",
      "/Users/fc/experiments/rag-project/.venv/lib/python3.9/site-packages/torch/utils/data/dataloader.py:683: UserWarning: 'pin_memory' argument is set as true but not supported on MPS now, then device pinned memory won't be used.\n",
      "  warnings.warn(warn_msg)\n",
      "/Users/fc/experiments/rag-project/.venv/lib/python3.9/site-packages/torch/utils/data/dataloader.py:683: UserWarning: 'pin_memory' argument is set as true but not supported on MPS now, then device pinned memory won't be used.\n",
      "  warnings.warn(warn_msg)\n",
      "/Users/fc/experiments/rag-project/.venv/lib/python3.9/site-packages/torch/utils/data/dataloader.py:683: UserWarning: 'pin_memory' argument is set as true but not supported on MPS now, then device pinned memory won't be used.\n",
      "  warnings.warn(warn_msg)\n",
      "/Users/fc/experiments/rag-project/.venv/lib/python3.9/site-packages/torch/utils/data/dataloader.py:683: UserWarning: 'pin_memory' argument is set as true but not supported on MPS now, then device pinned memory won't be used.\n",
      "  warnings.warn(warn_msg)\n",
      "/Users/fc/experiments/rag-project/.venv/lib/python3.9/site-packages/torch/utils/data/dataloader.py:683: UserWarning: 'pin_memory' argument is set as true but not supported on MPS now, then device pinned memory won't be used.\n",
      "  warnings.warn(warn_msg)\n",
      "/Users/fc/experiments/rag-project/.venv/lib/python3.9/site-packages/torch/utils/data/dataloader.py:683: UserWarning: 'pin_memory' argument is set as true but not supported on MPS now, then device pinned memory won't be used.\n",
      "  warnings.warn(warn_msg)\n",
      "/Users/fc/experiments/rag-project/.venv/lib/python3.9/site-packages/torch/utils/data/dataloader.py:683: UserWarning: 'pin_memory' argument is set as true but not supported on MPS now, then device pinned memory won't be used.\n",
      "  warnings.warn(warn_msg)\n",
      "/Users/fc/experiments/rag-project/.venv/lib/python3.9/site-packages/torch/utils/data/dataloader.py:683: UserWarning: 'pin_memory' argument is set as true but not supported on MPS now, then device pinned memory won't be used.\n",
      "  warnings.warn(warn_msg)\n",
      "/Users/fc/experiments/rag-project/.venv/lib/python3.9/site-packages/torch/utils/data/dataloader.py:683: UserWarning: 'pin_memory' argument is set as true but not supported on MPS now, then device pinned memory won't be used.\n",
      "  warnings.warn(warn_msg)\n",
      "/Users/fc/experiments/rag-project/.venv/lib/python3.9/site-packages/torch/utils/data/dataloader.py:683: UserWarning: 'pin_memory' argument is set as true but not supported on MPS now, then device pinned memory won't be used.\n",
      "  warnings.warn(warn_msg)\n",
      "/Users/fc/experiments/rag-project/.venv/lib/python3.9/site-packages/torch/utils/data/dataloader.py:683: UserWarning: 'pin_memory' argument is set as true but not supported on MPS now, then device pinned memory won't be used.\n",
      "  warnings.warn(warn_msg)\n",
      "/Users/fc/experiments/rag-project/.venv/lib/python3.9/site-packages/torch/utils/data/dataloader.py:683: UserWarning: 'pin_memory' argument is set as true but not supported on MPS now, then device pinned memory won't be used.\n",
      "  warnings.warn(warn_msg)\n",
      "/Users/fc/experiments/rag-project/.venv/lib/python3.9/site-packages/torch/utils/data/dataloader.py:683: UserWarning: 'pin_memory' argument is set as true but not supported on MPS now, then device pinned memory won't be used.\n",
      "  warnings.warn(warn_msg)\n",
      "/Users/fc/experiments/rag-project/.venv/lib/python3.9/site-packages/torch/utils/data/dataloader.py:683: UserWarning: 'pin_memory' argument is set as true but not supported on MPS now, then device pinned memory won't be used.\n",
      "  warnings.warn(warn_msg)\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "from glob import glob\n",
    "from docling.datamodel.base_models import InputFormat\n",
    "from docling.document_converter import DocumentConverter, PdfFormatOption\n",
    "from docling.datamodel.pipeline_options import PdfPipelineOptions, TableFormerMode\n",
    "\n",
    "pdf_dir = \"./docs\"\n",
    "pdf_files = glob(os.path.join(pdf_dir, \"*.pdf\"))\n",
    "md_files = []\n",
    "\n",
    "pipeline_options = PdfPipelineOptions(do_table_structure=True)\n",
    "pipeline_options.table_structure_options.mode = TableFormerMode.ACCURATE  # use more accurate TableFormer model\n",
    "\n",
    "doc_converter = DocumentConverter(\n",
    "    format_options={\n",
    "        InputFormat.PDF: PdfFormatOption(pipeline_options=pipeline_options)\n",
    "    }\n",
    ")\n",
    "\n",
    "# Convert each PDF to Markdown using Docling's Python API\n",
    "for pdf_path in pdf_files:\n",
    "    result = converter.convert(pdf_path)\n",
    "    md = result.document.export_to_markdown()\n",
    "    md_files.append(md)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 99,
   "id": "942f91c5",
   "metadata": {},
   "outputs": [],
   "source": [
    "from llama_index.core.schema import Document\n",
    "\n",
    "docs = [Document(text=md) for md in md_files]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 100,
   "id": "aacc0201",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[Document(id_='d6e79c5a-f314-4d63-84a5-44567e9b6deb', embedding=None, metadata={}, excluded_embed_metadata_keys=[], excluded_llm_metadata_keys=[], relationships={}, metadata_template='{key}: {value}', metadata_separator='\\n', text_resource=MediaResource(embeddings=None, data=None, text='<!-- image -->\\n\\n## Docling Technical Report\\n\\nVersion 1.0\\n\\nChristoph Auer Maksym Lysak Ahmed Nassar Michele Dolfi Nikolaos Livathinos Panos Vagenas Cesar Berrospi Ramis Matteo Omenetti Fabian Lindlbauer Kasper Dinkla Lokesh Mishra Yusik Kim Shubham Gupta Rafael Teixeira de Lima Valery Weber Lucas Morin Ingmar Meijer Viktor Kuropiatnyk Peter W. J. Staar\\n\\nAI4K Group, IBM Research R¨ uschlikon, Switzerland\\n\\n## Abstract\\n\\nThis technical report introduces Docling , an easy to use, self-contained, MITlicensed open-source package for PDF document conversion. It is powered by state-of-the-art specialized AI models for layout analysis (DocLayNet) and table structure recognition (TableFormer), and runs efficiently on commodity hardware in a small resource budget. The code interface allows for easy extensibility and addition of new features and models.\\n\\n## 1 Introduction\\n\\nConverting PDF documents back into a machine-processable format has been a major challenge for decades due to their huge variability in formats, weak standardization and printing-optimized characteristic, which discards most structural features and metadata. With the advent of LLMs and popular application patterns such as retrieval-augmented generation (RAG), leveraging the rich content embedded in PDFs has become ever more relevant. In the past decade, several powerful document understanding solutions have emerged on the market, most of which are commercial software, cloud offerings [3] and most recently, multi-modal vision-language models. As of today, only a handful of open-source tools cover PDF conversion, leaving a significant feature and quality gap to proprietary solutions.\\n\\nWith Docling , we open-source a very capable and efficient document conversion tool which builds on the powerful, specialized AI models and datasets for layout analysis and table structure recognition we developed and presented in the recent past [12, 13, 9]. Docling is designed as a simple, self-contained python library with permissive license, running entirely locally on commodity hardware. Its code architecture allows for easy extensibility and addition of new features and models.\\n\\nHere is what Docling delivers today:\\n\\n- · Converts PDF documents to JSON or Markdown format, stable and lightning fast\\n- · Understands detailed page layout, reading order, locates figures and recovers table structures\\n- · Extracts metadata from the document, such as title, authors, references and language\\n- · Optionally applies OCR, e.g. for scanned PDFs\\n- · Can be configured to be optimal for batch-mode (i.e high throughput, low time-to-solution) or interactive mode (compromise on efficiency, low time-to-solution)\\n- · Can leverage different accelerators (GPU, MPS, etc).\\n\\n## 2 Getting Started\\n\\nTo use Docling, you can simply install the docling package from PyPI. Documentation and examples are available in our GitHub repository at github.com/DS4SD/docling. All required model assets 1 are downloaded to a local huggingface datasets cache on first use, unless you choose to pre-install the model assets in advance.\\n\\nDocling provides an easy code interface to convert PDF documents from file system, URLs or binary streams, and retrieve the output in either JSON or Markdown format. For convenience, separate methods are offered to convert single documents or batches of documents. A basic usage example is illustrated below. Further examples are available in the Doclign code repository.\\n\\n```\\nfrom docling.document_converter import DocumentConverter Large\\n```\\n\\n```\\nsource = \"https://arxiv.org/pdf/2206.01062\" # PDF path or URL converter = DocumentConverter() result = converter.convert_single(source) print(result.render_as_markdown()) # output: \"## DocLayNet: A Human -Annotated Dataset for Document -Layout Analysis [...]\"\\n```\\n\\nOptionally, you can configure custom pipeline features and runtime options, such as turning on or off features (e.g. OCR, table structure recognition), enforcing limits on the input document size, and defining the budget of CPU threads. Advanced usage examples and options are documented in the README file. Docling also provides a Dockerfile to demonstrate how to install and run it inside a container.\\n\\n## 3 Processing pipeline\\n\\nDocling implements a linear pipeline of operations, which execute sequentially on each given document (see Fig. 1). Each document is first parsed by a PDF backend, which retrieves the programmatic text tokens, consisting of string content and its coordinates on the page, and also renders a bitmap image of each page to support downstream operations. Then, the standard model pipeline applies a sequence of AI models independently on every page in the document to extract features and content, such as layout and table structures. Finally, the results from all pages are aggregated and passed through a post-processing stage, which augments metadata, detects the document language, infers reading-order and eventually assembles a typed document object which can be serialized to JSON or Markdown.\\n\\n## 3.1 PDF backends\\n\\nTwo basic requirements to process PDF documents in our pipeline are a) to retrieve all text content and their geometric coordinates on each page and b) to render the visual representation of each page as it would appear in a PDF viewer. Both these requirements are encapsulated in Docling\\'s PDF backend interface. While there are several open-source PDF parsing libraries available for python, we faced major obstacles with all of them for different reasons, among which were restrictive\\n\\n1 see huggingface.co/ds4sd/docling-models/\\n\\nFigure 1: Sketch of Docling\\'s default processing pipeline. The inner part of the model pipeline is easily customizable and extensible.\\n\\n<!-- image -->\\n\\nlicensing (e.g. pymupdf [7]), poor speed or unrecoverable quality issues, such as merged text cells across far-apart text tokens or table columns (pypdfium, PyPDF) [15, 14].\\n\\nWe therefore decided to provide multiple backend choices, and additionally open-source a custombuilt PDF parser, which is based on the low-level qpdf [4] library. It is made available in a separate package named docling-parse and powers the default PDF backend in Docling. As an alternative, we provide a PDF backend relying on pypdfium , which may be a safe backup choice in certain cases, e.g. if issues are seen with particular font encodings.\\n\\n## 3.2 AI models\\n\\nAs part of Docling, we initially release two highly capable AI models to the open-source community, which have been developed and published recently by our team. The first model is a layout analysis model, an accurate object-detector for page elements [13]. The second model is TableFormer [12, 9], a state-of-the-art table structure recognition model. We provide the pre-trained weights (hosted on huggingface) and a separate package for the inference code as docling-ibm-models . Both models are also powering the open-access deepsearch-experience, our cloud-native service for knowledge exploration tasks.\\n\\n## Layout Analysis Model\\n\\nOur layout analysis model is an object-detector which predicts the bounding-boxes and classes of various elements on the image of a given page. Its architecture is derived from RT-DETR [16] and re-trained on DocLayNet [13], our popular human-annotated dataset for document-layout analysis, among other proprietary datasets. For inference, our implementation relies on the onnxruntime [5].\\n\\nThe Docling pipeline feeds page images at 72 dpi resolution, which can be processed on a single CPU with sub-second latency. All predicted bounding-box proposals for document elements are post-processed to remove overlapping proposals based on confidence and size, and then intersected with the text tokens in the PDF to group them into meaningful and complete units such as paragraphs, section titles, list items, captions, figures or tables.\\n\\n## Table Structure Recognition\\n\\nThe TableFormer model [12], first published in 2022 and since refined with a custom structure token language [9], is a vision-transformer model for table structure recovery. It can predict the logical row and column structure of a given table based on an input image, and determine which table cells belong to column headers, row headers or the table body. Compared to earlier approaches, TableFormer handles many characteristics of tables, such as partial or no borderlines, empty cells, rows or columns, cell spans and hierarchy both on column-heading or row-heading level, tables with inconsistent indentation or alignment and other complexities. For inference, our implementation relies on PyTorch [2].\\n\\nThe Docling pipeline feeds all table objects detected in the layout analysis to the TableFormer model, by providing an image-crop of the table and the included text cells. TableFormer structure predictions are matched back to the PDF cells in post-processing to avoid expensive re-transcription text in the table image. Typical tables require between 2 and 6 seconds to be processed on a standard CPU, strongly depending on the amount of included table cells.\\n\\n## OCR\\n\\nDocling provides optional support for OCR, for example to cover scanned PDFs or content in bitmaps images embedded on a page. In our initial release, we rely on EasyOCR [1], a popular thirdparty OCR library with support for many languages. Docling, by default, feeds a high-resolution page image (216 dpi) to the OCR engine, to allow capturing small print detail in decent quality. While EasyOCR delivers reasonable transcription quality, we observe that it runs fairly slow on CPU (upwards of 30 seconds per page).\\n\\nWe are actively seeking collaboration from the open-source community to extend Docling with additional OCR backends and speed improvements.\\n\\n## 3.3 Assembly\\n\\nIn the final pipeline stage, Docling assembles all prediction results produced on each page into a well-defined datatype that encapsulates a converted document, as defined in the auxiliary package docling-core . The generated document object is passed through a post-processing model which leverages several algorithms to augment features, such as detection of the document language, correcting the reading order, matching figures with captions and labelling metadata such as title, authors and references. The final output can then be serialized to JSON or transformed into a Markdown representation at the users request.\\n\\n## 3.4 Extensibility\\n\\nDocling provides a straight-forward interface to extend its capabilities, namely the model pipeline. A model pipeline constitutes the central part in the processing, following initial document parsing and preceding output assembly, and can be fully customized by sub-classing from an abstract baseclass ( BaseModelPipeline ) or cloning the default model pipeline. This effectively allows to fully customize the chain of models, add or replace models, and introduce additional pipeline configuration parameters. To use a custom model pipeline, the custom pipeline class to instantiate can be provided as an argument to the main document conversion methods. We invite everyone in the community to propose additional or alternative models and improvements.\\n\\nImplementations of model classes must satisfy the python Callable interface. The \\\\_\\\\_call\\\\_\\\\_ method must accept an iterator over page objects, and produce another iterator over the page objects which were augmented with the additional features predicted by the model, by extending the provided PagePredictions data model accordingly.\\n\\n## 4 Performance\\n\\nIn this section, we establish some reference numbers for the processing speed of Docling and the resource budget it requires. All tests in this section are run with default options on our standard test set distributed with Docling, which consists of three papers from arXiv and two IBM Redbooks, with a total of 225 pages. Measurements were taken using both available PDF backends on two different hardware systems: one MacBook Pro M3 Max, and one bare-metal server running Ubuntu 20.04 LTS on an Intel Xeon E5-2690 CPU. For reproducibility, we fixed the thread budget (through setting OMP NUM THREADS environment variable ) once to 4 (Docling default) and once to 16 (equal to full core count on the test hardware). All results are shown in Table 1.\\n\\nIf you need to run Docling in very low-resource environments, please consider configuring the pypdfium backend. While it is faster and more memory efficient than the default docling-parse backend, it will come at the expense of worse quality results, especially in table structure recovery.\\n\\nEstablishing GPU acceleration support for the AI models is currently work-in-progress and largely untested, but may work implicitly when CUDA is available and discovered by the onnxruntime and\\n\\ntorch runtimes backing the Docling pipeline. We will deliver updates on this topic at in a future version of this report.\\n\\nTable 1: Runtime characteristics of Docling with the standard model pipeline and settings, on our test dataset of 225 pages, on two different systems. OCR is disabled. We show the time-to-solution (TTS), computed throughput in pages per second, and the peak memory used (resident set size) for both the Docling-native PDF backend and for the pypdfium backend, using 4 and 16 threads.\\n\\n| CPU                         | Thread budget   | native backend   | native backend   | native backend   | pypdfium backend   | pypdfium backend   | pypdfium backend   |\\n|-----------------------------|-----------------|------------------|------------------|------------------|--------------------|--------------------|--------------------|\\n|                             |                 | TTS              | Pages/s          | Mem              | TTS                | Pages/s            | Mem                |\\n| Apple M3 Max                | 4               | 177 s 167 s      | 1.27 1.34        | 6.20 GB          | 103 s 92 s         | 2.18 2.45          | 2.56 GB            |\\n| (16 cores) Intel(R) E5-2690 | 16 4 16         | 375 s 244 s      | 0.60 0.92        | 6.16 GB          | 239 s 143 s        | 0.94 1.57          | 2.42 GB            |\\n\\n## 5 Applications\\n\\nThanks to the high-quality, richly structured document conversion achieved by Docling, its output qualifies for numerous downstream applications. For example, Docling can provide a base for detailed enterprise document search, passage retrieval or classification use-cases, or support knowledge extraction pipelines, allowing specific treatment of different structures in the document, such as tables, figures, section structure or references. For popular generative AI application patterns, such as retrieval-augmented generation (RAG), we provide quackling , an open-source package which capitalizes on Docling\\'s feature-rich document output to enable document-native optimized vector embedding and chunking. It plugs in seamlessly with LLM frameworks such as LlamaIndex [8]. Since Docling is fast, stable and cheap to run, it also makes for an excellent choice to build document-derived datasets. With its powerful table structure recognition, it provides significant benefit to automated knowledge-base construction [11, 10]. Docling is also integrated within the open IBM data prep kit [6], which implements scalable data transforms to build large-scale multi-modal training datasets.\\n\\n## 6 Future work and contributions\\n\\nDocling is designed to allow easy extension of the model library and pipelines. In the future, we plan to extend Docling with several more models, such as a figure-classifier model, an equationrecognition model, a code-recognition model and more. This will help improve the quality of conversion for specific types of content, as well as augment extracted document metadata with additional information. Further investment into testing and optimizing GPU acceleration as well as improving the Docling-native PDF backend are on our roadmap, too.\\n\\nWe encourage everyone to propose or implement additional features and models, and will gladly take your inputs and contributions under review . The codebase of Docling is open for use and contribution, under the MIT license agreement and in alignment with our contributing guidelines included in the Docling repository. If you use Docling in your projects, please consider citing this technical report.\\n\\n## References\\n\\n- [1] J. AI. Easyocr: Ready-to-use ocr with 80+ supported languages. https://github.com/ JaidedAI/EasyOCR , 2024. Version: 1.7.0.\\n- [2] J. Ansel, E. Yang, H. He, N. Gimelshein, A. Jain, M. Voznesensky, B. Bao, P. Bell, D. Berard, E. Burovski, G. Chauhan, A. Chourdia, W. Constable, A. Desmaison, Z. DeVito, E. Ellison, W. Feng, J. Gong, M. Gschwind, B. Hirsh, S. Huang, K. Kalambarkar, L. Kirsch, M. Lazos, M. Lezcano, Y. Liang, J. Liang, Y. Lu, C. Luk, B. Maher, Y. Pan, C. Puhrsch, M. Reso, M. Saroufim, M. Y. Siraichi, H. Suk, M. Suo, P. Tillet, E. Wang, X. Wang, W. Wen, S. Zhang, X. Zhao, K. Zhou, R. Zou, A. Mathews, G. Chanan, P. Wu, and S. Chintala. Pytorch 2: Faster\\n\\nmachine learning through dynamic python bytecode transformation and graph compilation. In Proceedings of the 29th ACM International Conference on Architectural Support for Programming Languages and Operating Systems, Volume 2 (ASPLOS \\'24) . ACM, 4 2024. doi: 10.1145/3620665.3640366. URL https://pytorch.org/assets/pytorch2-2.pdf .\\n\\n- [3] C. Auer, M. Dolfi, A. Carvalho, C. B. Ramis, and P. W. Staar. Delivering document conversion as a cloud service with high throughput and responsiveness. In 2022 IEEE 15th International Conference on Cloud Computing (CLOUD) , pages 363-373. IEEE, 2022.\\n- [4] J. Berkenbilt. Qpdf: A content-preserving pdf document transformer, 2024. URL https: //github.com/qpdf/qpdf .\\n- [5] O. R. developers. Onnx runtime. https://onnxruntime.ai/ , 2024. Version: 1.18.1.\\n- [6] IBM. Data Prep Kit: a community project to democratize and accelerate unstructured data preparation for LLM app developers, 2024. URL https://github.com/IBM/ data-prep-kit .\\n- [7] A. S. Inc. PyMuPDF, 2024. URL https://github.com/pymupdf/PyMuPDF .\\n- [8] J. Liu. LlamaIndex, 11 2022. URL https://github.com/jerryjliu/llama\\\\_index .\\n- [9] M. Lysak, A. Nassar, N. Livathinos, C. Auer, and P. Staar. Optimized Table Tokenization for Table Structure Recognition. In Document Analysis and Recognition - ICDAR 2023: 17th International Conference, San Jos´ e, CA, USA, August 21-26, 2023, Proceedings, Part II , pages 37-50, Berlin, Heidelberg, Aug. 2023. Springer-Verlag. ISBN 978-3-031-41678-1. doi: 10. 1007/978-3-031-41679-8 3. URL https://doi.org/10.1007/978-3-031-41679-8\\\\_3 .\\n- [10] L. Mishra, S. Dhibi, Y. Kim, C. Berrospi Ramis, S. Gupta, M. Dolfi, and P. Staar. Statements: Universal information extraction from tables with large language models for ESG KPIs. In D. Stammbach, J. Ni, T. Schimanski, K. Dutia, A. Singh, J. Bingler, C. Christiaen, N. Kushwaha, V. Muccione, S. A. Vaghefi, and M. Leippold, editors, Proceedings of the 1st Workshop on Natural Language Processing Meets Climate Change (ClimateNLP 2024) , pages 193-214, Bangkok, Thailand, Aug. 2024. Association for Computational Linguistics. URL https://aclanthology.org/2024.climatenlp-1.15 .\\n- [11] L. Morin, V. Weber, G. I. Meijer, F. Yu, and P. W. J. Staar. Patcid: an open-access dataset of chemical structures in patent documents. Nature Communications , 15(1):6532, August 2024. ISSN 2041-1723. doi: 10.1038/s41467-024-50779-y. URL https://doi.org/10.1038/ s41467-024-50779-y .\\n- [12] A. Nassar, N. Livathinos, M. Lysak, and P. Staar. Tableformer: Table structure understanding with transformers. In Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition , pages 4614-4623, 2022.\\n- [13] B. Pfitzmann, C. Auer, M. Dolfi, A. S. Nassar, and P. Staar. Doclaynet: a large humanannotated dataset for document-layout segmentation. pages 3743-3751, 2022.\\n- [14] pypdf Maintainers. pypdf: A Pure-Python PDF Library, 2024. URL https://github.com/ py-pdf/pypdf .\\n- [15] P. Team. PyPDFium2: Python bindings for PDFium, 2024. URL https://github.com/ pypdfium2-team/pypdfium2 .\\n- [16] Y. Zhao, W. Lv, S. Xu, J. Wei, G. Wang, Q. Dang, Y. Liu, and J. Chen. Detrs beat yolos on real-time object detection, 2023.\\n\\n## Appendix\\n\\nIn this section, we illustrate a few examples of Docling\\'s output in Markdown and JSON.\\n\\n## DocLayNet: A Large Human-Annotated Dataset for Document-Layout Analysis\\n\\n## DocLayNet: A Large Human-Annotated Dataset for Document-Layout Analysis\\n\\nBirgit Pfitzmann IBM Research Rueschlikon, Switzerland bpf@zurich.ibm.com\\n\\nChristoph Auer IBM Research Rueschlikon, Switzerland cau@zurich.ibm.com\\n\\nMichele Dolfi IBM Research Rueschlikon, Switzerland dol@zurich.ibm.com\\n\\nAhmed S. Nassar IBM Research Rueschlikon, Switzerland ahn@zurich.ibm.com\\n\\nPeter Staar IBM Research Rueschlikon, Switzerland taa@zurich.ibm.com\\n\\n## ABSTRACT\\n\\nAccurate document layout analysis is a key requirement for highquality PDF document conversion. With the recent availability of public, large ground-truth datasets such as PubLayNet and DocBank, deep-learning models have proven to be very effective at layout detection and segmentation. While these datasets are of adequate size to train such models, they severely lack in layout variability since they are sourced from scientific article repositories such as PubMed and arXiv only. Consequently, the accuracy of the layout segmentation drops significantly when these models are applied on more challenging and diverse layouts. In this paper, we present DocLayNet , a new, publicly available, document-layout annotation dataset in COCO format. It contains 80863 manually annotated pages from diverse data sources to represent a wide variability in layouts. For each PDF page, the layout annotations provide labelled bounding-boxes with a choice of 11 distinct classes. DocLayNet also provides a subset of double- and triple-annotated pages to determine the inter-annotator agreement. In multiple experiments, we provide baseline accuracy scores (in mAP) for a set of popular object detection models. We also demonstrate that these models fall approximately 10% behind the inter-annotator agreement. Furthermore, we provide evidence that DocLayNet is of sufficient size. Lastly, we compare models trained on PubLayNet, DocBank and DocLayNet, showing that layout predictions of the DocLayNettrained models are more robust and thus the preferred choice for general-purpose document-layout analysis.\\n\\n## CCS CONCEPTS\\n\\n· Informationsystems → Documentstructure ; · Appliedcomputing → Document analysis ; · Computing methodologies → Machine learning Computer vision ; ; Object detection ;\\n\\nPermission to make digital or hard copies of part or all of this work for personal or classroom use is granted without fee provided that copies are not made or distributed for profit or commercial advantage and that copies bear this notice and the full citation on the first page. Copyrights for third-party components of this work must be honored. For all other uses, contact the owner/author(s). KDD \\'22, August 14-18, 2022, Washington, DC, USA © 2022 Copyright held by the owner/author(s). ACM ISBN 978-1-4503-9385-0/22/08. https://doi.org/10.1145/3534678.3539043\\n\\nBirgit Pfitzmann IBM Research Rueschlikon, Switzerland bpf@zurich.ibm.com\\n\\nChristoph Auer IBM Research Rueschlikon, Switzerland cau@zurich.ibm.com\\n\\nMichele Dolfi IBM Research Rueschlikon, Switzerland dol@zurich.ibm.com\\n\\nAhmed S. Nassar IBM Research Rueschlikon, Switzerland ahn@zurich.ibm.com\\n\\nPeter Staar IBM Research Rueschlikon, Switzerland taa@zurich.ibm.com\\n\\n## ABSTRACT\\n\\nAccurate document layout analysis is a key requirement for highquality PDF document conversion. With the recent availability of public, large groundtruth datasets such as PubLayNet and DocBank, deep-learning models have proven to be very effective at layout detection and segmentation. While these datasets are of adequate size to train such models, they severely lack in layout variability since they are sourced from scientific article repositories such as PubMed and arXiv only. Consequently, the accuracy of the layout segmentation drops significantly when these models are applied on more challenging and diverse layouts. In this paper, we present DocLayNet , a new, publicly available, document-layout annotation dataset in COCO format. It contains 80863 manually annotated pages from diverse data sources to represent a wide variability in layouts. For each PDF page, the layout annotations provide labelled bounding-boxes with a choice of 11 distinct classes. DocLayNet also provides a subset of double- and triple-annotated pages to determine the inter-annotator agreement. In multiple experiments, we provide baseline accuracy scores (in mAP) for a set of popular object detection models. We also demonstrate that these models fall approximately 10% behind the inter-annotator agreement. Furthermore, we provide evidence that DocLayNet is of sufficient size. Lastly, we compare models trained on PubLayNet, DocBank and DocLayNet, showing that layout predictions of the DocLayNettrained models are more robust and thus the preferred choice for general-purpose document-layout analysis.\\n\\n## CCS CONCEPTS\\n\\nÆ Information systems → Document structure ; Æ Applied computing → Document analysis ; Æ Computing methodologies → Machine learning ; Computer vision ; Object detection ;\\n\\nPermission to make digital or hard copies of part or all of this work for personal or classroom use is granted without fee provided that copies are not made or distributed for profit or commercial advantage and that copies bear this notice and the full citation on the first page. Copyrights for third-party components of this work must be honored. For all other uses, contact the owner/author(s).\\n\\nKDD \\'22, August 14-18, 2022, Washington, DC, USA \\' 2022 Copyright held by the owner/author(s). ACM ISBN 978-1-4503-9385-0/22/08. https://doi.org/10.1145/3534678.3539043\\n\\nFigure 1: Four examples of complex page layouts across different document categories\\n\\n## KEYWORDS\\n\\nPDF document conversion, layout segmentation, object-detection, data set, Machine Learning\\n\\n## ACM Reference Format:\\n\\nBirgit Pfitzmann, Christoph Auer, Michele Dolfi, Ahmed S. Nassar, and Peter Staar. 2022. DocLayNet: A Large Human-Annotated Dataset for DocumentLayout Analysis. In Proceedings of the 28th ACM SIGKDD Conference on Knowledge Discovery and Data Mining (KDD \\'22), August 14-18, 2022, Washington, DC, USA. ACM, New York, NY, USA, 9 pages. https://doi.org/10.1145/ 3534678.3539043\\n\\n<!-- image -->\\n\\n<!-- image -->\\n\\nAGL Energy Limited  ABN 74 1\\n\\n5 061 375\\n\\n<!-- image -->\\n\\n<!-- image -->\\n\\nFigure 1: Four examples of complex page layouts across different document categories\\n\\n## KEYWORDS\\n\\nPDF document conversion, layout segmentation, object-detection, data set, Machine Learning\\n\\n## ACMReference Format:\\n\\nBirgit Pfitzmann, Christoph Auer, Michele Dolfi, Ahmed S. Nassar, and Peter Staar. 2022. DocLayNet: A Large Human-Annotated Dataset for DocumentLayout Analysis. In Proceedings of the 28th ACM SIGKDD Conference on Knowledge Discovery and Data Mining (KDD \\'22), August 14-18, 2022, Washington, DC, USA. ACM, New York, NY, USA, 9 pages. https://doi.org/10.1145/ 3534678.3539043\\n\\n1 INTRODUCTION\\n\\nDespite the substantial improvements achieved with machine-learning (ML) approaches and deep neural networks in recent years, document conversion remains a challenging problem, as demonstrated by the numerous public competitions held on this topic [1-4]. The challenge originates from the huge variability in PDF documents regarding layout, language and formats (scanned, programmatic or a combination of both). Engineering a single ML model that can be applied on all types of documents and provides high-quality layout segmentation remains to this day extremely challenging [5]. To highlight the variability in document layouts, we show a few example documents from the DocLayNet dataset in Figure 1. Figure 2: Title page of the DocLayNet paper (arxiv.org/pdf/2206.01062) - left PDF, right rendered Markdown. If recognized, metadata such as authors are appearing first under the title. Text content inside figures is currently dropped, the caption is retained and linked to the figure in the JSON representation (not shown).\\n\\nKDD \\'22, August 14-18, 2022, Washington, DC, USA Birgit Pfitzmann, Christoph Auer, Michele Dolfi, Ahmed S. Nassar, and Peter Staar\\n\\nTable 2: Prediction performance (mAP@0.5-0.95) of object detection networks on DocLayNet test set. The MRCNN (Mask R-CNN) and FRCNN (Faster R-CNN) models with ResNet-50 or ResNet-101 backbone were trained based on the network architectures from the detectron2 model zoo (Mask R-CNN R50, R101-FPN 3x, Faster R-CNN R101-FPN 3x), with default configurations. The YOLO implementation utilized was YOLOv5x6 [13]. All models were initialised using pre-trained weights from the COCO 2017 dataset.\\n\\n|                                                                                                        | human                                                                   | MRCNN R50 R101                                                                                                          | FRCNN R101                                                  | YOLO v5x6                                                   |\\n|--------------------------------------------------------------------------------------------------------|-------------------------------------------------------------------------|-------------------------------------------------------------------------------------------------------------------------|-------------------------------------------------------------|-------------------------------------------------------------|\\n| Caption Footnote Formula List-item Page-footer Page-header Picture Section-header Table Text Title All | 84-89 83-91 83-85 87-88 93-94 85-89 69-71 83-84 77-81 84-86 60-72 82-83 | 68.4 71.5 70.9 71.8 60.1 63.4 81.2 80.8 61.6 59.3 71.9 70.0 71.7 72.7 67.6 69.3 82.2 82.9 84.6 85.8 76.7 80.4 72.4 73.5 | 70.1 73.7 63.5 81.0 58.9 72.0 72.0 68.4 82.2 85.4 79.9 73.4 | 77.7 77.2 66.2 86.2 61.1 67.9 77.1 74.6 86.3 88.1 82.7 76.8 |\\n\\nto avoid this at any cost in order to have clear, unbiased baseline numbers for human document-layout annotation. Third, we introduced the feature of snapping boxes around text segments to obtain a pixel-accurate annotation and again reduce time and effort. The CCS annotation tool automatically shrinks every user-drawn box to the minimum bounding-box around the enclosed text-cells for all purely text-based segments, which excludes only Table and Picture . For the latter, we instructed annotation staff to minimise inclusion of surrounding whitespace while including all graphical lines. A downside of snapping boxes to enclosed text cells is that some wrongly parsed PDF pages cannot be annotated correctly and need to be skipped. Fourth, we established a way to flag pages as rejected for cases where no valid annotation according to the label guidelines could be achieved. Example cases for this would be PDF pages that render incorrectly or contain layouts that are impossible to capture with non-overlapping rectangles. Such rejected pages are not contained in the final dataset. With all these measures in place, experienced annotation staff managed to annotate a single page in a typical timeframe of 20s to 60s, depending on its complexity.\\n\\n## 5 EXPERIMENTS\\n\\nThe primary goal of DocLayNet is to obtain high-quality ML models capable of accurate document-layout analysis on a wide variety of challenging layouts. As discussed in Section 2, object detection models are currently the easiest to use, due to the standardisation of ground-truth data in COCO format [16] and the availability of general frameworks such as detectron2 [17]. Furthermore, baseline numbers in PubLayNet and DocBank were obtained using standard object detection models such as Mask R-CNN and Faster R-CNN. As such, we will relate to these object detection methods in this\\n\\n<!-- image -->\\n\\nThird, achienec\\n\\n## EXPERIMENTS\\n\\nchalenongayouls ground-vuth dawa such WC\\n\\n<!-- image -->\\n\\nFigure 5: Prediction performance (mAP@0.5-0.95) of a Mask R-CNNnetworkwithResNet50backbonetrainedonincreasing fractions of the DocLayNet dataset. The learning curve flattens around the 80% mark, indicating that increasing the size of the DocLayNet dataset with similar data will not yield significantly better predictions.\\n\\npaper and leave the detailed evaluation of more recent methods mentioned in Section 2 for future work.\\n\\nIn this section, we will present several aspects related to the performance of object detection models on DocLayNet. Similarly as in PubLayNet, we will evaluate the quality of their predictions using mean average precision (mAP) with 10 overlaps that range from 0.5 to 0.95 in steps of 0.05 (mAP@0.5-0.95). These scores are computed by leveraging the evaluation code provided by the COCO API [16].\\n\\n## Baselines for Object Detection\\n\\nIn Table 2, we present baseline experiments (given in mAP) on Mask R-CNN [12], Faster R-CNN [11], and YOLOv5 [13]. Both training and evaluation were performed on RGB images with dimensions of 1025 × 1025 pixels. For training, we only used one annotation in case of redundantly annotated pages. As one can observe, the variation in mAP between the models is rather low, but overall between 6 and 10% lower than the mAP computed from the pairwise human annotations on triple-annotated pages. This gives a good indication that the DocLayNet dataset poses a worthwhile challenge for the research community to close the gap between human recognition and ML approaches. It is interesting to see that Mask R-CNN and Faster R-CNN produce very comparable mAP scores, indicating that pixel-based image segmentation derived from bounding-boxes does not help to obtain better predictions. On the other hand, the more recent Yolov5x model does very well and even out-performs humans on selected labels such as Text , Table and Picture . This is not entirely surprising, as Text , Table and Picture are abundant and the most visually distinctive in a document.\\n\\ncoioct dcochon modols\\n\\n## Baselines for Object Detection\\n\\nmak enbrel\\n\\nFigure 3: Page 6 of the DocLayNet paper. If recognized, metadata such as authors are appearing first under the title. Elements recognized as page headers or footers are suppressed in Markdown to deliver uninterrupted content in reading order. Tables are inserted in reading order. The paragraph in \\'5. Experiments\\' wrapping over the column end is broken up in two and interrupted by the table.\\n\\nKDD \\'22, August 14-18, 2022, Washington, DC, USA\\n\\nBirgit Pfitzmann, Christoph Auer, Michele Dolfi, Ahmed S. Nassar, and Peter Staar\\n\\nTable 1: DocLayNet dataset overview. Along with the frequency of each class label, we present the relative occurrence (as %\\n\\nbetween pairwise annotations from the triple-annotated pages, from which we obtain accuracy ranges.\\n\\nof row \\'Total\\') in the train, test and validation sets. The inter-annotator agreement is computed as the mAP@0.5-0.95 metric\\n\\n<!-- image -->\\n\\n<!-- image -->\\n\\n| class label    | Count   | % of Total   | % of Total   | % of Total   | triple inter-annotator mAP @0.5-0.95 (%)   | triple inter-annotator mAP @0.5-0.95 (%)   | triple inter-annotator mAP @0.5-0.95 (%)   | triple inter-annotator mAP @0.5-0.95 (%)   | triple inter-annotator mAP @0.5-0.95 (%)   | triple inter-annotator mAP @0.5-0.95 (%)   | triple inter-annotator mAP @0.5-0.95 (%)   |\\n|----------------|---------|--------------|--------------|--------------|--------------------------------------------|--------------------------------------------|--------------------------------------------|--------------------------------------------|--------------------------------------------|--------------------------------------------|--------------------------------------------|\\n| class label    | Count   | Train        | Test         | Val          | All                                        | Fin                                        | Man                                        | Sci                                        | Law                                        | Pat                                        | Ten                                        |\\n| Caption        | 22524   | 2.04         | 1.77         | 2.32         | 84-89                                      | 40-61                                      | 86-92                                      | 94-99                                      | 95-99                                      | 69-78                                      | n/a                                        |\\n| Footnote       | 6318    | 0.60         | 0.31         | 0.58         | 83-91                                      | n/a                                        | 100                                        | 62-88                                      | 85-94                                      | n/a                                        | 82-97                                      |\\n| Formula        | 25027   | 2.25         | 1.90         | 2.96         | 83-85                                      | n/a                                        | n/a                                        | 84-87                                      | 86-96                                      | n/a                                        | n/a                                        |\\n| List-item      | 185660  | 17.19        | 13.34        | 15.82        | 87-88                                      | 74-83                                      | 90-92                                      | 97-97                                      | 81-85                                      | 75-88                                      | 93-95                                      |\\n| Page-footer    | 70878   | 6.51         | 5.58         | 6.00         | 93-94                                      | 88-90                                      | 95-96                                      | 100                                        | 92-97                                      | 100                                        | 96-98                                      |\\n| Page-header    | 58022   | 5.10         | 6.70         | 5.06         | 85-89                                      | 66-76                                      | 90-94                                      | 98-100                                     | 91-92                                      | 97-99                                      | 81-86                                      |\\n| Picture        | 45976   | 4.21         | 2.78         | 5.31         | 69-71                                      | 56-59                                      | 82-86                                      | 69-82                                      | 80-95                                      | 66-71                                      | 59-76                                      |\\n| Section-header | 142884  | 12.60        | 15.77        | 12.85        | 83-84                                      | 76-81                                      | 90-92                                      | 94-95                                      | 87-94                                      | 69-73                                      | 78-86                                      |\\n| Table          | 34733   | 3.20         | 2.27         | 3.60         | 77-81                                      | 75-80                                      | 83-86                                      | 98-99                                      | 58-80                                      | 79-84                                      | 70-85                                      |\\n| Text           | 510377  | 45.82        | 49.28        | 45.00        | 84-86                                      | 81-86                                      | 88-93                                      | 89-93                                      | 87-92                                      | 71-79                                      | 87-95                                      |\\n| Title          | 5071    | 0.47         | 0.30         | 0.50         | 60-72                                      | 24-63                                      | 50-63                                      | 94-100                                     | 82-96                                      | 68-79                                      | 24-56                                      |\\n| Total          | 1107470 | 941123       | 99816        | 66531        | 82-83                                      | 71-74                                      | 79-81                                      | 89-94                                      | 86-91                                      | 71-76                                      | 68-85                                      |\\n\\n<!-- image -->\\n\\ninclude publication repositories such as arXiv\\n\\nTable 1: DocLayNet dataset overview. Along with the frequency of each class label, we present the relative occurrence (as % of row \"Total\") in the train, test and validation sets. The inter-annotator agreement is computed as the mAP@0.5-0.95 metric between pairwise annotations from the triple-\\n\\nannotated pages, from which we obtain accuracy ranges.\\n\\n<!-- image -->\\n\\n|                       |         | %of Total   | %of Total   | %of Total   | triple inter- annotator mAP @ 0.5-0.95 (%)   | triple inter- annotator mAP @ 0.5-0.95 (%)   | triple inter- annotator mAP @ 0.5-0.95 (%)   | triple inter- annotator mAP @ 0.5-0.95 (%)   | triple inter- annotator mAP @ 0.5-0.95 (%)   | triple inter- annotator mAP @ 0.5-0.95 (%)   | triple inter- annotator mAP @ 0.5-0.95 (%)   |\\n|-----------------------|---------|-------------|-------------|-------------|----------------------------------------------|----------------------------------------------|----------------------------------------------|----------------------------------------------|----------------------------------------------|----------------------------------------------|----------------------------------------------|\\n| class label           | Count   | Train       | Test        | Val         | All                                          | Fin                                          | Man                                          | Sci                                          | Law                                          | Pat                                          | Ten                                          |\\n| Caption               | 22524   | 2.04        | 1.77        | 2.32        | 84-89                                        | 40-61                                        | 86-92                                        | 94-99                                        | 95-99                                        | 69-78                                        | n/a                                          |\\n| Footnote              | 6318    | 0.60        | 0.31        | 0.58        | 83-91                                        | n/a                                          | 100                                          | 62-88                                        | 85-94                                        | n/a                                          | 82-97                                        |\\n| Formula               | 25027   | 2.25        | 1.90        | 2.96        | 83-85                                        | n/a                                          | n/a                                          | 84-87                                        | 86-96                                        | n/a                                          | n/a                                          |\\n| List-item             | 185660  | 17.19       | 13.34       | 15.82       | 87-88                                        | 74-83                                        | 90-92                                        | 97-97                                        | 81-85                                        | 75-88                                        | 93-95                                        |\\n| Page- footer          | 70878   | 6.51        | 5.58        | 6.00        | 93-94                                        | 88-90                                        | 95-96                                        | 100                                          | 92-97                                        | 100                                          | 96-98                                        |\\n| Page- header offices, | 58022   | 5.10        | 6.70        | 5.06        | 85-89                                        | 66-76                                        | 90-94                                        | 98-100                                       | 91-92                                        | 97-99                                        | 81-86                                        |\\n| Picture               | 45976   | 4.21        | 2.78        | 5.31        | 69-71                                        | 56-59                                        | 82-86                                        | 69-82                                        | 80-95                                        | 66-71                                        | 59-76                                        |\\n| Section- header not   | 142884  | 12.60       | 15.77       | 12.85       | 83-84                                        | 76-81                                        | 90-92                                        | 94-95                                        | 87-94                                        | 69-73                                        | 78-86                                        |\\n| Table                 | 34733   | 3.20        | 2.27        | 3.60        | 77-81                                        | 75-80                                        | 83-86                                        | 98-99                                        | 58-80                                        | 79-84                                        | 70-85                                        |\\n| Text                  | 510377  | 45.82       | 49.28       | 45.00       | 84-86                                        | 81-86                                        | 88-93                                        | 89-93                                        | 87-92                                        | 71-79                                        | 87-95                                        |\\n| Title [22], a         | 5071    | 0.47        | 0.30        | 0.50        | 60-72                                        | 24-63                                        | 50-63                                        | 94-100                                       | 82-96                                        | 68-79                                        | 24-56                                        |\\n| Total in-             | 1107470 | 941123      | 99816       | 66531       | 82-83                                        | 71-74                                        | 79-81                                        | 89-94                                        | 86-91                                        | 71-76                                        | 68-85                                        |\\n\\n3\\n\\n,\\n\\ngovernment offices,\\n\\nWe reviewed the col-\\n\\n,\\n\\nPage-\\n\\nTitle and\\n\\n.\\n\\npage. Specificity ensures that the choice of label is not ambiguous,\\n\\n<!-- image -->\\n\\nwe distributed the annotation workload and performed continuous be annotated. We refrained from class labels that are very specific\\n\\nonly. For phases three and four, a group of 40 dedicated annotators while coverage ensures that all meaningful items on a page can\\n\\nquality controls. Phase one and two required a small team of experts to a document category, such as\\n\\nAbstract in the\\n\\nScientific Articles were assembled and supervised.\\n\\ncategory. We also avoided class labels that are tightly linked to the\\n\\nPhase 1: Data selection and preparation.\\n\\nOur inclusion cri-\\n\\nAuthor\\n\\nAffiliation\\n\\nteria for documents were described in Section 3. A large effort went into ensuring that all documents are free to use. The data sources in DocBank, are often only distinguishable by discriminating on 3 https://arxiv.org/ Figure 4: Table 1 from the DocLayNet paper in the original PDF (A), as rendered Markdown (B) and in JSON representation (C). Spanning table cells, such as the multi-column header \\'triple interannotator mAP@0.5-0.95 (%)\\', is repeated for each column in the Markdown representation (B), which guarantees that every data point can be traced back to row and column headings only by its grid coordinates in the table. In the JSON representation, the span information is reflected in the fields of each table cell (C).\\n\\nsemantics of the text. Labels such as and\\n\\n,\\n\\nas seen', path=None, url=None, mimetype=None), image_resource=None, audio_resource=None, video_resource=None, text_template='{metadata_str}\\n\\n{content}'),\n",
       " Document(id_='bb76898b-58fc-4228-8556-13963befdec8', embedding=None, metadata={}, excluded_embed_metadata_keys=[], excluded_llm_metadata_keys=[], relationships={}, metadata_template='{key}: {value}', metadata_separator='\\n', text_resource=MediaResource(embeddings=None, data=None, text='## DSPY: COMPILING DECLARATIVE LANGUAGE MODEL CALLS INTO SELF-IMPROVING PIPELINES\\n\\nOmar Khattab, 1 Arnav Singhvi, 2 Paridhi Maheshwari, 4 Zhiyuan Zhang, 1 Keshav Santhanam, 1 Sri Vardhamanan, 6 Saiful Haq, 6 Ashutosh Sharma, 6 Thomas T. Joshi, 7 Hanna Moazam, 8 Heather Miller, 3 9 , Matei Zaharia, 2 Christopher Potts 1\\n\\n- 1 Stanford University, 2 UC Berkeley, 3 Carnegie Mellon University,\\n- 4 Amazon Alexa AI, 5 Dashworks Technologies, Inc.,\\n- 6 IIT Bombay, 7 Calera Capital, 8 Microsoft, 9 Two Sigma Investments\\n\\nokhattab@cs.stanford.edu\\n\\n## ABSTRACT\\n\\nThe ML community is rapidly exploring techniques for prompting language models (LMs) and for stacking them into pipelines that solve complex tasks. Unfortunately, existing LM pipelines are typically implemented using hard-coded \\'prompt templates\\', i.e. lengthy strings discovered via trial and error. Toward a more systematic approach for developing and optimizing LM pipelines, we introduce DSPy, a programming model that abstracts LM pipelines as text transformation graphs , i.e. imperative computation graphs where LMs are invoked through declarative modules. DSPy modules are parameterized , meaning they can learn (by creating and collecting demonstrations) how to apply compositions of prompting, finetuning, augmentation, and reasoning techniques. We design a compiler that will optimize any DSPy pipeline to maximize a given metric. We conduct two case studies, showing that succinct DSPy programs can express and optimize sophisticated LM pipelines that reason about math word problems, tackle multihop retrieval, answer complex questions, and control agent loops. Within minutes of compiling, a few lines of DSPy allow GPT-3.5 and llama2-13b-chat to selfbootstrap pipelines that outperform standard few-shot prompting (generally by over 25% and 65%, respectively) and pipelines with expert-created demonstrations (by up to 5-46% and 16-40%, respectively). On top of that, DSPy programs compiled to open and relatively small LMs like 770M-parameter T5 and llama2-13b-chat are competitive with approaches that rely on expert-written prompt chains for proprietary GPT-3.5 .\\n\\nDSPy is available at https://github.com/stanfordnlp/dspy .\\n\\n## 1 INTRODUCTION\\n\\nLanguage models (LMs) are enabling researchers to build NLP systems at higher levels of abstraction and with lower data requirements than ever before (Bommasani et al., 2021). This is fueling an exploding space of \\'prompting\\' techniques-and lightweight finetuning techniques-for adapting LMs to new tasks (Kojima et al., 2022), eliciting systematic reasoning from them (Wei et al., 2022; Wang et al., 2022b), and augmenting them with retrieved sources (Guu et al., 2020; Lazaridou et al., 2022; Khattab et al., 2022) or with tools (Yao et al., 2022; Schick et al., 2023). Most of these techniques are explored in isolation, but interest has been growing in building multi-stage pipelines and agents that decompose complex tasks into more manageable calls to LMs in an effort to improve performance (Qi et al., 2019; Khattab et al., 2021a; Karpas et al., 2022; Dohan et al., 2022; Khot et al., 2022; Khattab et al., 2022; Chen et al., 2022; Pourreza &amp; Rafiei, 2023; Shinn et al., 2023).\\n\\nUnfortunately, LMs are known to be sensitive to how they are prompted for each task, and this is exacerbated in pipelines where multiple LM calls have to interact effectively. As a result, the LM\\n\\ncalls in existing LM pipelines and in popular developer frameworks are generally implemented using hard-coded \\'prompt templates\\', that is, long strings of instructions and demonstrations that are hand crafted through manual trial and error. We argue that this approach, while pervasive, can be brittle and unscalable-conceptually akin to hand-tuning the weights for a classifier. A given string prompt might not generalize to different pipelines or across different LMs, data domains, or even inputs.\\n\\nToward a more systematic approach to designing AI pipelines, we introduce the DSPy programming model. 1 DSPy pushes building new LM pipelines away from manipulating free-form strings and closer to programming (composing modular operators to build text transformation graphs) where a compiler automatically generates optimized LM invocation strategies and prompts from a program. We draw inspiration from the consensus that emerged around neural network abstractions (Bergstra et al., 2013), where (1) many general-purpose layers can be modularly composed in any complex architecture and (2) the model weights can be trained using optimizers instead of being hand-tuned.\\n\\nTo this end, we propose the DSPy programming model (Sec 3). We first translate string-based prompting techniques, including complex and task-dependent ones like Chain of Thought (Wei et al., 2022) and ReAct (Yao et al., 2022), into declarative modules that carry natural-language typed signatures . DSPy modules are task-adaptive components-akin to neural network layers-that abstract any particular text transformation, like answering a question or summarizing a paper. We then parameterize each module so that it can learn its desired behavior by iteratively bootstrapping useful demonstrations within the pipeline. Inspired directly by PyTorch abstractions (Paszke et al., 2019), DSPy modules are used via expressive define-by-run computational graphs. Pipelines are expressed by (1) declaring the modules needed and (2) using these modules in any logical control flow (e.g., if statements, for loops, exceptions, etc.) to logically connect the modules.\\n\\nWethen develop the DSPy compiler (Sec 4), which optimizes any DSPy program to improve quality or cost. The compiler inputs are the program, a few training inputs with optional labels, and a validation metric. The compiler simulates versions of the program on the inputs and bootstraps example traces of each module for self-improvement, using them to construct effective few-shot prompts or finetuning small LMs for steps of the pipeline. Optimization in DSPy is highly modular: it is conducted by teleprompters , 2 which are general-purpose optimization strategies that determine how the modules should learn from data. In this way, the compiler automatically maps the declarative modules to high-quality compositions of prompting, finetuning, reasoning, and augmentation.\\n\\nProgramming models like DSPy could be assessed along many dimensions, but we focus on the role of expert-crafted prompts in shaping system performance. We are seeking to reduce or even remove their role through DSPy modules (e.g., versions of popular techniques like Chain of Thought) and teleprompters. We report on two expansive case studies: math word problems (GMS8K; Cobbe et al. 2021) and multi-hop question answering (HotPotQA; Yang et al. 2018) with explorations of chain of thought, multi-chain reflection, multi-hop retrieval, retrieval-augmented question answering, and agent loops. Our evaluations use a number of different compiling strategies effectively and show that straightforward DSPy programs outperform systems using hand-crafted prompts, while also allowing our programs to use much smaller and hence more efficient LMs effectively.\\n\\nOverall, this work proposes the first programming model that translates prompting techniques into parameterized declarative modules and introduces an effective compiler with general optimization strategies (teleprompters) to optimize arbitrary pipelines of these modules. Our main contributions are empirical and algorithmic: with DSPy, we have found that we can implement very short programs that can bootstrap self-improving multi-stage NLP systems using LMs as small as llama2-13b-chat and T5-Large (770M parameters). Without hand-crafted prompts and within minutes to tens of minutes of compiling, compositions of DSPy modules can raise the quality of simple programs from 33% to 82% (Sec 6) and from 32% to 46% (Sec 7) for GPT-3.5 and, similarly, from 9% to 47% (Sec 6) and from 22% to 41% (Sec 7) for llama2-13b-chat .\\n\\n1 DSPy is pronounced dee-ess-pie . It\\'s the second iteration of our earlier Demonstrate-Search-Predict framework (DSP; Khattab et al. 2022). This paper introduces the key concepts in DSPy. For more extensive and up-to-date documentation of the framework, we refer readers to https://github.com/stanfordnlp/dspy .\\n\\n2 We derive the name teleprompters from the notion of abstracting and automating the task of prompting, in particular, such that it happens at a distance , without manual intervention.\\n\\n## 2 RELATED WORK\\n\\nThis work is inspired by the role that Torch (Collobert et al., 2002), Theano (Bergstra et al., 2010; 2011; Al-Rfou et al., 2016), Chainer (Tokui et al., 2015), and others played in the development in deep learning by providing powerful abstractions. A similar transformation is emerging with higherlevel pipelines of LMs, and we are seeking to offer a solid conceptual framework and programming abstractions for what we call foundation model programming . We draw on differentiable programming (Wang et al., 2018) but applied to LM calls rather than neural networks, and borrow syntactic elements from PyTorch (Paszke et al., 2019).\\n\\nIn-context learning (McCann et al. 2018; Radford et al. 2018; Brown et al. 2020) is a key mechanism for foundation model programming. A growing body of work has revealed that, especially with instruction tuning (Ouyang et al., 2022), we can elicit sophisticated behavior via prompting (Wei et al., 2022; Wang et al., 2022b; Press et al., 2022; Yao et al., 2022; Khot et al., 2022; Madaan et al., 2023). Similarly, forms of weak supervision that would normally require task-specific (Khattab et al., 2021a;b) or hand-built (Ratner et al., 2016; Hancock et al., 2018) heuristics are now done by LMs (Wang et al., 2022b; Zelikman et al., 2022; Zhang et al., 2022; Shao et al., 2023).\\n\\nIn-context learning methods now routinely invoke tools, leading to LM pipelines that use retrieval models (Chen et al., 2017; Lewis et al., 2020; Guu et al., 2020; Lazaridou et al., 2022; Izacard et al., 2022), multimodal foundation models, and more traditional tools like APIs (Nakano et al., 2021) and calculators. A number of toolkits have been developed to facilitate this, including LangChain (Chase, 2022), Semantic Kernel (Microsoft, 2023), LlamaIndex (Liu, 2022), and many other retrieval and agent libraries. These toolkits provide pre-packaged chains and agents that connect LMs with numerous accessible tools. However, they suffer from the pervasive prompt engineering challenges we address in DSPy: they express task-specific behavior through hand-written prompt templates (for detailed discussion, see Appendix B).\\n\\nResearchers are starting to apply discrete optimization and RL to find effective prompts, generally for a single logical LM call (Guo et al., 2023; Pryzant et al., 2023; Huang et al., 2022; Yang et al., 2023). DSPy seeks to generalize this space: it offers a rich framework for optimizing arbitrary pipelines from high-level declarative signatures , by bootstrapping high-quality multi-stage demonstrations with constraints. In this framework, DSPy teleprompters may apply optimization using model selection techniques like cross-validation or, in principle, with sophisticated techniques involving RL and LM feedback (Hu et al., 2023; Zhao et al., 2023a; Shinn et al., 2023) or learned or Bayesian hyperparameter optimization methods (Bergstra et al., 2013; Akiba et al., 2019).\\n\\nThe present paper seeks to motivate DSPy as a programming model and to report new empirical findings from applying the DSPy compiler. This is inspired by formative work by Bergstra et al. (2010; 2013), Paszke et al. (2019), and Wolf et al. (2020), who support their respective programming models with a mix of benchmark numbers and some qualitative measures. For the current paper, we focus on showing that DSPy and its compiler allow us to build outstanding LM systems without hand-crafted prompt strings, but instead from truly modular units, and that this opens up doors for systematically exploring a rich design space at a very high programmatic level of abstraction.\\n\\n## 3 THE DSPY PROGRAMMING MODEL\\n\\nWe present DSPy, which treats LMs as abstract devices for text generation, 3 and optimizes their usage in arbitrary computational graphs. DSPy programs are expressed in Python: each program takes the task input (e.g., a question to answer or a paper to summarize) and returns the output (e.g., an answer or a summary) after a series of steps. DSPy contributes three abstractions toward automatic optimization: signatures, modules, and teleprompters. Signatures abstract the input/output behavior of a module; modules replace existing hand-prompting techniques and can be composed in arbitrary pipelines; and teleprompters optimize all modules in the pipeline to maximize a metric.\\n\\n3 We assume access to one or more LMs, which consume a prompt string and return text completions. This may be a promptable LM capable of in-context learning (e.g., GPT-3.5 or Llama2-7b) or a smaller finetuneable LM (e.g., T5-base). An LM may be selected as the default; operations will use it unless configured otherwise.\\n\\n## 3.1 NATURAL LANGUAGE SIGNATURES CAN ABSTRACT PROMPTING &amp; FINETUNING\\n\\nInstead of free-form string prompts, DSPy programs use natural language signatures to assign work to the LM. A DSPy signature is natural-language typed declaration of a function: a short declarative spec that tells DSPy what a text transformation needs to do (e.g., \\'consume questions and return answers\\'), rather than how a specific LM should be prompted to implement that behavior. More formally, a DSPy signature is a tuple of input fields and output fields (and an optional instruction ). Afield consists of field name and optional metadata. 4 In typical usage, the roles of fields are inferred by DSPy as a function of field names. For instance, the DSPy compiler will use in-context learning to interpret question differently from answer and will iteratively refine its usage of these fields.\\n\\nSignatures offer two benefits over prompts: they can be compiled into self-improving and pipelineadaptive prompts or finetunes. This is primarily done by bootstrapping (Sec 4) useful demonstrating examples for each signature. Additionally, they handle structured formatting and parsing logic to reduce (or, ideally, avoid) brittle string manipulation in user programs.\\n\\nIn practice, DSPy signatures can be expressed with a shorthand notation like question -&gt; answer , so that line 1 in the following is a complete DSPy program for a basic question-answering system (with line 2 illustrating usage and line 3 the response when GPT-3.5 is the LM):\\n\\n```\\n1 qa = dspy.Predict(\"question -> answer\") 2 qa(question=\"Where is Guaran´ ı spoken?\") 3 # Out: Prediction(answer=\\'Guaran´ ı is spoken mainly in South America.\\')\\n```\\n\\nIn the shorthand notation, each field\\'s name indicates the semantic role that the input (or output) field plays in the transformation. DSPy will parse this notation and expand the field names into meaningful instructions for the LM, so that english document -&gt; french translation would prompt for English to French translation. When needed, DSPy offers more advanced programming interfaces for expressing more explicit constraints on signatures (Appendix A).\\n\\n## 3.2 PARAMETERIZED &amp; TEMPLATED MODULES CAN ABSTRACT PROMPTING TECHNIQUES\\n\\nAkin to type signatures in programming languages, DSPy signatures simply define an interface and provide type-like hints on the expected behavior. To use a signature, we must declare a module with that signature, like we instantiated a Predict module above. A module declaration like this returns a function having that signature.\\n\\nThe Predict Module The core module for working with signatures in DSPy is Predict (simplified pseudocode in Appendix D.1). Internally, Predict stores the supplied signature, an optional LM to use (initially None , but otherwise overrides the default LM for this module), and a list of demonstrations for prompting (initially empty). Like layers in PyTorch, the instantiated module behaves as a callable function: it takes in keyword arguments corresponding to the signature input fields (e.g., question ), formats a prompt to implement the signature and includes the appropriate demonstrations, calls the LM, and parses the output fields. When Predict detects it\\'s being used in compile mode, it will also internally track input/output traces to assist the teleprompter at bootstrapping the demonstrations.\\n\\nOther Built-in Modules DSPy modules translate prompting techniques into modular functions that support any signature, contrasting with the standard approach of prompting LMs with task-specific details (e.g., hand-written few-shot examples). To this end, DSPy includes a number of more sophisticated modules like ChainOfThought , ProgramOfThought , MultiChainComparison , and ReAct . 5 These can all be used interchangeably to implement a DSPy signature. For instance, simply chang-\\n\\n4 String descriptions of the task and the fields are also optional and usually omitted. Fields can carry optional field prefix and description . By default, fields are assumed to hold free-form strings; we are actively exploring optional data type as a way to specify constraints on valid values (e.g., bool or int ) and more gracefully handle formatting and parsing logic, though this feature is not core to DSPy at the time of writing.\\n\\n5 These modules generalize prompting techniques from the literature, respectively, by Wei et al. (2022), Chen et al. (2022), Yoran et al. (2023), and Yao et al. (2022) and, in doing so, generalize the ideas on zero-shot prompting and rationale self-generation from Kojima et al. (2022), Zelikman et al. (2022), Zhang et al. (2022), and Huang et al. (2022) to parameterized modules that can bootstrap arbitrary multi-stage pipelines.\\n\\ning Predict to ChainOfThought in the above program leads to a system that thinks step by step before committing to its output field.\\n\\nImportantly, all of these modules are implemented in a few lines of code by expanding the userdefined signature and calling Predict one or more times on new signatures as appropriate. For instance, we show a simplified implementation of the built-in ChainOfThought below.\\n\\n```\\n1 class ChainOfThought(dspy.Module): 2 def __init__(self, signature): 3 # Modify signature from \\'*inputs -> *outputs \\' to \\'*inputs -> rationale , *outputs \\'. 4 rationale_field = dspy.OutputField(prefix=\"Reasoning: Let\\'s think step by step.\") 5 signature = dspy.Signature(signature).prepend_output_field(rationale_field) 6 7 # Declare a sub-module with the modified signature. 8 self.predict = dspy.Predict(signature) 9 10 def forward(self, **kwargs): 11 # Just forward the inputs to the sub-module. 12 return self.predict(**kwargs)\\n```\\n\\nThis is a fully-fledged module capable of learning effective few-shot prompting for any LM or task. We contrast that with Appendix C, which copies long reasoning prompts hand-written by sources ranging from recent research to popular prompting libraries.\\n\\nParameterization Uniquely, DSPy parameterizes these prompting techniques. To understand this parameterization, observe that any LM call seeking to implement a particular signature needs to specify parameters that include: (1) the specific LM to call (Chen et al., 2023), (2) the prompt instructions (Yang et al., 2023) and the string prefix of each signature field and, most importantly, (3) the demonstrations used as few-shot prompts (for frozen LMs) or as training data (for finetuning). We focus primarily on automatically generating and selecting useful demonstrations. In our case studies, we find that bootstrapping good demonstrations gives us a powerful way to teach sophisticated pipelines of LMs new behaviors systematically.\\n\\nTools DSPy programs may use tools, which are modules that execute computation. We support retrieval models through a dspy.Retrieve module. At the time of writing, DSPy has built-in support for ColBERTv2, Pyserini, and Pinecone retrievers, and we have explored experimental dspy.SQL for executing SQL queries and dspy.PythonInterpreter for executing Python code in a sandbox.\\n\\nPrograms DSPy modules can be composed in arbitrary pipelines in a define-by-run interface. Inspired directly by PyTorch and Chainer, one first declares the modules needed at initialization, allowing DSPy to keep track of them for optimization, and then one expresses the pipeline with arbitrary code that calls the modules in a forward method. As a simple illustration, we offer the following simple but complete retrieval-augmented generation (RAG) system.\\n\\n```\\n1 class RAG(dspy.Module): 2 def __init__(self, num_passages=3): 3 # \\'Retrieve \\' will use the user\\'s default retrieval settings unless overriden. 4 self.retrieve = dspy.Retrieve(k=num_passages) 5 # \\'ChainOfThought \\' with signature that generates answers given retrieval & question. 6 self.generate_answer = dspy.ChainOfThought(\"context , question -> answer\") 7 8 def forward(self, question): 9 context = self.retrieve(question).passages 10 return self.generate_answer(context=context , question=question)\\n```\\n\\nTo highlight modularity, we use ChainOfThought as a drop-in replacement of the basic Predict . One can now simply write RAG()(\"Where is Guaran´ ı spoken?\") to use it. Notice that, if we use a signature \"context, question -&gt; search query\" , we get a system that generates search queries rather than answers.\\n\\n## 3.3 TELEPROMPTERS CAN AUTOMATE PROMPTING FOR ARBITRARY PIPELINES\\n\\nWhen compiling a DSPy program, we generally invoke a teleprompter , which is an optimizer that takes the program, a training set, and a metric-and returns a new optimized program. Different teleprompters (Sec 4) apply different strategies for optimization.\\n\\nIn DSPy, training sets may be small , potentially a handful of examples, though larger data enables more powerful optimization. Training examples may be incomplete , i.e., only input values are necessary. Labels for the pipeline steps are not required, unless they need to be used in the metric. In practice, we typically assume labels only for (at most) the program\\'s final output, not the intermediate steps. This label-efficiency is critical for modularity: building a new pipeline in DSPy requires simply recompiling the new pipeline\\'s code, not annotating data specific to the new pipeline.\\n\\nMetrics can be simple notions like exact match (EM) or F1, but they can be entire DSPy programs that balance multiple concerns. For example, we may compile the RAG module above against a dataset of question-answer pairs qa trainset and the metric EM. The goal of optimization here is to effectively bootstrap few-shot demonstrations. The following code achieves this:\\n\\n```\\n1 # Small training set with only questions and final answers. 2 qa_trainset = [dspy.Example(question=\"What is the capital of France?\", answer=\"Paris\")] 3 4 # The teleprompter will bootstrap missing labels: reasoning chains and retrieval contexts. 5 teleprompter = dspy.BootstrapFewShot(metric=dspy.evaluate.answer_exact_match) 6 compiled_rag = teleprompter.compile(RAG(), trainset=qa_trainset)\\n```\\n\\nIn this example, the BootstrapFewShot teleprompter (Sec 4, Appendix E.1) simulates RAG on the training example(s). It will collect demonstrations of each module (i.e., examples of its input-output behavior) that collectively lead to valid output (i.e., respecting the signatures and the metric).\\n\\nIf one wanted to push the compiled program to be extractive given its retrieved contexts, one could define a custom metric to use in place of dspy.evaluate.answer exact match :\\n\\n```\\n1 2 3 4 5 6 7\\n```\\n\\n```\\ndef answer_and_context_match(example , pred , trace=None): answer_match = dspy.evaluate.answer_exact_match(example , pred) # Is the prediction a substring of some passage? context_match = any((pred.answer.lower() in c) for c in pred.context) return answer_match and context_match\\n```\\n\\nNotice that behavior like this might be more accurately checked by another DSPy program that checks for faithful grounding of answers. Such metrics are fully supported and encouraged in DSPy.\\n\\nTeleprompters can be composed by specifying a teacher program. DSPy will sample demonstrations from this program for prompt optimization. This composition can enable very rich pipelines, where expensive programs (e.g., complex expensive ensembles using large LMs) supervise cheap programs (e.g., simple pipelines using smaller LMs). One may start with compiled rag from above (say, compiled to use a large Llama2-13b-chat LM) but now fine-tune Flan-T5-large to create an efficient program:\\n\\n```\\n1 # Larger set of questions with *no labels*. Labels for all steps will be bootstrapped. 2 unlabeled_questions = [dspy.Example(question=\"What is the capital of Germany?\"), ...] 3 4 # As we assumes no answer , we use \\'answer_passage_match \\' to filter ungrounded answers. 5 finetuning_teleprompter = BootstrapFinetune(metric=dspy.evaluate.answer_passage_match) 6 7 # We set \\'teacher=compiled_rag \\' to compose. Bootstrapping will now use \\'compiled_rag \\'. 8 compiled_rag_via_finetune = finetuning_teleprompter.compile(RAG(), teacher=compiled_rag , trainset=unlabeled_questions , target=\\'google/flan-t5-large\\')\\n```\\n\\n## 4 THE DSPY COMPILER\\n\\nA key source of DSPy\\'s expressive power is its ability to compile-or automatically optimize-any program in this programming model. Compiling relies on a teleprompter, which is an optimizer for DSPy programs that improves the quality (or cost) of modules via prompting or finetuning, which are unified in DSPy. While DSPy does not enforce this when creating new teleprompters, typical teleprompters go through three stages.\\n\\nStage 1: Candidate Generation The compiler first (recursively) finds all unique Predict modules (predictors) in a program, including those nested under other modules. For each unique predictor p , the teleprompter may generate candidate values for the parameters of p : the instructions, field descriptions, or-most importantly-demonstrations (i.e., example input-output pairs). In this iter-\\n\\nation of DSPy, we focus on demonstrations and find that simple rejection-sampling-like approaches can help bootstrap highly effective multi-stage systems.\\n\\nConsider the simplest non-trivial teleprompter in DSPy, BootstrapFewShot (simplified pseudocode in Appendix E.1). This teleprompter will simulate a teacher program (or, if unset, the zero-shot version of the program being compiled) on some training inputs, possibly one or more times with a high temperature. When running in compile mode, multi-stage traces are tracked transparently and in a thread-safe fashion throughout execution. The program\\'s metric is used to filter for multistage traces that together help the pipeline pass the metric. We thus obtain potential labels for all signatures in the program by throwing away the bad examples and using the good examples as potential demonstrations, though these design decisions are under user control.\\n\\nWhile LMs can be highly unreliable, we find they can be rather efficient at searching the space of solutions for multi-stage designs. A well-decomposed program can typically find at least a few training examples where the LM can pass the constraints enforced by the signatures and metrics, allowing us to bootstrap iteratively if needed.\\n\\nStage 2: Parameter Optimization Now each parameter has a discrete set of candidates: demonstrations, instructions, etc. Many hyperparameter tuning algorithms (e.g., random search or Treestructured Parzen Estimators as in HyperOpt (Bergstra et al., 2013) and Optuna (Akiba et al., 2019)) can be applied for selection among candidates. We report simplified implementations of DSPy\\'s BootstrapFewShotWithRandomSearch and BootstrapFewShotWithOptuna in Appendix E.2 and Appendix E.3.\\n\\nAnother type of optimization is finetuning with BootstrapFinetune , where the demonstrations are used to update the LM\\'s weights for each predictor. When this is applied, the LM parameter of each module is updated to the new LM weights. Typically, we are optimizing average quality using the metric with cross-validation over the training set or a validation set. This is applicable even with no labels for any stages, depending on the nature of metric.\\n\\nStage 3: Higher-Order Program Optimization A different type of optimization that the DSPy compiler supports is modifying the control flow of the program. One of the simplest forms of these is ensembles, which we use in the case studies in this work. An ensemble will bootstrap multiple copies of the same program, and then replace the program with a new one that runs them all in parallel and reduces their predictions into one with a custom function (e.g., majority voting). In future work, this stage can easily accommodate techniques for more dynamic (i.e., test-time) bootstrapping as well as automatic backtracking-like logic.\\n\\n## 5 GOALS OF EVALUATION\\n\\nProgramming frameworks can be evaluated along many dimensions: computational efficiency, developer efficiency, intuitiveness of the code and concepts, and so forth. In this paper, we focus on perhaps the most pressing issue for current LM pipelines: the role of hand-written, task-specific prompts in achieving performant systems. Our evaluations seek to test the following hypotheses:\\n\\n- H1 With DSPy, we can replace hand-crafted prompt strings with concise and well-defined modules, without reducing quality or expressive power.\\n- H2 Parameterizing the modules and treating prompting as an optimization problem makes DSPy better at adapting to different LMs, and it may outperform expert-written prompts.\\n- H3 The resulting modularity makes it possible to more thoroughly explore complex pipelines that have useful performance characteristics or that fit nuanced metrics.\\n\\nOur evaluation will explore these hypotheses using diverse task-program pairs. We hope this begins a shift from underspecified questions like \\'how do different LMs compare on GSM8K\\' toward \\'how they compare on GSM8K with program P when compiled with strategy S\\', which is a well-defined and reproducible run. Ultimately, our goal is to reduce the role of artful prompt construction in modern AI in favor of the development of new modular, composable programs and optimizers.\\n\\nTable 1: Results with in-context learning on GSM8K math word problems. Each row represents a separate pipeline: the module in the Program column is compiled against the examples in the Training set. The programs, compilers, and (small) training sets are defined in Section 6. Rows with ensemble build on the immediately preceding row. Notably, all programs in this table are expressed by composing two to four DSPy modules and teleprompters. Compiling the correct modules , instead of string prompts, improves different LMs from 4-20% accuracy to 49-88% accuracy.\\n\\n|            |               |             | GPT-3.5   | GPT-3.5   | Llama2-13b-chat   | Llama2-13b-chat   |\\n|------------|---------------|-------------|-----------|-----------|-------------------|-------------------|\\n| Program    | Compilation   | Training    | Dev       | Test      | Dev               | Test              |\\n| vanilla    | none          | n/a         | 24.0      | 25.2      | 7.0               | 9.4               |\\n|            | fewshot       | trainset    | 33.1      | -         | 4.3               | -                 |\\n|            | bootstrap     | trainset    | 44.0      | -         | 28.0              | -                 |\\n|            | bootstrap × 2 | trainset    | 64.7      | 61.7      | 37.3              | 36.5              |\\n|            | + ensemble    | trainset    | 62.7      | 61.9      | 39.0              | 34.6              |\\n|            | none          | n/a         | 50.0      | -         | 26.7              | -                 |\\n|            | fewshot       | trainset    | 63.0      | -         | 27.3              | -                 |\\n| CoT        | fewshot       | + human CoT | 78.6      | 72.4      | 34.3              | 33.7              |\\n|            | bootstrap     | trainset    | 80.3      | 72.9      | 43.3              | -                 |\\n|            | + ensemble    | trainset    | 88.3      | 81.6      | 43.7              | -                 |\\n| reflection | none          | n/a         | 65.0      | -         | 36.7              | -                 |\\n|            | fewshot       | trainset    | 71.7      | -         | 36.3              | -                 |\\n|            | bootstrap     | trainset    | 83.0      | 76.0      | 44.3              | 40.2              |\\n|            | + ensemble    | trainset    | 86.7      | -         | 49.0              | 46.9              |\\n\\n## 6 CASE STUDY: MATH WORD PROBLEMS\\n\\nWe evaluate on the popular GSM8K dataset with grade school math questions (Cobbe et al., 2021). Wesample 200 and 300 question-answer pairs from the official training set for training and development, respectively. Our final evaluations use the 1.3k official test set examples. We report extensive comparisons on the development set to avoid overfitting on test. Following prior work on GSM8K, we evaluate the accuracy of the final numerical value that appears in the LM output.\\n\\nPrograms Considered For this task, we consider three simple DSPy programs: a one-step Predict module ( vanilla ), a two-step ChainOfThought module ( CoT ), and finally a multi-stage ComparerOfThoughts module ( ThoughtReflection ). These are fully defined by the following code:\\n\\n```\\n1 vanilla = dspy.Predict(\"question -> answer\") # GSM8K Program \\'vanilla \\' 2 3 CoT = dspy.ChainOfThought(\"question -> answer\") # GSM8K Program \\'CoT\\' 1 class ThoughtReflection(dspy.Module): 2 def __init__(self, num_attempts): 3 self.predict = dspy.ChainOfThought(\"question -> answer\", n=num_attempts) 4 self.compare = dspy.MultiChainComparison(\\'question -> answer\\', M=num_attempts) 5 6 def forward(self, question): 7 completions = self.predict(question=question).completions 8 return self.compare(question=question , completions=completions) 9 10 reflection = ThoughtReflection(num_attempts=5) # GSM8K Program \\'reflection \\'\\n```\\n\\nIn reflection , five reasoning chains are sampled from the LM (alongside their answers) and they are compared in parallel by a built-in MultiChainComparison module, which generalizes Yoran et al. (2023). This generates a new answer taking into account the patterns from the five attempts. Critically, the modules used are all generic, none is specific math problems or particular LM.\\n\\nCompiling As we discussed in Section 4, DSPy programs can be compiled into new, optimized programs. In our experiments, we evaluate the programs zero-shot (no compiling) as well as a number of strategies for compiling. Our simplest compiler is LabeledFewShot :\\n\\n```\\n1 fewshot = dspy.LabeledFewShot(k=8).compile(program , trainset=trainset)\\n```\\n\\nHere, program can be any DSPy module. This simply samples k=8 random demonstrations from the trainset for the fields common to the training examples and the signature(s), in this case, question and answer , but not the reasoning for instance. We report the average of 3-5 runs (depending on the setting) when applying such random sampling.\\n\\nNext, we also consider bootstrapping few-shot examples with random search:\\n\\n```\\n1 tp = BootstrapFewShotWithRandomSearch(metric=gsm8k_accuracy) 2 bootstrap = tp.compile(program , trainset=trainset , valset=devset)\\n```\\n\\nThis will generate demonstration chains for examples in the training set and optimize the selection of demonstrations (from this set) to self-improve the program\\'s modules. As the name indicates, this is done with random search, treating the selection of demonstrations as a parameter to optimize.\\n\\nNext, if desired, this bootstrapping process can be nested in DSPy. In particular, we can use the optimized bootstrap program itself to further bootstrap another program. This is relevant, for example, whenever the original zero-shot program performs relatively poorly.\\n\\n```\\n1\\n```\\n\\n```\\nbootstrap2 = tp.compile(program , teacher=bootstrap , trainset=trainset , valset=devset)\\n```\\n\\n## And lastly, we consider ensembling these bootstraps:\\n\\n```\\n# A program that ensembles the top -7 candidate programs from a bootstrapping compiler run\\n```\\n\\n```\\n1 (in particular \\'bootstrap \\' or, when applicable , \\'bootstrap2 \\') with majority voting. 2 ensemble = Ensemble(reduce_fn=dspy.majority).compile(bootstrap.programs[:7])\\n```\\n\\nGSM8K includes human reasoning chains. Above, trainset does not include these reasoning chains. We also evaluate with trainset human CoT , which extends the examples in trainset with the human reasoning string. These two datasets can be used interchangeably as the value for the trainset parameter above. We note here that compiling generally runs on the order of minutes (or tens of minutes) as even the more expensive settings only require running the program a few thousand times (e.g., 10-20 trials over 150-300 validation examples) and they can occur in parallel.\\n\\nResults Our results are summarized in Table 1, which includes dev results as well as our evaluation of promising representatives of each approach on the test set. First, the vanilla program results show that GPT-3.5 and llama2-13b-chat struggle with math word problems when they have to predict the answers directly, that is, without using a reasoning chain first. This is most pronounced in the absence of good demonstrations, which can be seen in the none compilation setting (i.e., zero-shot instruction) and the fewshot setting (i.e., sampling random question-answer pairs). Interestingly, however, vanilla is helped substantially by compiling with bootstrap and by iterating this process into bootstrap × 2 . On inspecting the prompts bootstrapped (Appendix F), we see that the prompt allows the LM to leverage the answer field for reasoning first, which is permitted as the metric extracts the final numerical value for evaluation.\\n\\nNext, we consider the CoT program. While the expert human reasoning chains ( +human CoT ) provide a large boost when available, we can match or surpass this using bootstrap , substantiating our hypothesis that DSPy can cut the need for hand-crafted prompts. Beyond this, we see that the reflection program, while only a few lines longer than the others, is a clear winner, though CoT is quite effective with ensemble . Overall, the bootstrap compilation procedure leads to large gains for every program, across both LMs. Indeed, all programs in this table are expressed by composing two to four DSPy modules and teleprompters, and they reveal overall that-in the new paradigm prescribed by DSPy-it\\'s composing the right generic modules , rather than manipulating string prompts, that improves different LMs from 4-20% accuracy to 49-88% accuracy.\\n\\nWe can informally compare with the following. Zhang et al. (2022) reports 48% for text-davinci-002 , which aligns closely with our llama2-13b-chat results, and reports 59.4% with codex when employing a manual CoT approach and 62.8% with an automatic CoT method. Wang et al. (2022b) report 57% for CoT prompting with PaLM 540-B, which becomes 74% upon adding self-consistency. The Llama2 authors (Touvron et al., 2023) presents 28.7% for llama2-13b , 42.2% for llama2-34b , and 56.8% for llama2-70b . Intriguingly, our program with the 13b variant of the model is competitive with their 34b-based results even though we don\\'t use human reasoning chains in our program. Zhao et al. (2023b) reports 80.8% for CoT with gpt-3.5-turbo from April 2023. The GPT-4 authors (OpenAI, 2023) reports that GPT-3.5 scores 57.1% and GPT-4 elevates this to 92% but they note that GPT-4 was in fact pre-trained on a subset of GSM8K\\'s training set.\\n\\n## 7 CASE STUDY: COMPLEX QUESTION ANSWERING\\n\\nIn this case study, we explore the multi-hop question answering task with the HotPotQA (Yang et al., 2018) dataset in the open-domain \\'fullwiki\\' setting. For retrieval, we use a search index of the official Wikipedia 2017 \\'abstracts\\' dump of HotPotQA. Search is conducted by a ColBERTv2 (Santhanam et al., 2021) retriever. The HotPotQA test set is hidden, so we reserve the official validation set for our testing, and sample 1000 examples for that. We sub-divide the training set into 70%/30% train/validation splits. In the training (and thus validation) split, we keep only examples marked as \\'hard\\' in the original dataset, which matches the designation of the official validation and test sets. For training and for reporting development results, we sample 200 and 300 examples respectively.\\n\\nPrograms Considered Our simplest baseline is the vanilla program used in the previous case study on GSM8K (Sec 6); the \"question -&gt; answer\" signature is universal enough that it will work for this task (and many others) when compiled appropriately.\\n\\nOur baseline RAG program is the one given in Section 3.2 as a simple example of RAG with a dspy.ChainOfThought layer. We will see that this program does not excel at HotPotQA, and this motivates us to evaluate two multi-hop programs.\\n\\nTo that end, we first test ReAct (Yao et al., 2022), a multi-step agent for tool use, which is implemented as a built-in module in DSPy. In the simplest case, a ReAct module for a particular signature can be declared as follows in DSPy:\\n\\n1 react = dspy.ReAct(\"question -&gt; answer\", tools=[dspy.Retrieve(k=1)], max\\\\_iters=5) Wealso test the following custom program, which simulates the information flow in Baleen (Khattab et al., 2021a) and IRRR (Qi et al., 2020) and has similarities to IRCoT (Trivedi et al., 2022). 1 class BasicMultiHop(dspy.Module): 2 def \\\\_\\\\_init\\\\_\\\\_(self, passages\\\\_per\\\\_hop): 3 self.retrieve = dspy.Retrieve(k=passages\\\\_per\\\\_hop) 4 self.generate\\\\_query = dspy.ChainOfThought(\"context , question -&gt; search\\\\_query\") 5 self.generate\\\\_answer = dspy.ChainOfThought(\"context , question -&gt; answer\") 6 7 def forward(self, question): 8 context = [] 9 10 for hop in range(2): 11 query = self.generate\\\\_query(context=context , question=question).search\\\\_query 12 context += self.retrieve(query).passages 13 14 return self.generate\\\\_answer(context=context , question=question) 15 16 multihop = BasicMultiHop(passages\\\\_per\\\\_hop=3)\\n\\nCompiling For compilers, we continue to use the ones that we used for GSM8K (see Sec 6). We also consider two compositions of our teleprompters. For ReAct, we consider bootstrapping with BootstrapFewShotWithRandomSearch starting from an earlier bootstrap of the ReAct program. For the simple multihop program, we also consider fine-tuning with T5-Large starting from the earlier bootstrap of that program.\\n\\n```\\n1 multihop_t5 = dspy.BootstrapFinetune(metric=answer_exact_match).compile(program , teacher=bootstrap , trainset=trainset , target=\\'t5-large\\')\\n```\\n\\nResults Table 2 summarizes our results. Compared with the vanilla few-shot prompting, a chainof-thought and retrieval-augmented generation ( CoT RAG ) program can self-bootstrap in DSPy to increase answer EM substantially. However, this relies entirely on the ColBERTv2 retriever to find relevant passages directly from the original questions, limiting its passage recall. This is tackled in the react and multihop programs, which will generate queries for the retriever in multiple iterative \\'hops\\'. Indeed, overall, a simple multihop program performs the best, and in general bootstrap again proves to be very effective at raising its quality relative to its fewshot variant for both LMs.\\n\\nIn particular, we can see that bootstrap (and/or bootstrap × 2 ) can outperform both fewshot prompting (for multihop ) and expert human reasoning (for react ; adapted slightly from Yao et al. (2022) to our retrieval setting). Perhaps most importantly, we can make llama2-13b-chat competitive with GPT-3.5 by simply compiling our programs.\\n\\nTo assess the finetuning capacity of DSPy, we also evaluated the compiler multihop t5 defined above which produces a T5-Large (770M parameter) model. This program scores 39.3% answer EM and 46.0% passage accuracy on the dev set, using only 200 labeled inputs and 800 unlabeled\\n\\nTable 2: Results with in-context learning on HotPotQA multi-hop retrieval question answering. We report answer exact match (Ans) and pair-retrieval accuracy (Psg). Each row represents a separate pipeline: the module in the Program column is compiled against the examples in the Training set. The programs, compilers, and (small) training sets are defined in the main text. For HotPotQA, we use the training set (and not dev) directly for cross-validation. ∗ The marked result is evaluated on 50% of our test set due to cost.\\n\\n|          |               | GPT-3.5   | GPT-3.5   | GPT-3.5   | GPT-3.5   | Llama2-13b-chat   | Llama2-13b-chat   | Llama2-13b-chat   | Llama2-13b-chat   |\\n|----------|---------------|-----------|-----------|-----------|-----------|-------------------|-------------------|-------------------|-------------------|\\n| Program  | Compiler      | Dev       | Dev       | Test      | Test      | Dev               | Dev               | Test              | Test              |\\n|          |               | Ans       | Psg       | Ans       | Psg       | Ans               | Psg               | Ans               | Psg               |\\n| vanilla  | fewshot       | 34.3      | n/a       | 31.5      | n/a       | 27.5              | n/a               | 21.8              | n/a               |\\n| CoT RAG  | fewshot       | 36.4      | 36.0      | 29.8      | 34.4      | 34.5              | 36.0              | 28.0              | 34.4              |\\n| CoT RAG  | bootstrap     | 42.3      | 36.0      | -         | -         | 38.3              | 36.0              | 32.9              | 34.4              |\\n| react    | none          | 20.3      | -         | -         | -         | 20.0              | -                 | -                 | -                 |\\n|          | +human r      | 33.0      | -         | -         | -         | 28.3              | -                 | -                 | -                 |\\n|          | bootstrap     | 31.0      | -         | -         | -         | 24.7              | -                 | -                 | -                 |\\n|          | bootstrap × 2 | 39.0      | -         | -         | -         | 40.0              | -                 | -                 | -                 |\\n| multihop | fewshot       | 36.9      | 38.3      | 31.2      | 40.8      | 34.7              | 32.0              | 31.3              | 30.8              |\\n|          | bootstrap     | 48.7      | 47.0      | 39.6      | 43.8      | 42.0              | 48.3              | 36.4              | 43.5              |\\n|          | ensemble      | 54.7      | -         | 45.6 ∗    | -         | 50.0              | -                 | 41.0              | -                 |\\n\\nquestions. For compiling, we use a teacher program consisting of an ensemble (union) of two multihop with llama2-13b-chat . Considering its extremely small size and local availability, this compiled program with T5-Large would impose orders of magnitude lower costs for inference than a proprietary LM like GPT-3.5.\\n\\nOur results may be pegged against the evaluation on HotPotQA in a number of recent papers, though there is significant variation in evaluation methodology and test set samples across studies in this space. Using CoT prompting, Si et al. (2022) achieve 25.2% EM. With a \\'recite-and-answer\\' technique that uses PaLM-62B (Chowdhery et al., 2022) to recite evidence passages, Sun et al. (2022) achieve 26.5% EM. Wang et al. (2022a) achieve 33.8% EM and 44.6% F1 when applying selfconsistency for PaLM-540B. Yao et al. (2022) achieve 27.4% EM using ReAct with PaLM-540B and 30.8 with text-davinci-002 , with a tool giving it the ability for search using a Wikipedia API. They push their PaLM results to 35.1% EM by applying an additional CoT step with selfconsistency, which may resemble our ensemble approach in the sense of aggregating multiple answers. Trivedi et al. (2022) reports 49% using a pipeline with code-davinci-002 LM on a sample of 500 HotPotQA questions.\\n\\n## 8 CONCLUSION\\n\\nThis paper introduced DSPy, a new programming model for designing AI systems using pipelines of pretrained LMs and other tools. We presented three new concepts introduced in this abstraction (DSPy signatures, modules, and teleprompters), and showed in two very different case studies that it supports rapid development of highly effective systems that use relatively small LMs. We have maintained open-source versions of this framework for close to a year. In this period, we have seen and created a large number of programs that were compiled to high-quality systems by DSPy, spanning tasks from information extraction to low-resource synthetic data generation. In the interest of space and to maintain reasonable scope in this paper, we leave reporting on such tasks under controlled experimental conditions to future work. While in-context learning has proved transformative over the past 2-3 years of LM research, we argue that the true expressive power in this emerging paradigm is in building sophisticated text transformation graphs in which composable modules and optimizers (teleprompters) come together to leverage LMs in more systematic and reliable ways.\\n\\n## ACKNOWLEDGMENTS\\n\\nWe thank Josh Purtell for suggesting the apt name \\'text transformation graph\\' for the computational graph abstraction of DSPy. We thank Rick Battle, Igor Kotenkov, Lisa Li, David Hall, Ashwin Paranjape, Chris Manning, Percy Liang, and many researchers, developers, and users for valuable\\n\\ndiscussions and feedback. We thank Giuseppe Attanasio for his public LT E X GitHub-style Python A code formatting gist. 6\\n\\nThis work was partially supported by IBM as a founding member of the Stanford Institute for Human-Centered Artificial Intelligence (HAI), Oracle, Virtusa, and Cigna Healthcare. It was also partially supported by an HAI Azure compute grant. This research was supported in part by affiliate members and other supporters of the Stanford DAWN project-Facebook, Google, and VMware-as well as the NSF under CAREER grant CNS-1651570. Any opinions, findings, and conclusions or recommendations expressed in this material are those of the authors and do not necessarily reflect the views of the National Science Foundation. Omar Khattab is supported by the Apple Scholars in AI/ML fellowship.\\n\\n\\\\usepackage[pdftex]{graphicx} ... \\\\includegraphics[width=0.8\\\\linewidth]{myfile.pdf}\\n\\n## REFERENCES\\n\\nTakuya Akiba, Shotaro Sano, Toshihiko Yanase, Takeru Ohta, and Masanori Koyama. Optuna: A next-generation hyperparameter optimization framework. In Proceedings of the 25th ACM SIGKDD international conference on knowledge discovery &amp; data mining , pp. 2623-2631, 2019.\\n\\nRami Al-Rfou, Guillaume Alain, Amjad Almahairi, Christof Angermueller, Dzmitry Bahdanau, Nicolas Ballas, Fr´ ed´ric Bastien, e Justin Bayer, Anatoly Belikov, Alexander Belopolsky, et al. Theano: A Python framework for fast computation of mathematical expressions. arXiv e-prints , pp. arXiv-1605, 2016.\\n\\nJames Bergstra, Olivier Breuleux, Fr´ ed´ eric Bastien, Pascal Lamblin, Razvan Pascanu, Guillaume Desjardins, Joseph Turian, David Warde-Farley, and Yoshua Bengio. Theano: A CPU and GPU math compiler in Python. In Proc. 9th python in science conf , volume 1, pp. 3-10, 2010.\\n\\nJames Bergstra, Fr´ ed´ric Bastien, Olivier Breuleux, Pascal Lamblin, Razvan Pascanu, Olivier Dee lalleau, Guillaume Desjardins, David Warde-Farley, Ian Goodfellow, Arnaud Bergeron, et al. Theano: Deep learning on gpus with Python. In NIPS 2011, BigLearning Workshop, Granada, Spain , volume 3. Citeseer, 2011.\\n\\nJames Bergstra, Daniel Yamins, and David Cox. Making a science of model search: Hyperparameter optimization in hundreds of dimensions for vision architectures. In International conference on machine learning , pp. 115-123. PMLR, 2013.\\n\\nRishi Bommasani, Drew A Hudson, Ehsan Adeli, Russ Altman, Simran Arora, Sydney von Arx, Michael S Bernstein, Jeannette Bohg, Antoine Bosselut, Emma Brunskill, et al. On the opportunities and risks of foundation models. arXiv preprint arXiv:2108.07258 , 2021.\\n\\nTom Brown, Benjamin Mann, Nick Ryder, Melanie Subbiah, Jared D Kaplan, Prafulla Dhariwal, Arvind Neelakantan, Pranav Shyam, Girish Sastry, Amanda Askell, et al. Language models are few-shot learners. Advances in neural information processing systems , 33:1877-1901, 2020.\\n\\nHarrison Chase. Hwchase17/langchain. 2022. URL https://github.com/hwchase17/ langchain .\\n\\nDanqi Chen, Adam Fisch, Jason Weston, and Antoine Bordes. Reading Wikipedia to answer open-domain questions. In Proceedings of the 55th Annual Meeting of the Association for Computational Linguistics (Volume 1: Long Papers) , pp. 1870-1879, Vancouver, Canada, July 2017. Association for Computational Linguistics. doi: 10.18653/v1/P17-1171. URL https: //aclanthology.org/P17-1171 .\\n\\nLingjiao Chen, Matei Zaharia, and James Zou. Frugalgpt: How to use large language models while reducing cost and improving performance. arXiv preprint arXiv:2305.05176 , 2023.\\n\\n6 https://gist.github.com/g8a9/07c2be12ae02cfad4aa430d77dc940cb\\n\\n| Wenhu Chen, Xueguang Ma, Xinyi Wang, and William W Cohen. Program of thoughts prompt- ing: Disentangling computation from reasoning for numerical reasoning tasks. arXiv preprint arXiv:2211.12588 , 2022.                                                                                                                                                                                    |\\n|-----------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------|\\n| Aakanksha Chowdhery, Sharan Narang, Jacob Devlin, Maarten Bosma, Gaurav Mishra, Adam Roberts, Paul Barham, Hyung Won Chung, Charles Sutton, Sebastian Gehrmann, et al. Palm: Scaling language modeling with pathways. arXiv preprint arXiv:2204.02311 , 2022.                                                                                                                                 |\\n| Karl Cobbe, Vineet Kosaraju, Mohammad Bavarian, Mark Chen, Heewoo Jun, Lukasz Kaiser, Matthias Plappert, Jerry Tworek, Jacob Hilton, Reiichiro Nakano, et al. Training verifiers to solve math word problems. arXiv preprint arXiv:2110.14168 , 2021.                                                                                                                                         |\\n| Ronan Collobert, Samy Bengio, and Johnny Mari´thoz. e Torch: a modular machine learning software library. Technical report, Idiap, 2002.                                                                                                                                                                                                                                                      |\\n| David Dohan, Winnie Xu, Aitor Lewkowycz, Jacob Austin, David Bieber, Raphael Gontijo Lopes, Yuhuai Wu, Henryk Michalewski, Rif A Saurous, Jascha Sohl-Dickstein, et al. Language model cascades. arXiv preprint arXiv:2207.10342 , 2022.                                                                                                                                                      |\\n| Luyu Gao, Zhuyun Dai, Panupong Pasupat, Anthony Chen, Arun Tejasvi Chaganty, Yicheng Fan, Vincent Zhao, Ni Lao, Hongrae Lee, Da-Cheng Juan, et al. Rarr: Researching and revising what language models say, using language models. In Proceedings of the 61st Annual Meeting of the Association for Computational Linguistics (Volume 1: Long Papers) , pp. 16477-16508, 2023a.               |\\n| Luyu Gao, Aman Madaan, Shuyan Zhou, Uri Alon, Pengfei Liu, Yiming Yang, Jamie Callan, and Graham Neubig. Pal: Program-aided language models. In International Conference on Machine Learning , pp. 10764-10799. PMLR, 2023b.                                                                                                                                                                  |\\n| Qingyan Guo, Rui Wang, Junliang Guo, Bei Li, Kaitao Song, Xu Tan, Guoqing Liu, Jiang Bian, and Yujiu Yang. Connecting large language models with evolutionary algorithms yields powerful prompt optimizers. arXiv preprint arXiv:2309.08532 , 2023.                                                                                                                                           |\\n| Kelvin Guu, Kenton Lee, Zora Tung, Panupong Pasupat, and Ming-Wei Chang. Realm: Retrieval- augmented language model pre-training. arXiv preprint arXiv:2002.08909 , 2020. URL https: //arxiv.org/abs/2002.08909 .                                                                                                                                                                             |\\n| Braden Hancock, Paroma Varma, Stephanie Wang, Martin Bringmann, Percy Liang, and Christopher R´. e Training classifiers with natural language explanations. In Proceedings of the 56th Annual Meeting of the Association for Computational Linguistics (Volume 1: Long Papers) , pp. 1884- 1895. Association for Computational Linguistics, 2018. URL http://aclweb.org/anthology/ P18-1175 . |\\n| Bin Hu, Chenyang Zhao, Pu Zhang, Zihao Zhou, Yuanhang Yang, Zenglin Xu, and Bin Liu. En- abling intelligent interactions between an agent and an LLM: Areinforcement learning approach. arXiv preprint arXiv:2306.03604 , 2023. URL https://arxiv.org/abs/2306.03604 .                                                                                                                        |\\n| Jiaxin Huang, Shixiang Shane Gu, Le Hou, Yuexin Wu, Xuezhi Wang, Hongkun Yu, and Jiawei Han. Large language models can self-improve. arXiv preprint arXiv:2210.11610 , 2022.                                                                                                                                                                                                                  |\\n| Gautier Izacard, Patrick Lewis, Maria Lomeli, Lucas Hosseini, Fabio Petroni, Timo Schick, Jane Dwivedi-Yu, Armand Joulin, Sebastian Riedel, and Edouard Grave. Few-shot learning with re- trieval augmented language models. arXiv preprint arXiv:2208.03299 , 2022.                                                                                                                          |\\n| Ehud Karpas, Omri Abend, Yonatan Belinkov, Barak Lenz, Opher Lieber, Nir Ratner, Yoav Shoham, Hofit Bata, Yoav Levine, Kevin Leyton-Brown, et al. Mrkl systems: A modular, neuro-symbolic architecture that combines large language models, external knowledge sources and discrete rea- soning. arXiv preprint arXiv:2205.00445 , 2022.                                                      |\\n| Omar Khattab, Christopher Potts, and Matei Zaharia. Baleen: Robust Multi-Hop Reasoning at Scale via Condensed Retrieval. In Thirty-Fifth Conference on Neural Information Processing Systems , 2021a.                                                                                                                                                                                         |\\n| Omar Khattab, Christopher Potts, and Matei Zaharia. Relevance-guided supervision for openqa with ColBERT. Transactions of the Association for Computational Linguistics , 9:929-944, 2021b.                                                                                                                                                                                                   |\\n\\nOmar Khattab, Keshav Santhanam, Xiang Lisa Li, David Hall, Percy Liang, Christopher Potts, and Matei Zaharia. Demonstrate-search-predict: Composing retrieval and language models for knowledge-intensive nlp. arXiv preprint arXiv:2212.14024 , 2022.\\n\\nTushar Khot, Harsh Trivedi, Matthew Finlayson, Yao Fu, Kyle Richardson, Peter Clark, and Ashish Sabharwal. Decomposed prompting: A modular approach for solving complex tasks. arXiv preprint arXiv:2210.02406 , 2022.\\n\\nTakeshi Kojima, Shixiang Shane Gu, Machel Reid, Yutaka Matsuo, and Yusuke Iwasawa. Large language models are zero-shot reasoners. arXiv preprint arXiv:2205.11916 , 2022.\\n\\nAngeliki Lazaridou, Elena Gribovskaya, Wojciech Stokowiec, and Nikolai Grigorev. Internetaugmented language models through few-shot prompting for open-domain question answering. arXiv preprint arXiv:2203.05115 , 2022.\\n\\nPatrick Lewis, Ethan Perez, Aleksandra Piktus, Fabio Petroni, Vladimir Karpukhin, Naman Goyal, Heinrich K¨ uttler, Mike Lewis, Wen-tau Yih, Tim Rockt¨ aschel, Sebastian Riedel, and Douwe Kiela. Retrieval-augmented generation for knowledge-intensive nlp tasks. In H. Larochelle, M. Ranzato, R. Hadsell, M.F. Balcan, and H. Lin (eds.), Advances in Neural Information Processing Systems , volume 33, pp. 9459-9474. Curran Associates, Inc., 2020. URL https://proceedings.neurips.cc/paper files/paper/2020/file/ 6b493230205f780e1bc26945df7481e5-Paper.pdf .\\n\\nJerry Liu. LlamaIndex, 11 2022. URL https://github.com/jerryjliu/llama index .\\n\\nAman Madaan, Niket Tandon, Prakhar Gupta, Skyler Hallinan, Luyu Gao, Sarah Wiegreffe, Uri Alon, Nouha Dziri, Shrimai Prabhumoye, Yiming Yang, et al. Self-refine: Iterative refinement with self-feedback. arXiv preprint arXiv:2303.17651 , 2023.\\n\\nBryan McCann, Nitish Shirish Keskar, Caiming Xiong, and Richard Socher. The natural language decathlon: Multitask learning as question answering. arXiv:1806.08730, 2018. URL https: //arxiv.org/abs/1806.08730 .\\n\\nMicrosoft. Semantic kernel. 2023. URL https://learn.microsoft.com/semantic-kernel/ .\\n\\nReiichiro Nakano, Jacob Hilton, Suchir Balaji, Jeff Wu, Long Ouyang, Christina Kim, Christopher Hesse, Shantanu Jain, Vineet Kosaraju, William Saunders, Xu Jiang, Karl Cobbe, Tyna Eloundou, Gretchen Krueger, Kevin Button, Matthew Knight, Benjamin Chess, and John Schulman. WebGPT: Browser-assisted question-answering with human feedback, 2021. URL https: //arxiv.org/abs/2112.09332 .\\n\\nOpenAI. Gpt-4 technical report, 2023.\\n\\nLong Ouyang, Jeff Wu, Xu Jiang, Diogo Almeida, Carroll L Wainwright, Pamela Mishkin, Chong Zhang, Sandhini Agarwal, Katarina Slama, Alex Ray, et al. Training language models to follow instructions with human feedback. arXiv preprint arXiv:2203.02155 , 2022.\\n\\nAdam Paszke, Sam Gross, Francisco Massa, Adam Lerer, James Bradbury, Gregory Chanan, Trevor Killeen, Zeming Lin, Natalia Gimelshein, Luca Antiga, Alban Desmaison, Andreas Kopf, Edward Yang, Zachary DeVito, Martin Raison, Alykhan Tejani, Sasank Chilamkurthy, Benoit Steiner, Lu Fang, Junjie Bai, and Soumith Chintala. PyTorch: An imperative style, high-performance deep learning library. In H. Wallach, H. Larochelle, A. Beygelzimer, F. d\\'Alch´ e-Buc, E. Fox, and R. Garnett (eds.), Advances in Neural Information Processing Systems , volume 32. Curran Associates, Inc., 2019. URL https://proceedings.neurips.cc/paper files/paper/2019/ file/bdbca288fee7f92f2bfa9f7012727740-Paper.pdf .\\n\\nMohammadreza Pourreza and Davood Rafiei. Din-sql: Decomposed in-context learning of text-tosql with self-correction. arXiv preprint arXiv:2304.11015 , 2023.\\n\\nOfir Press, Muru Zhang, Sewon Min, Ludwig Schmidt, Noah A Smith, and Mike Lewis. Measuring and narrowing the compositionality gap in language models. arXiv preprint arXiv:2210.03350 , 2022.\\n\\n| optimization with\\' gradient descent\\' and beam search. arXiv preprint arXiv:2305.03495 , 2023.                                                                                                                                                                                                                                                                                                                                                                                                    |\\n|--------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------|\\n| Peng Qi, Xiaowen Lin, Leo Mehr, Zijian Wang, and Christopher D. Manning. Answering complex open-domain questions through iterative query generation. In Proceedings of the 2019 Con- ference on Empirical Methods in Natural Language Processing and the 9th International Joint Conference on Natural Language Processing (EMNLP-IJCNLP) , pp. 2590-2602, Hong Kong, China, 2019. Association for Computational Linguistics. doi: 10.18653/v1/D19-1261. URL https://aclanthology.org/D19-1261 . |\\n| Peng Qi, Haejun Lee, Oghenetegiri Sido, Christopher D Manning, et al. Retrieve, rerank, read, then iterate: Answering open-domain questions of arbitrary complexity from text. arXiv preprint arXiv:2010.12527 , 2020. URL https://arxiv.org/abs/2010.12527 .                                                                                                                                                                                                                                    |\\n| Alec Radford, Karthik Narasimhan, Tim Salimans, and Ilya Sutskever. Improving language un- derstanding by generative pre-training. Ms, OpenAI, 2018. URL https://openai.com/blog/ language-unsupervised/ .                                                                                                                                                                                                                                                                                       |\\n| Alexander J Ratner, Christopher M De Sa, Sen Wu, Daniel Selsam, and Christopher R´. e Data programming: Creating large training sets, quickly. In D. D. Lee, M. Sugiyama, U. V. Luxburg, I. Guyon, and R. Garnett (eds.), Advances in Neural Information Processing Systems 29 , pp. 3567-3575. Curran Associates, Inc., 2016. URL https://papers.nips.cc/paper/ 6523-data-programming-creating-large-training-sets-quickly .                                                                    |\\n| Keshav Santhanam, Omar Khattab, Jon Saad-Falcon, Christopher Potts, and Matei Zaharia. Col- BERTv2: Effective and Efficient Retrieval via Lightweight Late Interaction. arXiv preprint arXiv:2112.01488 , 2021.                                                                                                                                                                                                                                                                                  |\\n| Timo Schick, Jane Dwivedi-Yu, Roberto Dess`, ı Roberta Raileanu, Maria Lomeli, Luke Zettlemoyer, Nicola Cancedda, and Thomas Scialom. Toolformer: Language models can teach themselves to use tools. arXiv preprint arXiv:2302.04761 , 2023.                                                                                                                                                                                                                                                     |\\n| Zhihong Shao, Yeyun Gong, Yelong Shen, Minlie Huang, Nan Duan, and Weizhu Chen. Syn- thetic prompting: Generating chain-of-thought demonstrations for large language models. arXiv preprint arXiv:2302.00618 , 2023.                                                                                                                                                                                                                                                                             |\\n| Noah Shinn, Beck Labash, and Ashwin Gopinath. Reflexion: an autonomous agent with dynamic memory and self-reflection. arXiv preprint arXiv:2303.11366 , 2023.                                                                                                                                                                                                                                                                                                                                    |\\n| Chenglei Si, Zhe Gan, Zhengyuan Yang, Shuohang Wang, Jianfeng Wang, Jordan Boyd-Graber, and Lijuan Wang. Prompting gpt-3 to be reliable. arXiv preprint arXiv:2210.09150 , 2022.                                                                                                                                                                                                                                                                                                                 |\\n| Zhiqing Sun, Xuezhi Wang, Yi Tay, Yiming Yang, and Denny Zhou. Recitation-augmented language models. arXiv preprint arXiv:2210.01296 , 2022.                                                                                                                                                                                                                                                                                                                                                     |\\n| Seiya Tokui, Kenta Oono, Shohei Hido, and Justin Clayton. Chainer: a next-generation open source framework for deep learning. In Proceedings of workshop on machine learning systems (Learn- ingSys) in the twenty-ninth annual conference on neural information processing systems (NIPS) , volume 5, pp. 1-6, 2015.                                                                                                                                                                            |\\n| Hugo Touvron, Louis Martin, Kevin Stone, Peter Albert, Amjad Almahairi, Yasmine Babaei, Niko- lay Bashlykov, Soumya Batra, Prajjwal Bhargava, Shruti Bhosale, et al. Llama 2: Open founda- tion and fine-tuned chat models. arXiv preprint arXiv:2307.09288 , 2023.                                                                                                                                                                                                                              |\\n| Harsh Trivedi, Niranjan Balasubramanian, Tushar Khot, and Ashish Sabharwal. Interleaving re- trieval with chain-of-thought reasoning for knowledge-intensive multi-step questions. arXiv preprint arXiv:2212.10509 , 2022.                                                                                                                                                                                                                                                                       |\\n| Fei Wang, James Decker, Xilun Wu, Gregory Essertel, and Tiark Rompf. Backpropaga- tion with callbacks: Foundations for efficient and expressive differentiable programming. In S. Bengio, H. Wallach, H. Larochelle, K. Grauman, N. Cesa-Bianchi, and R. Gar- nett (eds.), Advances in Neural Information Processing Systems , volume 31. Curran Asso- ciates, Inc., 2018. URL https://proceedings.neurips.cc/paper files/paper/2018/file/ 34e157766f31db3d2099831d348a7933-Paper.pdf .          |\\n\\nXuezhi Wang, Jason Wei, Dale Schuurmans, Quoc Le, Ed Chi, and Denny Zhou. Rationaleaugmented ensembles in language models. arXiv preprint arXiv:2207.00747 , 2022a.\\n\\nXuezhi Wang, Jason Wei, Dale Schuurmans, Quoc Le, Ed Chi, and Denny Zhou. Self-consistency improves chain of thought reasoning in language models. arXiv preprint arXiv:2203.11171 , 2022b.\\n\\n- Jason Wei, Xuezhi Wang, Dale Schuurmans, Maarten Bosma, Ed Chi, Quoc Le, and Denny Zhou. Chain of thought prompting elicits reasoning in large language models. arXiv preprint arXiv:2201.11903 , 2022.\\n\\nThomas Wolf, Lysandre Debut, Victor Sanh, Julien Chaumond, Clement Delangue, Anthony Moi, Pierric Cistac, Tim Rault, Remi Louf, Morgan Funtowicz, Joe Davison, Sam Shleifer, Patrick von Platen, Clara Ma, Yacine Jernite, Julien Plu, Canwen Xu, Teven Le Scao, Sylvain Gugger, Mariama Drame, Quentin Lhoest, and Alexander Rush. Transformers: State-of-the-art natural language processing. In Proceedings of the 2020 Conference on Empirical Methods in Natural Language Processing: System Demonstrations , pp. 38-45, Online, 2020. Association for Computational Linguistics. doi: 10.18653/v1/2020.emnlp-demos.6. URL https: //aclanthology.org/2020.emnlp-demos.6 .\\n\\n- Chengrun Yang, Xuezhi Wang, Yifeng Lu, Hanxiao Liu, Quoc V Le, Denny Zhou, and Xinyun Chen. Large language models as optimizers. arXiv preprint arXiv:2309.03409 , 2023.\\n\\nZhilin Yang, Peng Qi, Saizheng Zhang, Yoshua Bengio, William W Cohen, Ruslan Salakhutdinov, and Christopher D Manning. Hotpotqa: A dataset for diverse, explainable multi-hop question answering. arXiv preprint arXiv:1809.09600 , 2018.\\n\\n- Shunyu Yao, Jeffrey Zhao, Dian Yu, Nan Du, Izhak Shafran, Karthik Narasimhan, and Yuan Cao. React: Synergizing reasoning and acting in language models. arXiv preprint arXiv:2210.03629 , 2022.\\n- Ori Yoran, Tomer Wolfson, Ben Bogin, Uri Katz, Daniel Deutch, and Jonathan Berant. Answering questions by meta-reasoning over multiple chains of thought. arXiv preprint arXiv:2304.13007 , 2023.\\n- Eric Zelikman, Yuhuai Wu, and Noah D Goodman. Star: Bootstrapping reasoning with reasoning. arXiv preprint arXiv:2203.14465 , 2022.\\n- Zhuosheng Zhang, Aston Zhang, Mu Li, and Alex Smola. Automatic chain of thought prompting in large language models. arXiv preprint arXiv:2210.03493 , 2022.\\n\\nAndrew Zhao, Daniel Huang, Quentin Xu, Matthieu Lin, Yong-Jin Liu, and Gao Huang. ExpeL: LLM agents are experiential learners. arXiv preprint arXiv:2308.10144 , 2023a. URL https: //arxiv.org/pdf/2308.10144 .\\n\\nXu Zhao, Yuxi Xie, Kenji Kawaguchi, Junxian He, and Qizhe Xie. Automatic model selection with large language models for reasoning. arXiv preprint arXiv:2305.14333 , 2023b.\\n\\n## A ADVANCED SIGNATURES\\n\\nWhen more control is desired, one can express signatures as Python classes to provide explicit instructions of the transformation and describe the format or role of each field more directly. For instance, the following signature generates search queries using context and an optional question:\\n\\n```\\n1 class GenerateSearchQuery(dspy.Signature): 2 \"\"\"Write a simple search query that will help answer a complex 3 4 context = dspy.InputField(desc=\"may contain relevant facts\") 5 question = dspy.InputField() 6 query = dspy.OutputField(dtype=dspy.SearchQuery)\\n```\\n\\n```\\nquestion.\"\"\"\\n```\\n\\nUsing the above, we can specify a complete system for the generation of a synthetic IR dataset where the queries are mediated by a question generated by the LM:\\n\\n```\\n1 query_gen = dspy.Predict(GenerateSearchQuery) 2 query_gen(context=\"Language typology\") 3 # Out: Prediction(question=\\'What are the main types of language classification?\\', query=\\'\"language classification\" OR \"language typology\" -wikipedia\\')\\n```\\n\\nIf questions are available, they can be supplied as shown: query gen(context=\"Language typology\", question=\"What are the primary language families of South America?\") .\\n\\nAs a work in progress feature, users can optionally specify the type of output fields as bool, int, float, list , or dict instead of the default free-form string type, as in contexts, question -&gt; answer found: bool .\\n\\n## B COMPARISON WITH EXISTING LIBRARIES LIKE LANGCHAIN AND LLAMAINDEX\\n\\nLangChain and LlamaIndex are perhaps the most popular library in the general space of prompting LMs. These libraries have a different focus compared to DSPy and they suffer internally from the prompt engineering challenges that DSPy aims to resolve. In particular, whereas the goal of DSPy is to tackle the fundamental challenges of prompt engineering for building new LM computational graphs, LangChain and LlamaIndex generally help application developers who need pre-packaged components and chains, e.g., implementations of popular and reusable pipelines (e.g., particular agents and specific retrieval pipelines) and tools (e.g., connections to various databases and implementations of long- and short-term memory for agents).\\n\\nThese off-the-shelf higher-level abstractions contrast with DSPy\\'s focus on introducing core composable operators. In particular, DSPy introduces signatures (to abstract prompts), modules (to abstract prompting techniques), and teleprompters to act as optimizers for arbitrary imperative code (DSPy programs) that chain modules together. Its goal is to help researchers and practitioners build new LM pipelines quickly and achieve very high quality through automatic compilation (selfimprovement) instead of manual prompt engineering.\\n\\nIn contrast, typical existing research implementations and existing libraries like LangChain and LlamaIndex are implemented using manual prompt engineering, which is the key problem that DSPy tackles. We conducted an informal study to highlight this. In late September 2023, we found that the LangChain codebase contains 50 strings exceeding 1000 characters, which are generally prompts, compared to none at all in DSPy. Indeed, a substantial number of LangChain\\'s Python files are singularly dedicated to task-related templating and prompt engineering with 12 prompts.py files and and 42 prompt.py files. DSPy, on the other hand, provides a structured framework that automatically bootstraps prompts. The library itself does not contain a single hand-written prompt demonstration for any tasks at the time of writing, despite the very high quality with various LMs.\\n\\nTo review the typical forms of prompt engineering in existing libraries, we consider the following in LangChain. The LangChain Program-Aided Language Model Gao et al. (2023a) chain program uses few-shot learning, leveraging a template that is 3982 characters long with 8 math word problems (Prompt 2) and corresponding outputted programs as learning examples for the language model. LangChain also contains a prompt for SQL query tasks for each of the databases like Oracle, GoogleSQL, DuckDB, Crate, and MySQL, with the average length of these prompts at 1058 characters. Other task areas such as QA with sources (Prompt B) and Graph QA also have signif-\\n\\nicantly lengthy prompt templates, with averages of 1337 and 722 characters, respectively. While expert-written prompts can be useful, we believe that LM- and task-adaptive prompts bootstrapped automatically can offer far more power (and are far more modular) than hard-coding a prompt per database provider inside the code base. The next appendix section contains a number of prompts copied from related research papers and existing libraries.\\n\\n## C SAMPLE LARGE PROMPTS\\n\\nThis section highlights a few popular existing frameworks that structure prompts with extensive prompt engineering templates. The primary objective is to capture how many words and characters are used for such large multi-line prompts defined for tasks or tools and present these example prompts retrieved from open-sourced papers and repositories. The formatting of these example prompts is adapted from Gao et al. (2023a).\\n\\n| Task/Tool Prompt                   | Source                        |   Words |   Characters |\\n|------------------------------------|-------------------------------|---------|--------------|\\n| Prompt 1: Text-evidence checker    | Gao et al. (2023a)            |     818 |         4964 |\\n| Prompt 2: Math word problems (PAL) | LangChain &Gao et al. (2023b) |     566 |         3957 |\\n| Prompt 3: ReAct                    | Yao et al. (2022)             |     593 |         3889 |\\n| Prompt 4: Zero-shot ReAct          | LangChain                     |     101 |          600 |\\n| Prompt 5: QA with sources          | LangChain                     |     992 |         6197 |\\n| Prompt 6: SQL MyScale querying     | LangChain                     |     343 |         2239 |\\n| Prompt 7: Relevant docs retrieval  | LlamaIndex                    |     129 |          719 |\\n| Prompt 8: IRS chatbot              | LlamaIndex                    |     389 |         2258 |\\n\\n```\\n1 [web] I will check some things you said. 2 3 (1) You said: Your nose switches back and forth between nostrils. When you sleep, you switch about every 45 minutes. This is to prevent a buildup of mucus. It\\'s called the nasal cycle. 4 I checked: How often do your nostrils switch? 5 I found this article: Although we don\\'t usually notice it, during the nasal cycle one nostril becomes congested and thus contributes less to airflow, while the other becomes decongested. On average, the congestion pattern switches about every 2 hours, according to a small 2016 study published in the journal PLOS One. 6 Your nose\\'s switching time is about every 2 hours, not 45 minutes. 7 This disagrees with what you said. 8 9 (2) You said: The Little House books were written by Laura Ingalls Wilder. The books were published by HarperCollins. 10 I checked: Who published the Little House books? 11 I found this article: These are the books that started it all -- the stories that captured the hearts and imaginations of children and young adults worldwide. Written by Laura Ingalls Wilder and published by HarperCollins, these beloved books remain a favorite to this day. 12 The Little House books were published by HarperCollins. 13 This agrees with what you said. 14 15 (3) You said: The Stanford Prison Experiment was conducted in the basement of Jordan Hall, Stanford\\'s psychology building. 16 I checked: Where was Stanford Prison Experiment conducted? 17 I found this article: Carried out August 15-21, 1971 in the basement of Jordan Hall, the Stanford Prison Experiment set out to examine the psychological effects of authority and powerlessness in a prison environment. 18 The Stanford Prison Experiment was conducted in Jordan Hall. 19 This agrees with what you said. 20 21 (4) You said: Social work is a profession that is based in the philosophical tradition of humanism. It is an intellectual discipline that has its roots in the 1800s. 22 I checked: When did social work have its roots? 23 I found this article: The Emergence and Growth of the Social work Profession<br><br> Social work\\'s roots were planted in the 1880s, when charity organization societies (COS) were created to organize municipal voluntary relief associations and settlement houses were established. 24 Social work has its roots in the 1880s, not 1800s. 25 This disagrees with what you said. 26 27 (5) You said: The Havel-Hakimi algorithm is an algorithm for converting the adjacency matrix of a graph into its adjacency list. It is named after Vaclav Havel and Samih Hakimi. 28 I checked: What is the Havel-Hakimi algorithm? 29 I found this article: The Havel-Hakimi algorithm constructs a special solution if a simple graph for the given degree sequence exists, or proves that one cannot find a positive answer. This construction is based on a recursive algorithm. The algorithm was published by Havel (1955), and later by Hakimi (1962). 30 Havel-Hakimi algorithm is for constructing a special solution if a simple graph for the given degree sequence exists, or proving that one cannot find a positive answer, not converting the adjacency matrix of a graph into its adjacency list. 31 This disagrees with what you said. 32 33 (6) You said: \"Time of My Life\" is a song by American singer-songwriter Bill Medley from the soundtrack of the 1987 film Dirty Dancing. The song was produced by Michael Lloyd. 34 I checked: Who was the producer of \"(I\\'ve Had) The Time of My Life\"? 35 I found this article: On September 8, 2010, the original demo of this song, along with a remix by producer Michael Lloyd, was released as digital files in an effort to raise money for the Patrick Swayze Pancreas Cancer Resarch Foundation at Stanford University. 36 \"Time of My Life\" was produced by Michael Lloyd. 37 This agrees with what you said. 38 39 (7) You said: Kelvin Hopins was suspended from the Labor Party because he had allegedly sexually harassed and behaved inappropriately towards a Labour Party activist, Ava Etemadzadeh. 40 I checked: Why was Kelvin Hopins suspeneded from the Labor Party? 41 I found this article: A former Labour MP has left the party before an inquiry into sexual harassment allegations against him was able to be concluded, the party has confirmed. Kelvin Hopkins was accused in 2017 of inappropriate physical contact and was suspended by the Labour party pending an investigation.This agrees with what you said. 42 Kelvin Hopins was suspended because he had allegedly sexually harassed and behaved inappropriately towards a Labour Party activist, Ava Etemadzadeh. 43 This agrees with what you said. 44 45 (8) You said: In the battles of Lexington and Concord, the British side was led by General Thomas Smith. 46 I checked: Who led the British side in the battle of Lexington and Concord? 47 I found this article: Interesting Facts about the Battles of Lexington and Concord. The British were led by Lieutenant Colonel Francis Smith. There were 700 British regulars. 48 The British side was led by Lieutenant Colonel Francis Smith, not General Thomas Hall. 49 This disagrees with what you said. 50 51 (9) You said: { text } 52 I checked: { query } 53 I found this article: { evidence } 54\\n```\\n\\nFigure 1: Example few-shot prompt using a reasoning chain for agreement model that identifies inconsistencies between text and evidence (Gao et al., 2023a).\\n\\n```\\n1 Q: Olivia has $23. She bought five bagels for $3 each. How much money does she have left? 2 3 # solution in Python: 4 5 6 def solution(): 7 \"\"\"Olivia has $23. She bought five bagels for $3 each. How much money does she have left?\"\"\" 8 money initial = 23 9 bagels = 5 10 bagel cost = 3 11 money spent = bagels * bagel cost 12 money left = money initial -money spent 13 result = money left 14 return result 15 16 17 18 19 20 Q: Michael had 58 golf balls. On tuesday, he lost 23 golf balls. On wednesday, he lost 2 more. How many golf balls did he have at the end of wednesday? 21 22 # solution in Python: 23 24 25 def solution(): 26 \"\"\"Michael had 58 golf balls. On tuesday, he lost 23 golf balls. On wednesday, he lost 2 more. How many golf balls did he have at the end of wednesday?\"\"\" 27 golf balls initial = 58 28 golf balls lost tuesday = 23 29 golf balls lost wednesday = 2 30 golf balls left = golf balls initial - golf balls lost tuesday -golf balls lost wednesday 31 result = golf balls left 32 return result 33 34 35 36 37 38 Q: There were nine computers in the server room. Five more computers were installed each day, from monday to thursday. How many computers are now in the server room? 39 40 # solution in Python: 41 42 43 def solution(): 44 \"\"\"There were nine computers in the server room. Five more computers were installed each day, from monday to thursday. How many computers are now in the server room?\"\"\" 45 computers initial = 9 46 computers per day = 5 47 num days = 4 48 computers added = computers per day * num days 49 computers total = computers initial + computers added 50 result = computers total 51 return result 52 53 54 55 56 57 Q: Shawn has five toys. For Christmas, he got two toys each from his mom and dad. How many toys does he have now? 58 59 # solution in Python: 60 61 62 def solution(): 63 \"\"\"Shawn has five toys. For Christmas, he got two toys each from his mom and dad. How many toys does he have now?\"\"\" 64 toys initial = 5 65 mom toys = 2 66 dad toys = 2 67 total received = mom toys + dad toys 68 total toys = toys initial + total received 69 result = total toys 70 return result 71 72 73 74 75 76 Q: Jason had 20 lollipops. He gave Denny some lollipops. Now Jason has 12 lollipops. How many lollipops did Jason give to Denny? 77 78 # solution in Python: 79 80 81 20\\n```\\n\\n```\\n1 2 3 4 def solution(): 5 \"\"\"Jason had 20 lollipops. He gave Denny some lollipops. Now Jason has 12 lollipops. How many lollipops did Jason give to Denny?\"\"\" 6 jason lollipops initial = 20 7 jason lollipops after = 12 8 denny lollipops = jason lollipops initial -jason lollipops after 9 result = denny lollipops 10 return result 11 12 13 14 15 16 Q: Leah had 32 chocolates and her sister had 42. If they ate 35, how many pieces do they have left in total? 17 18 # solution in Python: 19 20 def solution(): 21 \"\"\"Leah had 32 chocolates and her sister had 42. If they ate 35, how many pieces do they have left in total?\"\"\" 22 leah chocolates = 32 23 sister chocolates = 42 24 total chocolates = leah chocolates + sister chocolates 25 chocolates eaten = 35 26 chocolates left = total chocolates - chocolates eaten 27 result = chocolates left 28 return result 29 30 31 32 33 34 Q: If there are 3 cars in the parking lot and 2 more cars arrive, how many cars are in the parking lot? 35 36 # solution in Python: 37 38 39 def solution(): 40 \"\"\"If there are 3 cars in the parking lot and 2 more cars arrive, how many cars are in the parking lot?\"\"\" 41 cars initial = 3 42 cars arrived = 2 43 total cars = cars initial + cars arrived 44 result = total cars 45 return result 46 47 48 49 50 51 Q: There are 15 trees in the grove. Grove workers will plant trees in the grove today. After they are done, there will be 21 trees. How many trees did the grove workers plant today? 52 53 # solution in Python: 54 55 56 def solution(): 57 \"\"\"There are 15 trees in the grove. Grove workers will plant trees in the grove today. After they are done, there will be 21 trees. How many trees did the grove workers plant today?\"\"\" 58 trees initial = 15 59 trees after = 21 60 trees added = trees after - trees initial 61 result = trees added 62 return result 63 64 65 66 67 68 Q: { question } 69 70 # solution in Python:\\n```\\n\\nFigure 2: PAL example few-shot prompt for solving math questions by generating code.\\n\\n- 1 Solve a question answering task with interleaving Thought, Action, Observation steps. Thought can reason about the current situation, and Action can be three types:\\n- 2 (1) Search[entity], which searches the exact entity on Wikipedia and returns the first paragraph if it exists. If not, it will return some similar entities to search.\\n- 3 (2) Lookup[keyword], which returns the next sentence containing keyword in the current passage.\\n- 4 (3) Finish[answer], which returns the answer and finishes the task.\\n- 5 Here are some examples.\\n\\n6\\n\\nQuestion:\\n\\nWhat is the\\n\\nelevation range for\\n\\nthe area\\n\\nthat the\\n\\neastern sector of the\\n\\nColorado orogeny\\n\\nextends into?\\n\\n- 7 Action 1: Search[Colorado orogeny]\\n- 8 Observation 1: The Colorado orogeny was an episode of mountain building (an orogeny) in Colorado and surrounding areas.\\n- 9 Action 2: Lookup[eastern sector]\\n- 10 Observation 2: (Result 1 / 1) The eastern sector extends into the High Plains and is called the Central Plains orogeny.\\n- 11 Action 3: Search[High Plains]\\n- 12 Observation 3: High Plains refers to one of two distinct land regions:\\n- 13 Action 4: Search[High Plains (United States)]\\n- 14 Observation 4: The High Plains are a subregion of the Great Plains. From east to west, the High Plains rise in elevation from around 1,800 to 7,000 ft (550 to 2,130 m).[3]\\n- 15 Action 5: Finish[1,800 to 7,000 ft]\\n- 16 Question: Musician and satirist Allie Goertz wrote a song about the \"The Simpsons\" character Milhouse, who Matt Groening named after who?\\n- 17 Action 1: Search[Milhouse]\\n- 18 Observation 1: Milhouse Mussolini Van Houten is a recurring character in the Fox animated television series The Simpsons voiced by Pamela Hayden and created by Matt Groening.\\n- 19 Action 2: Lookup[named after]\\n- 20 Observation 2: (Result 1 / 1) Milhouse was named after U.S. president Richard Nixon, whose middle name was Milhous. 21 Action 3: Finish[Richard Nixon]\\n- 22 Question: Which documentary is about Finnish rock groups, Adam Clayton Powell or The Saimaa Gesture?\\n- 23 Action 1: Search[Adam Clayton Powell]\\n- 24 Observation 1: Could not find [Adam Clayton Powell]. Similar: [\\'Adam Clayton Powell III\\', \\'Seventh Avenue (Manhattan)\\', \\'Adam Clayton Powell Jr. State Office Building\\', \\'Isabel Washington Powell\\', \\'Adam Powell\\', \\'Adam Clayton Powell (film)\\', \\'Giancarlo Esposito\\'].\\n- 25 Action 2: Search[Adam Clayton Powell (film)]\\n- 26 Observation 2: Adam Clayton Powell is a 1989 American documentary film directed by Richard Kilberg. The film is about the rise and fall of influential African-American politician Adam Clayton Powell Jr.[3][4] It was later aired as part of the PBS series The American Experience.\\n- 27 Action 3: Finish[The Saimaa Gesture]\\n- 28 Question: What profession does Nicholas Ray and Elia Kazan have in common?\\n- 29 Action 1: Search[Nicholas Ray]\\n- 30 Observation 1: Nicholas Ray (born Raymond Nicholas Kienzle Jr., August 7, 1911 { June 16, 1979) was an American film director, screenwriter, and actor best known for the 1955 film Rebel Without a Cause.\\n- 31 Action 2: Search[Elia Kazan]\\n- 32 Observation 2: Elia Kazan was an American film and theatre director, producer, screenwriter and actor.\\n- 33 Action 3: Finish[director, screenwriter, actor]\\n- 34 Question: Which magazine was started first Arthur\\'s Magazine or First for Women?\\n- 35 Action 1: Search[Arthur\\'s Magazine]\\n- 36 Observation 1: Arthur\\'s Magazine (1844-1846) was an American literary periodical published in Philadelphia in the 19th century.\\n- 37 Action 2: Search[First for Women]\\n- 38 Observation 2: First for Women is a woman\\'s magazine published by Bauer Media Group in the USA.[1] The magazine was started in 1989.\\n- 39 Action 3: Finish[Arthur\\'s Magazine]\\n- 40 Question: Were Pavel Urysohn and Leonid Levin known for the same type of work?\\n- 41 Action 1: Search[Pavel Urysohn]\\n- 42 Observation 1: Pavel Samuilovich Urysohn (February 3, 1898 ˆ August 17, 1924) was a Soviet mathematician who is best known a for his contributions in dimension theory.\\n- 43 Action 2: Search[Leonid Levin]\\n- 44 Observation 2: Leonid Anatolievich Levin is a Soviet-American mathematician and computer scientist.\\n- 45 Action 3: Finish[yes]\\n\\nFigure 3: ReAct example prompt for interleaving Thought, Action, Observation steps.\\n\\n```\\n1 Answer the following questions as best you can. You have access to the following tools: 2 Search: useful for when you need to answer questions about the world 3 Use the following format: 4 Question: the input question you must answer 5 Thought: you should always think about what to do 6 Action: the action to take, should be one of [Search] 7 Action Input: the input to the action 8 Observation: the result of the action 9 ... (this Thought/Action/Action Input/Observation can repeat N times) 10 Thought: I now know the final answer 11 Final Answer: the final answer to the original input question 12 Begin! 13 Question: { question } 14 Thought:\\n```\\n\\nFigure 4: Langchain ReAct example prompt for interleaving Thought, Action, Observation steps.\\n\\n- 1 Given the following extracted parts of a long document and a question, create a final answer with references (\"SOURCES\"). 2 If you don\\'t know the answer, just say that you don\\'t know. Don\\'t try to make up an answer.\\n- 3 ALWAYS return a \"SOURCES\" part in your answer.\\n\\n4\\n\\n- 5 QUESTION: Which state/country\\'s law governs the interpretation of the contract?\\n- 6 =========\\n- 7 Content: This Agreement is governed by English law and the parties submit to the exclusive jurisdiction of the English courts in relation to any dispute (contractual or non-contractual) concerning this Agreement save that either party may apply to any court for an injunction or other relief to protect its Intellectual Property Rights.\\n- 8 Source: 28-pl\\n- 9 Content: No Waiver. Failure or delay in exercising any right or remedy under this Agreement shall not constitute a waiver of such (or any other) right or remedy.\\n- 10 11.7 Severability. The invalidity, illegality or unenforceability of any term (or part of a term) of this Agreement shall not affect the continuation in force of the remainder of the term (if any) and this Agreement.\\n- 11 11.8 No Agency. Except as expressly stated otherwise, nothing in this Agreement shall create an agency, partnership or joint venture of any kind between the parties.\\n- 12 11.9 No Third-Party Beneficiaries.\\n- 13 Source: 30-pl\\n- 14 Content: (b) if Google believes, in good faith, that the Distributor has violated or caused Google to violate any Anti-Bribery Laws (as defined in Clause 8.5) or that such a violation is reasonably likely to occur,\\n- 15 Source: 4-pl\\n- 16 =========\\n- 17 FINAL ANSWER: This Agreement is governed by English law.\\n- 18 SOURCES: 28-pl\\n\\n19\\n\\n- 20 QUESTION: What did the president say about Michael Jackson?\\n- 21 =========\\n- 22 Content: Madam Speaker, Madam Vice President, our First Lady and Second Gentleman. Members of Congress and the Cabinet. Justices of the Supreme Court. My fellow Americans.\\n- 23 Last year COVID-19 kept us apart. This year we are finally together again.\\n- 24 Tonight, we meet as Democrats Republicans and Independents. But most importantly as Americans.\\n- 25 With a duty to one another to the American people to the Constitution.\\n- 26 And with an unwavering resolve that freedom will always triumph over tyranny.\\n- 27 Six days ago, Russia\\'s Vladimir Putin sought to shake the foundations of the free world thinking he could make it bend to his menacing ways. But he badly miscalculated.\\n- 28 He thought he could roll into Ukraine and the world would roll over. Instead he met a wall of strength he never imagined. 29 He met the Ukrainian people.\\n- 30 From President Zelenskyy to every Ukrainian, their fearlessness, their courage, their determination, inspires the world.\\n- 31 Groups of citizens blocking tanks with their bodies. Everyone from students to retirees teachers turned soldiers defending their homeland.\\n- 32 Source: 0-pl\\n- 33 Content: And we won\\'t stop.\\n- 34 We have lost so much to COVID-19. Time with one another. And worst of all, so much loss of life.\\n- 35 Let\\'s use this moment to reset. Let\\'s stop looking at COVID-19 as a partisan dividing line and see it for what it is: A God-awful disease.\\n- 36 Let\\'s stop seeing each other as enemies, and start seeing each other for who we really are: Fellow Americans.\\n- 37 We can\\'t change how divided we\\'ve been. But we can change how we move forward|on COVID-19 and other issues we must face together.\\n- 38 I recently visited the New York City Police Department days after the funerals of Officer Wilbert Mora and his partner, Officer Jason Rivera.\\n- 39 They were responding to a 9-1-1 call when a man shot and killed them with a stolen gun.\\n- 40 Officer Mora was 27 years old.\\n- 41 Officer Rivera was 22.\\n- 42 Both Dominican Americans who\\'d grown up on the same streets they later chose to patrol as police officers.\\n- 43 I spoke with their families and told them that we are forever in debt for their sacrifice, and we will carry on their mission to restore the trust and safety every community deserves.\\n- 44 Source: 24-pl\\n- 45 Content: And a proud Ukrainian people, who have known 30 years of independence, have repeatedly shown that they will not tolerate anyone who tries to take their country backwards.\\n- 46 To all Americans, I will be honest with you, as I\\'ve always promised. A Russian dictator, invading a foreign country, has costs around the world.\\n- 47 And I\\'m taking robust action to make sure the pain of our sanctions is targeted at Russia\\'s economy. And I will use every tool at our disposal to protect American businesses and consumers.\\n- 48 Tonight, I can announce that the United States has worked with 30 other countries to release 60 Million barrels of oil from reserves around the world.\\n- 49 America will lead that effort, releasing 30 Million barrels from our own Strategic Petroleum Reserve. And we stand ready to do more if necessary, unified with our allies.\\n- 50 These steps will help blunt gas prices here at home. And I know the news about what\\'s happening can seem alarming.\\n- 51 But I want you to know that we are going to be okay.\\n- 52 Source: 5-pl\\n- 53 Content: More support for patients and families.\\n- 54 To get there, I call on Congress to fund ARPA-H, the Advanced Research Projects Agency for Health.\\n- 55 It\\'s based on DARPA|the Defense Department project that led to the Internet, GPS, and so much more.\\n- 56 ARPA-H will have a singular purpose|to drive breakthroughs in cancer, Alzheimer\\'s, diabetes, and more.\\n\\nFigure 5: Langchain example prompt for QA with sources.\\n\\n<!-- image -->\\n\\n- 1 You are a MyScale expert. Given an input question, first create a syntactically correct MyScale query to run, then look at the results of the query and return the answer to the input question.\\n- 2 MyScale queries has a vector distance function called DISTANCE(column, array) to compute relevance to the user\\'s question and sort the feature array column by the relevance.\\n- 3 When the query is asking for { top k } closest row, you have to use this distance function to calculate distance to entity\\'s array on vector column and order by the distance to retrieve relevant rows.\\n- 4 *NOTICE*: DISTANCE(column, array) only accept an array column as its first argument and a NeuralArray(entity) as its second argument. You also need a user defined function called NeuralArray(entity) to retrieve the entity\\'s array.\\n- 5 Unless the user specifies in the question a specific number of examples to obtain, query for at most { top k } results using the LIMIT clause as per MyScale. You should only order according to the distance function.\\n- 6 Never query for all columns from a table. You must query only the columns that are needed to answer the question. Wrap each column name in double quotes (\") to denote them as delimited identifiers.\\n- 7 Pay attention to use only the column names you can see in the tables below. Be careful to not query for columns that do not exist. Also, pay attention to which column is in which table.\\n- 8 Pay attention to use today() function to get the current date, if the question involves \"today\". ORDER BY clause should always be after WHERE clause. DO NOT add semicolon to the end of SQL. Pay attention to the comment in table schema.\\n\\n```\\n9 10 Use the following format: 11 ======== table info ======== 12 { table info } 13 Question: { input } 14 SQLQuery: 15 16 Here are some examples: 17 ======== table info ======== 18 CREATE TABLE \"ChatPaper\" ( 19 abstract String, 20 id String, 21 vector Array(Float32), 22 ) ENGINE = ReplicatedReplacingMergeTree() 23 ORDER BY id 24 PRIMARY KEY id 25 Question: What is Feature Pyramid Network? 26 SQLQuery: SELECT ChatPaper.title, ChatPaper.id, ChatPaper.authors FROM ChatPaper ORDER BY DISTANCE(vector, NeuralArray(PaperRank contribution)) LIMIT { top k } 27 28 Let\\'s begin: 29 ======== table info ======== 30 { table info } 31 Question: { input } 32 SQLQuery:\\n```\\n\\nFigure 6: Langchain example prompt for SQL querying using MyScale.\\n\\nFigure 7: LlamaIndex example prompt for returning relevant documents and corresponding summaries.\\n\\n<!-- image -->\\n\\n- 1 You are an IRS chatbot whose primary goal is to help users with filing their tax returns for the 2022 year.\\n- 2 Provide concise replies that are polite and professional.\\n- 3 Answer questions truthfully based on official government information, with consideration to context provided below on changes for 2022 that can affect tax refund.\\n- 4 Do not answer questions that are not related to United States tax procedures and respond with \"I can only help with any tax-related questions you may have.\".\\n- 5 If you do not know the answer to a question, respond by saying \\\\I do not know the answer to your question. You may be able to find your answer at www.irs.gov/faqs\"\\n\\n6\\n\\n- 7 Changes for 2022 that can affect tax refund:\\n- 8 Changes in the number of dependents, employment or self-employment income and divorce, among other factors, may affect your tax-filing status and refund. No additional stimulus payments. Unlike 2020 and 2021, there were no new stimulus payments for 2022 so taxpayers should not expect to get an additional payment.\\n- 9 Some tax credits return to 2019 levels. This means that taxpayers will likely receive a significantly smaller refund compared with the previous tax year. Changes include amounts for the Child Tax Credit (CTC), the Earned Income Tax Credit (EITC) and the Child and Dependent Care Credit will revert to pre-COVID levels.\\n- 10 For 2022, the CTC is worth $2,000 for each qualifying child. A child must be under age 17 at the end of 2022 to be a qualifying child. For the EITC, eligible taxpayers with no children will get $560 for the 2022 tax year. The Child and Dependent Care Credit returns to a maximum of $2,100 in 2022.\\n- 11 No above-the-line charitable deductions. During COVID, taxpayers were able to take up to a $600 charitable donation tax deduction on their tax returns. However, for tax year 2022, taxpayers who don\\'t itemize and who take the standard deduction, won\\'t be able to deduct their charitable contributions.\\n- 12 More people may be eligible for the Premium Tax Credit. For tax year 2022, taxpayers may qualify for temporarily expanded eligibility for the premium tax credit.\\n- 13 Eligibility rules changed to claim a tax credit for clean vehicles. Review the changes under the Inflation Reduction Act of 2022 to qualify for a Clean Vehicle Credit.\\n\\nFigure 8: LlamaIndex example prompt for IRS chatbot guidelines.\\n\\n## D MODULES\\n\\n## D.1 PREDICT\\n\\n```\\n1 class Predict(dspy.Module): 2 def __init__(self, signature , **config): 3 self.signature = dspy.Signature(signature) 4 self.config = config 5 6 # Module Parameters. 7 self.lm = dspy.ParameterLM(None) # use the default LM 8 self.demonstrations = dspy.ParameterDemonstrations([]) 9 10 def forward(self, **kwargs): 11 lm = get_the_right_lm(self.lm, kwargs) 12 signature = get_the_right_signature(self.signature , kwargs) 13 demonstrations = get_the_right_demonstrations(self.demonstrations , kwargs) 14 15 prompt = signature(demos=self.demos, **kwargs) 16 completions = lm.generate(prompt , **self.config) 17 prediction = Prediction.from_completions(completions , signature=signature) 18 19 if dsp.settings.compiling is not None: 20 trace = dict(predictor=self, inputs=kwargs , outputs=prediction) 21 dspy.settings.traces.append(trace) 22 23 return prediction\\n```\\n\\n## D.2 CHAIN OF THOUGHT\\n\\n```\\n1 class ChainOfThought(dspy.Module): 2 def __init__(self, signature): 3 4 # Modify signature from \\'*inputs -> *outputs \\' to \\'*inputs -> rationale , *outputs \\'. 5 rationale_field = dspy.OutputField(prefix=\"Reasoning: Let\\'s think step by step.\") 6 signature = dspy.Signature(signature).prepend_output_field(rationale_field) 7 8 # Declare a sub-module with the modified signature. 9 self.predict = dspy.Predict(self.signature) 10 11 def forward(self, **kwargs): 12 # Just forward the inputs to the sub-module. 13 return self.predict(**kwargs)\\n```\\n\\n## E TELEPROMPTERS\\n\\n## E.1 BOOTSTRAPFEWSHOT\\n\\n```\\n1 class SimplifiedBootstrapFewShot(Teleprompter): 2 def __init__(self, metric=None): 3 self.metric = metric 4 5 def compile(self, student , trainset , teacher=None): 6 teacher = teacher if teacher is not None else student 7 compiled_program = student.deepcopy() 8 9 # Step 1. Prepare mappings between student and teacher Predict modules. 10 # Note: other modules will rely on Predict internally. 11 assert student_and_teacher_have_compatible_predict_modules(student , teacher) 12 name2predictor , predictor2name = map_predictors_recursively(student , teacher) 13 14 # Step 2. Bootstrap traces for each Predict module. 15 # We\\'ll loop over the training set. We\\'ll try each example once for simplicity. 16 for example in trainset: 17 if we_found_enough_bootstrapped_demos(): break 18 19 # turn on compiling mode which will allow us to keep track of the traces 20 with dspy.setting.context(compiling=True): 21 # run the teacher program on the example , and get its final prediction 22 # note that compiling=True may affect the internal behavior here 23 prediction = teacher(**example.inputs()) 24 25 # get the trace of the all interal Predict calls from teacher program 26 predicted_traces = dspy.settings.trace 27 28 # if the prediction is valid , add the example to the traces 29 if self.metric(example , prediction , predicted_traces): 30 for predictor , inputs , outputs in predicted_traces: 31 d = dspy.Example(automated=True, **inputs , **outputs) 32 predictor_name = self.predictor2name[id(predictor)] 33 compiled_program[predictor_name].demonstrations.append(d) 34 35 36 return compiled_program E.2 BOOTSTRAPFEWSHOTWITHRANDOMSEARCH\\n```\\n\\n```\\n1 class SimplifiedBootstrapFewShotWithRandomSearch(Teleprompter): 2 def __init__(self, metric = None , trials=16): 3 self.metric = metric 4 self.trials = trials 5 6 def compile(self, student , *, teacher=None, trainset , valset=None): 7 # we can do forms of cross -validation if valset is unset. 8 valset = trainset if valset is None else valset 9 candidates = [] for seed in range(self.trials): # Create a new basic bootstrap few-shot program. shuffled_trainset = shuffle(trainset , seed=seed) tp = BootstrapFewShot(metric=metric , max_bootstrap_demos=random_size()) candidate_program = tp.compile(student , shuffled_trainset , teacher) # Step 2: Evaluate the generated candidate program. score = evaluate_program(candidate_program , self.metric , valset) candidates.append((score, candidate_program)) # return the best candidate program. return max(candidates , key=lambda x: x[0])[1]\\n```\\n\\n```\\n10 11 12 13 14 15 16 17 18 19 20 21 22\\n```\\n\\n## E.3 BOOTSTRAPFEWSHOTWITHOPTUNA\\n\\n```\\n1 class SimplifiedBootstrapFewShotWithOptuna(Teleprompter): 2 def __init__(self, metric , trials=16): 3 self.metric = metric 4 self.trials = trials 5 6 def objective(self, trial): 7 pool = self.pool 8 9 # Step 1: Create copy of student program. 10 candidate_program = self.student.reset_copy() 11 12 # Step 2: Based on trial , select demos for each predictor in program. 13 # Note. For simplicity , we can just select a single demo for each predictor. 14 # But we can easily tune the number of demonstrations to select here. 15 for (name, predictor1), (_, predictor2) in \\\\ 16 zip(pool.named_predictors(), candidate_program.named_predictors()): 17 all_demos = predictor1.demos 18 demo_index = trial.suggest_int(f\"demo_index_for_{name}\", 0, len(all_demos) -1) 19 predictor2.demos = [all_demos[demo_index]] 20 21 # Step 3: Evaluate the modified candidate program. 22 score = evaluate_program(candidate_program , self.metric , self.valset) 23 24 # Step 4: Store the candidate for Optuna to select highest -scoring program. 25 trial.set_user_attr(\"program\", candidate_program) 26 return score 27 28 def compile(self, student , trainset , teacher=None, valset=None): 29 self.trainset = trainset 30 self.valset = trainset if valset is None else valset 31 32 self.student = student.deepcopy() 33 self.teacher = teacher.deepcopy() if teacher else student.deepcopy() 34 35 # Leverage BootstrapFewshot to create a large number of potential demonstrations. 36 tp = BootstrapFewShot() 37 self.pool = tp.compile(self.student , self.teacher , self.trainset , self.metric) 38 39 # Use Optuna to find the best program by optimizing the objective function. 40 best_program = optimize_with_optuna(self.objective) 41 42 print(\\'Best score:\\', best_program.score) 43 print(\\'Best program:\\', best_program) 44 return best_program\\n```\\n\\n## F EXAMPLES OF THE PROMPTS AUTOMATICALLY GENERATED BY DSPY\\n\\nFor GSM8K, we include the prompt bootstrapped by DSPy for GSM8K llama2-13b-chat for the vanilla program compiled with bootstrap × 2 in Figure 9.\\n\\nWe also include a CoT prompt for GSM8K and a generate query prompt from the multihop program for HotPotQA. All of these, particularly their demonstrations\\' labels and their selection, are generated by DSPy automatically using llama2-13b-chat .\\n\\n```\\n1 Given the fields \\'question\\', produce the fields \\'answer\\'. 2 3 ---4 5 Follow the following format. 6 7 Question: $ { question } 8 Answer: $ { answer } 9 10 ---11 12 Question: Jimmy and Irene go shopping for clothes on a Tuesday, where senior citizens get a 10% discount on their purchases. Jimmy picks out 3 shorts from the $15 rack. Irene grabs 5 shirts from the $17 rack. How much money do they give to the cashier? 13 Answer: Jimmy picks out 3 shorts at $15 each = $45. Irene grabs 5 shirts at $17 each = $85. Total cost = $45 + $85 = $130. Since senior citizens get a 10% discount, they will pay 10% of $130 = $13. So, they will give the cashier $130 -$13 = $117. ---14 15 ---16 17 Question:\\n```\\n\\nFigure 9: Copy of the prompt automatically generated by DSPy for GSM8K Llama2-13b-chat vanilla program compiled with bootstrap × 2 .\\n\\nFigure 10: Shortened copy of the prompt automatically generated by DSPy for GSM8K Llama213b-chat CoT program compiled with bootstrap .\\n\\n<!-- image -->\\n\\n```\\n1 Given the fields \\'context\\', \\'question\\', produce the fields \\'search query\\'. 2 3 ---4 5 Follow the following format. 6 7 Context: $ { context } 8 Question: $ { question } 9 Reasoning: Let\\'s think step by step in order to $ { produce the search query } . We ... 10 Search Query: $ { search query } 11 12 ---13 14 Context: 15 [1] Twilight (novel series) | Twilight is a series of four vampire-themed fantasy romance novels by American author Stephenie Meyer. ... 16 [2] Harper Connelly Mysteries | The Harper Connelly Mysteries is a series of fantasy mystery novels written by Charlaine Harris, and first published in 2005. ... 17 [3] The Dark Heroine | The Dark Heroine is a series of vampire-themed fantasy romance novels written by English author Abigail Gibbs, published by HarperCollins in 2012. ... 18 19 Question: In which year was the first of the vampire-themed fantasy romance novels for which The Twilight Saga: The Official Illustrated Guide serves as a spin-off encyclopedic reference book first published? 20 21 Reasoning: Let\\'s think step by step in order to determine the year the first of the vampire-themed fantasy romance novels was first published. ... 22 23 Search Query: When was the first of the vampire-themed fantasy romance novels published? 24 25 ---26 27 Context: 28 [1] The Victorians | The Victorians -Their Story In Pictures is a 2009 British documentary series which focuses on Victorian art and culture. ... 29 [2] The Caxtons | The Caxtons: A Family Picture is an 1849 Victorian novel by Edward Bulwer-Lytton that was popular in its time. 30 [3] Victorian (comics) | The Victorian is a 25-issue comic book series published by Penny-Farthing Press and starting in 1999. ... 31 32 Question: The Victorians - Their Story In Pictures is a documentary series written by an author born in what year? 33 34 Reasoning: Let\\'s think step by step in order to produce the search query. We know that the documentary series is about Victorian art and culture, and it was written and presented by Jeremy Paxman. Therefore, we need to find the year in which Jeremy Paxman was born. 35 36 Search Query: Jeremy Paxman birth year 37 38 ---39 40 41 Context:\\n```\\n\\nFigure 11: Shortened copy of the prompt automatically generated by DSPy for HotPotQA Llama213b-chat multi-hop program (generating second hop query) compiled with bootstrap .', path=None, url=None, mimetype=None), image_resource=None, audio_resource=None, video_resource=None, text_template='{metadata_str}\\n\\n{content}'),\n",
       " Document(id_='97bf2013-c24f-4db8-ba8a-b24b15930265', embedding=None, metadata={}, excluded_embed_metadata_keys=[], excluded_llm_metadata_keys=[], relationships={}, metadata_template='{key}: {value}', metadata_separator='\\n', text_resource=MediaResource(embeddings=None, data=None, text='Rational AI\\n\\n## Your Data, Your AI\\n\\nEnsuring a safe LLM adoption with Rational AI\\n\\nSince 2016, we\\'ve foster frictionless been developing enabling technologies to collaboration between humans and machines.\\n\\nWe specialize in:\\n\\n- ↦ training cutting-edge LLMs\\n- ↦ building complex platforms\\n- ↦ ensure trustworthy AI development\\n\\nOur design studio creates user-centric experiences , and our system integration expertise guarantees seamless operation.\\n\\nWith a prestigious clientele and a global network of collaborators, we\\'re your trusted partner for achieving your goals.\\n\\n<!-- image -->\\n\\n## Intercultura\\n\\nLEONARDO\\n\\nGoogle org\\n\\n<!-- image -->\\n\\n<!-- image -->\\n\\n<!-- image -->\\n\\n<!-- image -->\\n\\n## list\\n\\n<!-- image -->\\n\\n## AI adoption in Italy\\n\\n<!-- image -->\\n\\n<!-- image -->\\n\\n95% \\ue09895%\\n\\nPrefer hybrid or on-premises solutions Would prefer hybrid or on-premises solutions\\n\\n## Top fields of application\\n\\n<!-- image -->\\n\\nSource: \\'Intelligenza Artificiale in Italia - La rivoluzione che sta cambiando il businessˮ. Minsait with all\\'Università Luiss Guido Carli, 2024. Sample of 502 organizations in 11 sectors.\\n\\n## The italian market\\n\\n<!-- image -->\\n\\n- Data analysis &amp; info extraction\\n- Language interpretation\\n- Customer recomendations\\n- Video &amp; Photo analysis\\n- Process Orchestration Systems\\n- Generative Al\\n\\n37%\\n\\nCompanies that will activate their first AI project within 12 months\\n\\nSource: Osservatorio Artificial Intelligence della School of Management of Politecnico di Milano, 2024\\n\\nBy 2023 the Italian AI market will be worth 760M€.\\n\\nThe global market\\n\\n2024\\n\\n1.811,75 B\\n\\n€\\n\\n760 M€\\n\\n279,22\\n\\nB$\\n\\nIn 2024 the world market is worth 279.22 billion $ (the circle green).\\n\\nThe revenue forecast in 2030 is 1,811.75 billion $ (the circle red)\\n\\nThe growth rate CAGR of 36.6% from 2024 to 2030\\n\\nSource: Grand View Research, 2024. The ratio between circles is calculated using area as the proportionality value.\\n\\n<!-- image -->\\n\\n\"Is now the right time to adopt AI technologies?\"\\n\\n...donʼt be late.\\n\\n## The Black Box Issue\\n\\nLack of transparency in training methods, unreliable data sources, and questionable development choices are hindering AI adoption among individuals, businesses, and organizations worldwide.\\n\\n<!-- image -->\\n\\n## Data Ownership and Control\\n\\nWhen organizations share data with AI providers, they risk losing control over how their proprietary information is used. Sensitive data may be incorporated into AI models without clear consent, raising concerns about intellectual property and potential misuse. Ensuring data protection and maintaining ownership rights becomes a key issue when relying on external AI solutions.\\n\\n<!-- image -->\\n\\n## Privacy, Security, and Compliance\\n\\nAI systems must navigate regulations like GDPR, CCPA, and the upcoming AI Act, designed to protect personal data. However, as AI uses massive datasets, ensuring compliance and preventing security breaches can be challenging . Striking a balance between innovation, privacy, and security will remain crucial as AI grows more complex and integrated into sensitive industries.\\n\\n<!-- image -->\\n\\n## Accountability and Reliability\\n\\nLLMs sometimes produce unreliable or fabricated responses, (hallucinations) , which undermine trust in AI systems. These issues, combined with the difficulty in understanding how decisions are made, complicate accountability . In high-stakes areas like healthcare or law, ensuring the reliability of AI-generated outputs is essential to avoid serious errors and liability concerns.\\n\\n## We all share the same problems…\\n\\n<!-- image -->\\n\\n<!-- image -->\\n\\n<!-- image -->\\n\\n## Overwhelming unorganized data\\n\\nNew employees may find it difficult to navigate and filter through a large amount of data and information that exists in a organization KB which can be scattered and not categorized optimally .\\n\\nIt can be challenging to find, elaborate and use the relevant information in short timeframes.\\n\\n## Cost of customer service\\n\\nDeliver constant and insightful feedback and help to customers through service portals, chats, ticketing systems and mails require a high headcount and high costs for salaries and continuous formation.\\n\\nScaling support for more customers is both costly and slow .\\n\\n## Lack of context and pertinence\\n\\nThe knowledge base may contain the information needed, but doesʼt have context about the userʼs problem, neither  their background, preferences, history (searches, activity…).\\n\\nThe answers are often generic, donʼt get to the point or are not relevant to the user specific case.\\n\\n<!-- image -->\\n\\n## GDPR compliance and security\\n\\nLLMs can pose risks like data leakage and privacy violations by mishandling sensitive data or being exploited maliciously.\\n\\nFor instance, files uploaded to ChatGPT may be used in training, and once included, cannot be removed, violating the Right to Be Forgotten.\\n\\n## Donʼt be like Google, avoid hallucinations\\n\\n<!-- image -->\\n\\n<!-- image -->\\n\\nWHO\\n\\nWHAT\\n\\nWe build trustworthy enterprise LLM\\n\\nmodels and privacy-preserving AI technologies.\\n\\nWe redefine how organizations integrate, consume and share information to ensure a safe LLM adoption for Enterprises.\\n\\n## Technology features…\\n\\n<!-- image -->\\n\\n<!-- image -->\\n\\n## Knowledge consolidation\\n\\nRational AI solutions extract, connect and summarize information from various sources integrating chatbots, email, ticketing systems, CRM, ERP, company documentation and much more.\\n\\n## Context understanding\\n\\nRational AI solutions go beyond simple information retrieval. They analyze the context of user queries to provide accurate and relevant answers.\\n\\n<!-- image -->\\n\\n## Continuous adaptation\\n\\nRational AI solutions use advanced techniques like RAG to adapt dynamically to the evolution of business and information . The knowledge base remains up-to-date and always in line with the latest developments.\\n\\n## …which bring extraordinary benefits\\n\\n<!-- image -->\\n\\nCustom LLMs trained on proprietary data keeps sensitive information secure and compliant with data privacy regulations.\\n\\n<!-- image -->\\n\\n<!-- image -->\\n\\nHost your custom LLMs on your preferred infrastructure for full control and ownership over model access, usage &amp; availability.\\n\\n<!-- image -->\\n\\nScale AI cost-effectively as your business grows, leveraging existing infrastructure and avoiding vendor lock-in .\\n\\nEmpower highly accurate and relevant AI solutions tailored to your unique business challenges and domain-specific data.\\n\\n## Rational AI Solutions\\n\\n<!-- image -->\\n\\n<!-- image -->\\n\\n<!-- image -->\\n\\n<!-- image -->\\n\\n<!-- image -->\\n\\n<!-- image -->\\n\\n## KNOWLEDGE\\n\\nOptimizes knowledge management within organizations\\n\\n- CARE\\n\\nEmpowers virtual agents with AI trained on your data to deliver superior CX\\n\\n<!-- image -->\\n\\n- GEN\\n\\n- VISION\\n\\n<!-- image -->\\n\\n- REPUTATION\\n\\n<!-- image -->\\n\\n## Optimizes knowledge management within organizations\\n\\nAnalyze and consolidate data from various formats such as PDFs, spreadsheets, DBs, CRMs, and ERPs. Access actionable knowledge thanks to forecasts, summaries, and visualizations to simplify onboarding and facilitate efficient data retrieval.\\n\\n<!-- image -->\\n\\n<!-- image -->\\n\\nCARE\\n\\n## Empowers virtual agents with AI trained on your data to deliver superior CX\\n\\nSupport your customers and users with timely, insightful and accurate conversational assistance by leveraging your policies, knowledge base, and internal data.\\n\\n<!-- image -->\\n\\n## KNOWLEDGE\\n\\n## Optimizes knowledge management within organizations\\n\\nAnalyze and consolidate data from various formats such as PDFs, spreadsheets, DBs, CRMs, and ERPs. Access actionable knowledge thanks to forecasts, summaries, and visualizations to simplify onboarding and facilitate efficient data retrieval.\\n\\n<!-- image -->\\n\\n## CARE\\n\\nEmpowers virtual agents with AI trained on your data to deliver superior CX\\n\\nSupport your customers and users with timely, insightful and accurate conversational assistance by leveraging your policies, knowledge base, and internal data.\\n\\n<!-- image -->\\n\\n## GEN\\n\\n## Helps companies integrate generative AI into their workflows\\n\\nGenerate coherent and personalized content, from text to images and audio, and integrate it into your infrastructure, website and workflow to improve productivity and creativity.\\n\\n<!-- image -->\\n\\n## VISION\\n\\nDelivers advanced segmentation and recognition of both physical and digital objects\\n\\nDetect tables, interpret charts, digitize paper documents, and enable interaction with them. Equip your LLMs with state-of-the-art multimodal capabilities to enhance their understanding and responsiveness.\\n\\n<!-- image -->\\n\\n## BASE\\n\\nEnables the fastest way to improve and evolve your LLMs knowledge\\n\\nCurate your LLM datasets by adding, removing and editing information leveraging integrations with third parties datastreams and simple upload of documents, PDFs, spreadsheets and more.\\n\\n<!-- image -->\\n\\n## REPUTATION\\n\\nProvides real-time monitoring of your brand and products across the web\\n\\nAnalyze feedback from reviews, social media, and news outlets, to gain in-depth insights into reputation dynamics and make informed decisions to protect and enhance your brand image.\\n\\n## Ownership &amp; Hosting\\n\\n<!-- image -->\\n\\n## Ownership and IP of the\\n\\nmodel , alongside customer-specific code used to train and operate the model, will be transferred to the client.\\n\\n<!-- image -->\\n\\nGeckosoft retains the IP of Rationale AI Control Room and related no-code solutions . The models can be deployed and trained independently of the Rationale AI platform, avoiding vendor lock-in.\\n\\n<!-- image -->\\n\\n<!-- image -->\\n\\n<!-- image -->\\n\\nModels, data and web apps can be hosted in three ways :\\n\\n- 1. On-premise on clientʼs private infrastructure\\n- 2. Your favorite public cloud , managed by you\\n- 3. Rational AI Cloud \\ue081AWS\\ue082\\n\\nMulti environment deploy possible for every option.\\n\\n<!-- image -->\\n\\nWITH RATIONAL KNOWLEDGE\\ue092\\n\\nAnalyze and consolidate data from various formats and external sources such as PDFs, spreadsheets, DBs, CRMs, and ERPs.\\n\\nIntegrate third-parties services to access your organization data and build knowledge continuously with RAG technology to enhance productivity and efficiency in everyday operations.\\n\\nAccess Rational Knowledge through custom interfaces in your platform or leverage our plug-and-play conversational agent chats.\\n\\n<!-- image -->\\n\\nWITH RATIONAL CARE\\ue092\\n\\nLeverage information from your policies, knowledge base, and internal data, to provide timely, cost-effective, and accurate assistance at scale.\\n\\nAutomate customer support with smart conversational agents with deep knowledge of your business, documentation, and users history to resolve up to 80% of tickets without human intervention.\\n\\nPersonalize customer experience with AI-powered chats, knowledge bases and search tools integrated in your website, shop or mobile app.\\n\\n<!-- image -->\\n\\nWITH RATIONAL BASE\\ue092\\n\\nUpdate your custom LLMs pulling in raw and unstructured data from databases, ERP systems, websites, shops, and documents.\\n\\nIntegrate third-parties services to access your organization and customersʼ data and build knowledge.\\n\\nCurate your LLMs knowledge by removing, updating and adding information to keep them always aligned with your business and IP evolutions.\\n\\n<!-- image -->\\n\\nWITH RATIONAL GEN\\ue092\\n\\nGenerate coherent and personalized content leveraging existing data: text, images, graphs, audio, forecasts, all to enhance creativity and productivity.\\n\\nIntegrate third-parties services to access continuously data and build knowledge live with RAG technology, keeping aligned with your partners and customers.\\n\\nScale your business capabilities starting from your existing data: test product variations, simulate market responses, develop innovative solutions, generate content to expand your IP.\\n\\n## Rational AI\\n\\n## Components\\n\\n<!-- image -->\\n\\n- ↦ Customization : Built specifically around your organizationʼs data, tailored LLMs deliver highly accurate, domain-specific solutions that meet your businessʼs unique needs.\\n- ↦ Enhanced Security &amp; Privacy : Your data stays within your control, ensuring compliance with data protection regulations and reducing the risk of breaches.\\n- ↦ Cost Control : Scale AI cost-effectively, avoiding unnecessary expenses while leveraging your own infrastructure.\\n- ↦ Full Control : Maintain complete oversight on who accesses the models and which data feed it, ensuring maximum security.\\n\\nAI Control Room\\n\\nDATA INPUT\\n\\nChatbots\\n\\nEmails\\n\\nSocial media chats\\n\\nTicketing systems\\n\\nCRMs, ERPs\\n\\nShop orders\\n\\nRational AI\\n\\nStatus\\n\\nOpen\\n\\nClosed\\n\\nHuman needed\\n\\nTopic\\n\\nRelunds\\n\\nDosage\\n\\nComplaini\\n\\nChildren\\n\\nMore\\n\\nSentiment\\n\\nFriendly\\n\\nPolite\\n\\nMeutral\\n\\nFrustrated\\n\\nAngry\\n\\nHostile\\n\\nDashboard\\n\\nUser\\n\\nHailee €.\\n\\nManya G.\\n\\nRois A.\\n\\nRozanna G.\\n\\nNara €.\\n\\nBellina\\n\\nDorelle G.\\n\\nSidonia P\\n\\nTerrijo A.\\n\\nTammie U.\\n\\nBrinna P.\\n\\nMagda R.\\n\\nLynde L\\n\\nAI PROCESSING\\n\\nConversations\\n\\nSettings\\n\\nTopic\\n\\nCan you provide infor\\\\_\\n\\n0 What is the recomme\\n\\n0 Ihave recently starte\\\\_\\\\_\\n\\ncall\\\\_\\n\\n0 Is there generic ver \\\\_\\n\\n0 \"m having trouble fin \\\\_\\n\\n0 Could you explain the\\\\_\\n\\n0 Thanksi\\n\\n0 Is it safe to consume\\\\_\\n\\n0 Thanks!\\n\\n0 Thank you\\n\\nHave nice day :\\n\\nUnusual syntomps\\n\\nRelunds\\n\\nUnusual syntomps\\n\\nRelunds\\n\\nRelunds\\n\\nUnusual syntomps\\n\\nRelunds\\n\\nReturn Device\\n\\nDosage\\n\\nDosage\\n\\nComplaint\\n\\nChildren children\\n\\nSentiment\\n\\nPolite 86%\\n\\nPolite 86%\\n\\nPolite 86%\\n\\nPolite 86%\\n\\nAngry 86%\\n\\nPolite 86%\\n\\nFrustrated 863\\n\\nPolite 86%\\n\\nPolite 86%\\n\\nPolite 86%\\n\\nFriendly 863\\n\\nPolite 86%\\n\\n863\\n\\nHapDy\\n\\nFriendly 863\\n\\nLast message\\n\\n09.41\\n\\nmiutes ago miutes ag0\\n\\n10 miutes ago\\n\\n16*13\\n\\n15.45\\n\\n15-32\\n\\n22 Aug 23\\n\\n21 Aug 23\\n\\n19 Aug 23\\n\\n11 Aug 23\\n\\nSearch user \\\\_\\n\\nStatus\\n\\nOpen\\n\\nOpen\\n\\nOpen\\n\\nOpen\\n\\nHuman needed\\n\\nOpen\\n\\nOpen\\n\\nOpen\\n\\nClosed\\n\\nOpen\\n\\nClosed\\n\\nClosed\\n\\nClosed\\n\\nClosed\\n\\nINFORMATION OUTPUT\\n\\nHappy 76%\\n\\nHappy 76%\\n\\nPolite 86%\\n\\nAngry 56%\\n\\nPolite 86%\\n\\nAngry 56%\\n\\nLive sentiment analysis\\n\\nof the conversation with critical\\n\\nmessages notifications\\n\\nSuggestion C\\n\\nAction\\n\\nAnswers and proactive actions\\n\\nbased on userʼs account info and history\\n\\nReviews, Opinions\\n\\nHistoric Data\\n\\nTrends, analytics and proactive suggestions to\\n\\nimprove your operations\\n\\nTrend €\\n\\nAction B\\n\\nAI Control Room\\n\\nDATA INPUT\\n\\nChatbots\\n\\nEmails\\n\\nSocial media chats\\n\\nTicketing systems\\n\\nCRMs, ERPs\\n\\nShop orders\\n\\nRational AI\\n\\nUser\\n\\nManya €.\\n\\nRois F.\\n\\nRozanna G.\\n\\nNara E.\\n\\nBellina A\\n\\nCarmita G.\\n\\nSidonia P.\\n\\nTerrijo L.\\n\\nTammie M.\\n\\nBrinna F.\\n\\nMagda\\n\\nLynde\\n\\nDashboard\\n\\nAI PROCESSING\\n\\nConversations\\n\\nConversatlon\\n\\nSettings\\n\\nDetails\\n\\nCan vou provide information on\\n\\nTyping\\n\\nWhat is the recommended dosag\\n\\nIhave recently started taking call this bullshit\\n\\nIs Ihere generic version availab\\n\\nCould you explain the interaction\\n\\nThanks!\\n\\nIs it safe\\n\\nThanksi\\n\\nThank you\\n\\nHave nice day\\n\\nYes consume alcohol whil\\n\\nModel\\n\\nBasic info\\n\\nUser profile\\n\\nGIVEN NAME\\n\\nRozanna\\n\\nGwinth\\n\\nFurther details\\n\\nGender\\n\\nADDRESS\\n\\nLast orders\\n\\n13 Oct 23\\n\\nTachipirina 500.. FA-4294953\\n\\nApr 23\\n\\n18 Jan 23\\n\\nFA-2948549\\n\\nSynflex\\n\\nFA-1485914\\n\\n4443476393641\\n\\n22 April 1990\\n\\nFemale\\n\\nVia Volturno 39,56126,\\n\\nPisa, PisaItalia com\\n\\nRozanna G.\\n\\nPolite\\n\\n4\\n\\nensure there are no specific concerns with your prescribed antibiotic\\n\\nRozanna Gwinth question\\n\\nRozanna Gwinth\\n\\nIm currently taking an antibiotic for sinus\\n\\ninfection.\\n\\nsafe to take Tachipirina alongside it?\\n\\nSmart Assistant\\n\\n2 min ago 16\\n\\nIt\\'s generally safe {0 take Tachipirina with consult with your doctor or pharmacist t0\\n\\nensure there are no specific concerns with\\n\\nyour prescribed antibiotic.\\n\\nSmart Assistant\\n\\n2 min ago 1G\\n\\nyou are thinking assume Tachipirina,\\n\\nplease read carefully the manual before:\\n\\nRozanna Gwinth\\n\\nOk, thank voul\\n\\nTake overthe conversation\\n\\n3 min ago\\n\\nSummary\\n\\nINFORMATION OUTPUT\\n\\nHappy 76%\\n\\nHappy 76%\\n\\nPolite 86%\\n\\nAngry 56%\\n\\nPolite 86%\\n\\nAngry 56%\\n\\nLive sentiment analysis\\n\\nof the conversation with critical\\n\\nmessages notifications\\n\\nSuggestion C\\n\\nAction\\n\\nAnswers and proactive actions\\n\\nbased on userʼs account info and history\\n\\nReviews, Opinions\\n\\nHistoric Data\\n\\nTrends, analytics and proactive suggestions to\\n\\nimprove your operations\\n\\nTrend €\\n\\nAction B\\n\\n<!-- image -->\\n\\n## Conversational AI Agents\\n\\n## ↦ Multi-channel\\n\\nInteract with the Smart Assistant with l ive chats, ticketing systems, WhatsApp, voice calls, and much more\\n\\n## ↦ Multi-modal\\n\\nUpload files, pictures, documentsʼ scan to analyze, understand and summarize information ; ask for summaries, analytics and insights\\n\\n## ↦ Interactive\\n\\nServe different and custom interactive widgets to improve the user experience and delight the user\\n\\n<!-- image -->\\n\\n## Rational AI Case Studies\\n\\n## SBE\\ue088VARVIT\\n\\nSBE\\ue088VARVIT is a global leader in fastener production, with an annual capacity exceeding 100,000 tons. Their products are exported to over 70 countries worldwide.\\n\\n- ↦ Instant Document Retrieval : company documentation is made readily available through a chatbot agent, shortening offer propositions and product planning timelines.\\n- ↦ Automated Integration : the system is able to comprehend client specifications from PDFs and converts them into actionable plans fully integrated with the company\\'s ERP.\\n\\n<!-- image -->\\n\\n<!-- image -->\\n\\n## Bodoni\\n\\nThe Bodoni project , born from the collaboration between Geckosoft, FIEG, LUISS Data Lab, and funded by the Google.org fellowship, aims to combat disinformation and promote reliable information. In this context, Rational AI has powerer two of the main platform components: the CoEditor and the Explore section.\\n\\n- ↦ CoEditor : an AI-powered writing assistant that helps journalists improve their articles by suggesting sources, verifying facts in real-time, and reducing the risk of disinformation.\\n- ↦ Explore section : user can interact with natural language to ask explore and the graph of news and topics to identify stories, outliers, and trending topics.\\n\\nThanks to Rational AI, the Bodoni project aims to provide media professionals with advanced tools to enhance the quality of journalism and fight fake news.\\n\\n## *Bodoni\\n\\n<!-- image -->\\n\\n## Imperatore Travel\\n\\nImperatore Travel is a leading agency, offering bespoke travel experiences. They cater to clients worldwide, with an annual turnover of over €50M.\\n\\n- ↦ We curated an extensive knowledge base of hotels, restaurants, and experiences to enrich every journey with the best recommendations and insights.\\n- ↦ Personalized travel recommendations are crafted to suit each customer\\'s unique preferences, leveraging their travel history and current trends.\\n- ↦ The 24/7 Chatbot support , trained on over 200,000 customer care emails, provides real-time assistance on travel queries, personalized itinerary suggestions,, automated complaint management and automated cancellation processes.\\n\\nHello Mario! How can help?\\n\\n<!-- image -->\\n\\n## You\\n\\n3 min ago\\n\\nCan you suggest me some places to visit in Amman? I\\'ve only a few hours to visit the city and don\\'t want to take a taxi or else to move got\\n\\n<!-- image -->\\n\\n## Smart Assistant\\n\\n<!-- image -->\\n\\nSure, here you can find the main attractions of Amman. I\\'ve selected the most important and near to each other; so you can manage to visit them all in just a few hours.\\n\\n<!-- image -->\\n\\nCitadel Hill (Jabal alQala\\'a): This ancient citadel represents one of the most significant historical sites in Am\\n\\n<!-- image -->\\n\\n<!-- image -->\\n\\n## The Uluru\\n\\nUluru, a US-based platform offering personalized executive function coaching for kids, partnered with Rational AI to create solution to personalize coaching, monitor user interactions, and detect when students or parents needed extra help.\\n\\n- ↦ A virtual agent that delivers personalized exercises based on psychometric screening and student profiles.\\n- ↦ Real-time monitoring of user interactions, enabling human intervention when AI support is insufficient.\\n- ↦ AI-powered tools to re-format assignments into manageable steps for each studentʼs needs, with reminders and time estimates tailored to individual student needs.\\n\\n<!-- image -->\\n\\n<!-- image -->\\n\\n## People\\n\\nPeople S.p.A. is a prominent HR solutions provider, offering innovative staffing and recruitment services. They operate globally, connecting businesses with top talent and enhancing workforce efficiency across various industries.\\n\\n- ↦ Indexing &amp; Accessibility : employees can easily access their accounting information, including leave balances, severance pay, and insurance, through WhatsApp.\\n- ↦ Enhanced Security : careful attention to securing access to personal information ensures privacy and protection of sensitive data, maintaining confidentiality at all times.\\n\\n<!-- image -->\\n\\n<!-- image -->\\n\\n## Lenovys Neym\\n\\nLenovys is an innovative company specializing in operational excellence, leadership, and performance improvement through Lean Thinking and agile methodologies. One of their flagship products is NEYM, a project management tool designed to optimize workflows and improve efficiency across various industries. Rational AI is significantly enhancing NEYM in the following ways:\\n\\n- ↦ AI assistant with context knowledge : Rational AI helps users retrieve and understand information from all ongoing projects and activities by leveraging contextual awareness. This makes it easier to navigate complex datasets and find relevant information quickly.\\n- ↦ Contextual help to improve input : The solution provides real-time recommendations, grammar and spell check, and autocomplete features based on both current context and historical data. This enhances the quality and speed of information entry across all project fields, improving overall productivity.\\n\\n## LENOVYS\\n\\n<!-- image -->\\n\\n## Tailored AI solution deployed in weeks, not years\\n\\nExecutive briefing\\n\\nTechnology assessment\\n\\nAI solution deployed in production\\n\\n2 Hours\\n\\n2\\ue0883 Days\\n\\n1 Week\\n\\n<!-- image -->\\n\\n<!-- image -->\\n\\nCEO Davide Anzalone davide@geckosoft.it\\n\\nCTO Fabio Severino severino@geckosoft.it\\n\\nAccount Manager\\n\\nMarco Zeo m.zeo@geckosoft.it\\n\\nhttps://rational.is https://geckosoft.it\\n\\n<!-- image -->', path=None, url=None, mimetype=None), image_resource=None, audio_resource=None, video_resource=None, text_template='{metadata_str}\\n\\n{content}')]"
      ]
     },
     "execution_count": 100,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "docs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 101,
   "id": "3f43cb9d",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(list, 3)"
      ]
     },
     "execution_count": 101,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "type(docs), len(docs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 102,
   "id": "182a5dcc",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<!-- image -->\n",
      "\n",
      "## Docling Technical Report\n",
      "\n",
      "Version 1.0\n",
      "\n",
      "Christoph Auer Maksym Lysak Ahmed Nassar Michele Dolfi Nikolaos Livathinos Panos Vagenas Cesar Berrospi Ramis Matteo Omenetti Fabian Lindlbauer Kasper Dinkla Lokesh Mishra Yusik Kim Shubham Gupta Rafael Teixeira de Lima Valery Weber Lucas Morin Ingmar Meijer Viktor Kuropiatnyk Peter W. J. Staar\n",
      "\n",
      "AI4K Group, IBM Research R¨ uschlikon, Switzerland\n",
      "\n",
      "## Abstract\n",
      "\n",
      "This technical report introduces Docling , an easy to use, self-contained, MITlicensed open-source package for PDF document conversion. It is powered by state-of-the-art specialized AI models for layout analysis (DocLayNet) and table structure recognition (TableFormer), and runs efficiently on commodity hardware in a small resource budget. The code interface allows for easy extensibility and addition of new features and models.\n",
      "\n",
      "## 1 Introduction\n",
      "\n",
      "Converting PDF documents back into a machine-processable format has been a major challenge for decades due to their huge variability in formats, weak standardization and printing-optimized characteristic, which discards most structural features and metadata. With the advent of LLMs and popular application patterns such as retrieval-augmented generation (RAG), leveraging the rich content embedded in PDFs has become ever more relevant. In the past decade, several powerful document understanding solutions have emerged on the market, most of which are commercial software, cloud offerings [3] and most recently, multi-modal vision-language models. As of today, only a handful of open-source tools cover PDF conversion, leaving a significant feature and quality gap to proprietary solutions.\n",
      "\n",
      "With Docling , we open-source a very capable and efficient document conversion tool which builds on the powerful, specialized AI models and datasets for layout analysis and table structure recognition we developed and presented in the recent past [12, 13, 9]. Docling is designed as a simple, self-contained python library with permissive license, running entirely locally on commodity hardware. Its code architecture allows for easy extensibility and addition of new features and models.\n",
      "\n",
      "Here is what Docling delivers today:\n",
      "\n",
      "- · Converts PDF documents to JSON or Markdown format, stable and lightning fast\n",
      "- · Understands detailed page layout, reading order, locates figures and recovers table structures\n",
      "- · Extracts metadata from the document, such as title, authors, references and language\n",
      "- · Optionally applies OCR, e.g. for scanned PDFs\n",
      "- · Can be configured to be optimal for batch-mode (i.e high throughput, low time-to-solution) or interactive mode (compromise on efficiency, low time-to-solution)\n",
      "- · Can leverage different accelerators (GPU, MPS, etc).\n",
      "\n",
      "## 2 Getting Started\n",
      "\n",
      "To use Docling, you can simply install the docling package from PyPI. Documentation and examples are available in our GitHub repository at github.com/DS4SD/docling. All required model assets 1 are downloaded to a local huggingface datasets cache on first use, unless you choose to pre-install the model assets in advance.\n",
      "\n",
      "Docling provides an easy code interface to convert PDF documents from file system, URLs or binary streams, and retrieve the output in either JSON or Markdown format. For convenience, separate methods are offered to convert single documents or batches of documents. A basic usage example is illustrated below. Further examples are available in the Doclign code repository.\n",
      "\n",
      "```\n",
      "from docling.document_converter import DocumentConverter Large\n",
      "```\n",
      "\n",
      "```\n",
      "source = \"https://arxiv.org/pdf/2206.01062\" # PDF path or URL converter = DocumentConverter() result = converter.convert_single(source) print(result.render_as_markdown()) # output: \"## DocLayNet: A Human -Annotated Dataset for Document -Layout Analysis [...]\"\n",
      "```\n",
      "\n",
      "Optionally, you can configure custom pipeline features and runtime options, such as turning on or off features (e.g. OCR, table structure recognition), enforcing limits on the input document size, and defining the budget of CPU threads. Advanced usage examples and options are documented in the README file. Docling also provides a Dockerfile to demonstrate how to install and run it inside a container.\n",
      "\n",
      "## 3 Processing pipeline\n",
      "\n",
      "Docling implements a linear pipeline of operations, which execute sequentially on each given document (see Fig. 1). Each document is first parsed by a PDF backend, which retrieves the programmatic text tokens, consisting of string content and its coordinates on the page, and also renders a bitmap image of each page to support downstream operations. Then, the standard model pipeline applies a sequence of AI models independently on every page in the document to extract features and content, such as layout and table structures. Finally, the results from all pages are aggregated and passed through a post-processing stage, which augments metadata, detects the document language, infers reading-order and eventually assembles a typed document object which can be serialized to JSON or Markdown.\n",
      "\n",
      "## 3.1 PDF backends\n",
      "\n",
      "Two basic requirements to process PDF documents in our pipeline are a) to retrieve all text content and their geometric coordinates on each page and b) to render the visual representation of each page as it would appear in a PDF viewer. Both these requirements are encapsulated in Docling's PDF backend interface. While there are several open-source PDF parsing libraries available for python, we faced major obstacles with all of them for different reasons, among which were restrictive\n",
      "\n",
      "1 see huggingface.co/ds4sd/docling-models/\n",
      "\n",
      "Figure 1: Sketch of Docling's default processing pipeline. The inner part of the model pipeline is easily customizable and extensible.\n",
      "\n",
      "<!-- image -->\n",
      "\n",
      "licensing (e.g. pymupdf [7]), poor speed or unrecoverable quality issues, such as merged text cells across far-apart text tokens or table columns (pypdfium, PyPDF) [15, 14].\n",
      "\n",
      "We therefore decided to provide multiple backend choices, and additionally open-source a custombuilt PDF parser, which is based on the low-level qpdf [4] library. It is made available in a separate package named docling-parse and powers the default PDF backend in Docling. As an alternative, we provide a PDF backend relying on pypdfium , which may be a safe backup choice in certain cases, e.g. if issues are seen with particular font encodings.\n",
      "\n",
      "## 3.2 AI models\n",
      "\n",
      "As part of Docling, we initially release two highly capable AI models to the open-source community, which have been developed and published recently by our team. The first model is a layout analysis model, an accurate object-detector for page elements [13]. The second model is TableFormer [12, 9], a state-of-the-art table structure recognition model. We provide the pre-trained weights (hosted on huggingface) and a separate package for the inference code as docling-ibm-models . Both models are also powering the open-access deepsearch-experience, our cloud-native service for knowledge exploration tasks.\n",
      "\n",
      "## Layout Analysis Model\n",
      "\n",
      "Our layout analysis model is an object-detector which predicts the bounding-boxes and classes of various elements on the image of a given page. Its architecture is derived from RT-DETR [16] and re-trained on DocLayNet [13], our popular human-annotated dataset for document-layout analysis, among other proprietary datasets. For inference, our implementation relies on the onnxruntime [5].\n",
      "\n",
      "The Docling pipeline feeds page images at 72 dpi resolution, which can be processed on a single CPU with sub-second latency. All predicted bounding-box proposals for document elements are post-processed to remove overlapping proposals based on confidence and size, and then intersected with the text tokens in the PDF to group them into meaningful and complete units such as paragraphs, section titles, list items, captions, figures or tables.\n",
      "\n",
      "## Table Structure Recognition\n",
      "\n",
      "The TableFormer model [12], first published in 2022 and since refined with a custom structure token language [9], is a vision-transformer model for table structure recovery. It can predict the logical row and column structure of a given table based on an input image, and determine which table cells belong to column headers, row headers or the table body. Compared to earlier approaches, TableFormer handles many characteristics of tables, such as partial or no borderlines, empty cells, rows or columns, cell spans and hierarchy both on column-heading or row-heading level, tables with inconsistent indentation or alignment and other complexities. For inference, our implementation relies on PyTorch [2].\n",
      "\n",
      "The Docling pipeline feeds all table objects detected in the layout analysis to the TableFormer model, by providing an image-crop of the table and the included text cells. TableFormer structure predictions are matched back to the PDF cells in post-processing to avoid expensive re-transcription text in the table image. Typical tables require between 2 and 6 seconds to be processed on a standard CPU, strongly depending on the amount of included table cells.\n",
      "\n",
      "## OCR\n",
      "\n",
      "Docling provides optional support for OCR, for example to cover scanned PDFs or content in bitmaps images embedded on a page. In our initial release, we rely on EasyOCR [1], a popular thirdparty OCR library with support for many languages. Docling, by default, feeds a high-resolution page image (216 dpi) to the OCR engine, to allow capturing small print detail in decent quality. While EasyOCR delivers reasonable transcription quality, we observe that it runs fairly slow on CPU (upwards of 30 seconds per page).\n",
      "\n",
      "We are actively seeking collaboration from the open-source community to extend Docling with additional OCR backends and speed improvements.\n",
      "\n",
      "## 3.3 Assembly\n",
      "\n",
      "In the final pipeline stage, Docling assembles all prediction results produced on each page into a well-defined datatype that encapsulates a converted document, as defined in the auxiliary package docling-core . The generated document object is passed through a post-processing model which leverages several algorithms to augment features, such as detection of the document language, correcting the reading order, matching figures with captions and labelling metadata such as title, authors and references. The final output can then be serialized to JSON or transformed into a Markdown representation at the users request.\n",
      "\n",
      "## 3.4 Extensibility\n",
      "\n",
      "Docling provides a straight-forward interface to extend its capabilities, namely the model pipeline. A model pipeline constitutes the central part in the processing, following initial document parsing and preceding output assembly, and can be fully customized by sub-classing from an abstract baseclass ( BaseModelPipeline ) or cloning the default model pipeline. This effectively allows to fully customize the chain of models, add or replace models, and introduce additional pipeline configuration parameters. To use a custom model pipeline, the custom pipeline class to instantiate can be provided as an argument to the main document conversion methods. We invite everyone in the community to propose additional or alternative models and improvements.\n",
      "\n",
      "Implementations of model classes must satisfy the python Callable interface. The \\_\\_call\\_\\_ method must accept an iterator over page objects, and produce another iterator over the page objects which were augmented with the additional features predicted by the model, by extending the provided PagePredictions data model accordingly.\n",
      "\n",
      "## 4 Performance\n",
      "\n",
      "In this section, we establish some reference numbers for the processing speed of Docling and the resource budget it requires. All tests in this section are run with default options on our standard test set distributed with Docling, which consists of three papers from arXiv and two IBM Redbooks, with a total of 225 pages. Measurements were taken using both available PDF backends on two different hardware systems: one MacBook Pro M3 Max, and one bare-metal server running Ubuntu 20.04 LTS on an Intel Xeon E5-2690 CPU. For reproducibility, we fixed the thread budget (through setting OMP NUM THREADS environment variable ) once to 4 (Docling default) and once to 16 (equal to full core count on the test hardware). All results are shown in Table 1.\n",
      "\n",
      "If you need to run Docling in very low-resource environments, please consider configuring the pypdfium backend. While it is faster and more memory efficient than the default docling-parse backend, it will come at the expense of worse quality results, especially in table structure recovery.\n",
      "\n",
      "Establishing GPU acceleration support for the AI models is currently work-in-progress and largely untested, but may work implicitly when CUDA is available and discovered by the onnxruntime and\n",
      "\n",
      "torch runtimes backing the Docling pipeline. We will deliver updates on this topic at in a future version of this report.\n",
      "\n",
      "Table 1: Runtime characteristics of Docling with the standard model pipeline and settings, on our test dataset of 225 pages, on two different systems. OCR is disabled. We show the time-to-solution (TTS), computed throughput in pages per second, and the peak memory used (resident set size) for both the Docling-native PDF backend and for the pypdfium backend, using 4 and 16 threads.\n",
      "\n",
      "| CPU                         | Thread budget   | native backend   | native backend   | native backend   | pypdfium backend   | pypdfium backend   | pypdfium backend   |\n",
      "|-----------------------------|-----------------|------------------|------------------|------------------|--------------------|--------------------|--------------------|\n",
      "|                             |                 | TTS              | Pages/s          | Mem              | TTS                | Pages/s            | Mem                |\n",
      "| Apple M3 Max                | 4               | 177 s 167 s      | 1.27 1.34        | 6.20 GB          | 103 s 92 s         | 2.18 2.45          | 2.56 GB            |\n",
      "| (16 cores) Intel(R) E5-2690 | 16 4 16         | 375 s 244 s      | 0.60 0.92        | 6.16 GB          | 239 s 143 s        | 0.94 1.57          | 2.42 GB            |\n",
      "\n",
      "## 5 Applications\n",
      "\n",
      "Thanks to the high-quality, richly structured document conversion achieved by Docling, its output qualifies for numerous downstream applications. For example, Docling can provide a base for detailed enterprise document search, passage retrieval or classification use-cases, or support knowledge extraction pipelines, allowing specific treatment of different structures in the document, such as tables, figures, section structure or references. For popular generative AI application patterns, such as retrieval-augmented generation (RAG), we provide quackling , an open-source package which capitalizes on Docling's feature-rich document output to enable document-native optimized vector embedding and chunking. It plugs in seamlessly with LLM frameworks such as LlamaIndex [8]. Since Docling is fast, stable and cheap to run, it also makes for an excellent choice to build document-derived datasets. With its powerful table structure recognition, it provides significant benefit to automated knowledge-base construction [11, 10]. Docling is also integrated within the open IBM data prep kit [6], which implements scalable data transforms to build large-scale multi-modal training datasets.\n",
      "\n",
      "## 6 Future work and contributions\n",
      "\n",
      "Docling is designed to allow easy extension of the model library and pipelines. In the future, we plan to extend Docling with several more models, such as a figure-classifier model, an equationrecognition model, a code-recognition model and more. This will help improve the quality of conversion for specific types of content, as well as augment extracted document metadata with additional information. Further investment into testing and optimizing GPU acceleration as well as improving the Docling-native PDF backend are on our roadmap, too.\n",
      "\n",
      "We encourage everyone to propose or implement additional features and models, and will gladly take your inputs and contributions under review . The codebase of Docling is open for use and contribution, under the MIT license agreement and in alignment with our contributing guidelines included in the Docling repository. If you use Docling in your projects, please consider citing this technical report.\n",
      "\n",
      "## References\n",
      "\n",
      "- [1] J. AI. Easyocr: Ready-to-use ocr with 80+ supported languages. https://github.com/ JaidedAI/EasyOCR , 2024. Version: 1.7.0.\n",
      "- [2] J. Ansel, E. Yang, H. He, N. Gimelshein, A. Jain, M. Voznesensky, B. Bao, P. Bell, D. Berard, E. Burovski, G. Chauhan, A. Chourdia, W. Constable, A. Desmaison, Z. DeVito, E. Ellison, W. Feng, J. Gong, M. Gschwind, B. Hirsh, S. Huang, K. Kalambarkar, L. Kirsch, M. Lazos, M. Lezcano, Y. Liang, J. Liang, Y. Lu, C. Luk, B. Maher, Y. Pan, C. Puhrsch, M. Reso, M. Saroufim, M. Y. Siraichi, H. Suk, M. Suo, P. Tillet, E. Wang, X. Wang, W. Wen, S. Zhang, X. Zhao, K. Zhou, R. Zou, A. Mathews, G. Chanan, P. Wu, and S. Chintala. Pytorch 2: Faster\n",
      "\n",
      "machine learning through dynamic python bytecode transformation and graph compilation. In Proceedings of the 29th ACM International Conference on Architectural Support for Programming Languages and Operating Systems, Volume 2 (ASPLOS '24) . ACM, 4 2024. doi: 10.1145/3620665.3640366. URL https://pytorch.org/assets/pytorch2-2.pdf .\n",
      "\n",
      "- [3] C. Auer, M. Dolfi, A. Carvalho, C. B. Ramis, and P. W. Staar. Delivering document conversion as a cloud service with high throughput and responsiveness. In 2022 IEEE 15th International Conference on Cloud Computing (CLOUD) , pages 363-373. IEEE, 2022.\n",
      "- [4] J. Berkenbilt. Qpdf: A content-preserving pdf document transformer, 2024. URL https: //github.com/qpdf/qpdf .\n",
      "- [5] O. R. developers. Onnx runtime. https://onnxruntime.ai/ , 2024. Version: 1.18.1.\n",
      "- [6] IBM. Data Prep Kit: a community project to democratize and accelerate unstructured data preparation for LLM app developers, 2024. URL https://github.com/IBM/ data-prep-kit .\n",
      "- [7] A. S. Inc. PyMuPDF, 2024. URL https://github.com/pymupdf/PyMuPDF .\n",
      "- [8] J. Liu. LlamaIndex, 11 2022. URL https://github.com/jerryjliu/llama\\_index .\n",
      "- [9] M. Lysak, A. Nassar, N. Livathinos, C. Auer, and P. Staar. Optimized Table Tokenization for Table Structure Recognition. In Document Analysis and Recognition - ICDAR 2023: 17th International Conference, San Jos´ e, CA, USA, August 21-26, 2023, Proceedings, Part II , pages 37-50, Berlin, Heidelberg, Aug. 2023. Springer-Verlag. ISBN 978-3-031-41678-1. doi: 10. 1007/978-3-031-41679-8 3. URL https://doi.org/10.1007/978-3-031-41679-8\\_3 .\n",
      "- [10] L. Mishra, S. Dhibi, Y. Kim, C. Berrospi Ramis, S. Gupta, M. Dolfi, and P. Staar. Statements: Universal information extraction from tables with large language models for ESG KPIs. In D. Stammbach, J. Ni, T. Schimanski, K. Dutia, A. Singh, J. Bingler, C. Christiaen, N. Kushwaha, V. Muccione, S. A. Vaghefi, and M. Leippold, editors, Proceedings of the 1st Workshop on Natural Language Processing Meets Climate Change (ClimateNLP 2024) , pages 193-214, Bangkok, Thailand, Aug. 2024. Association for Computational Linguistics. URL https://aclanthology.org/2024.climatenlp-1.15 .\n",
      "- [11] L. Morin, V. Weber, G. I. Meijer, F. Yu, and P. W. J. Staar. Patcid: an open-access dataset of chemical structures in patent documents. Nature Communications , 15(1):6532, August 2024. ISSN 2041-1723. doi: 10.1038/s41467-024-50779-y. URL https://doi.org/10.1038/ s41467-024-50779-y .\n",
      "- [12] A. Nassar, N. Livathinos, M. Lysak, and P. Staar. Tableformer: Table structure understanding with transformers. In Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition , pages 4614-4623, 2022.\n",
      "- [13] B. Pfitzmann, C. Auer, M. Dolfi, A. S. Nassar, and P. Staar. Doclaynet: a large humanannotated dataset for document-layout segmentation. pages 3743-3751, 2022.\n",
      "- [14] pypdf Maintainers. pypdf: A Pure-Python PDF Library, 2024. URL https://github.com/ py-pdf/pypdf .\n",
      "- [15] P. Team. PyPDFium2: Python bindings for PDFium, 2024. URL https://github.com/ pypdfium2-team/pypdfium2 .\n",
      "- [16] Y. Zhao, W. Lv, S. Xu, J. Wei, G. Wang, Q. Dang, Y. Liu, and J. Chen. Detrs beat yolos on real-time object detection, 2023.\n",
      "\n",
      "## Appendix\n",
      "\n",
      "In this section, we illustrate a few examples of Docling's output in Markdown and JSON.\n",
      "\n",
      "## DocLayNet: A Large Human-Annotated Dataset for Document-Layout Analysis\n",
      "\n",
      "## DocLayNet: A Large Human-Annotated Dataset for Document-Layout Analysis\n",
      "\n",
      "Birgit Pfitzmann IBM Research Rueschlikon, Switzerland bpf@zurich.ibm.com\n",
      "\n",
      "Christoph Auer IBM Research Rueschlikon, Switzerland cau@zurich.ibm.com\n",
      "\n",
      "Michele Dolfi IBM Research Rueschlikon, Switzerland dol@zurich.ibm.com\n",
      "\n",
      "Ahmed S. Nassar IBM Research Rueschlikon, Switzerland ahn@zurich.ibm.com\n",
      "\n",
      "Peter Staar IBM Research Rueschlikon, Switzerland taa@zurich.ibm.com\n",
      "\n",
      "## ABSTRACT\n",
      "\n",
      "Accurate document layout analysis is a key requirement for highquality PDF document conversion. With the recent availability of public, large ground-truth datasets such as PubLayNet and DocBank, deep-learning models have proven to be very effective at layout detection and segmentation. While these datasets are of adequate size to train such models, they severely lack in layout variability since they are sourced from scientific article repositories such as PubMed and arXiv only. Consequently, the accuracy of the layout segmentation drops significantly when these models are applied on more challenging and diverse layouts. In this paper, we present DocLayNet , a new, publicly available, document-layout annotation dataset in COCO format. It contains 80863 manually annotated pages from diverse data sources to represent a wide variability in layouts. For each PDF page, the layout annotations provide labelled bounding-boxes with a choice of 11 distinct classes. DocLayNet also provides a subset of double- and triple-annotated pages to determine the inter-annotator agreement. In multiple experiments, we provide baseline accuracy scores (in mAP) for a set of popular object detection models. We also demonstrate that these models fall approximately 10% behind the inter-annotator agreement. Furthermore, we provide evidence that DocLayNet is of sufficient size. Lastly, we compare models trained on PubLayNet, DocBank and DocLayNet, showing that layout predictions of the DocLayNettrained models are more robust and thus the preferred choice for general-purpose document-layout analysis.\n",
      "\n",
      "## CCS CONCEPTS\n",
      "\n",
      "· Informationsystems → Documentstructure ; · Appliedcomputing → Document analysis ; · Computing methodologies → Machine learning Computer vision ; ; Object detection ;\n",
      "\n",
      "Permission to make digital or hard copies of part or all of this work for personal or classroom use is granted without fee provided that copies are not made or distributed for profit or commercial advantage and that copies bear this notice and the full citation on the first page. Copyrights for third-party components of this work must be honored. For all other uses, contact the owner/author(s). KDD '22, August 14-18, 2022, Washington, DC, USA © 2022 Copyright held by the owner/author(s). ACM ISBN 978-1-4503-9385-0/22/08. https://doi.org/10.1145/3534678.3539043\n",
      "\n",
      "Birgit Pfitzmann IBM Research Rueschlikon, Switzerland bpf@zurich.ibm.com\n",
      "\n",
      "Christoph Auer IBM Research Rueschlikon, Switzerland cau@zurich.ibm.com\n",
      "\n",
      "Michele Dolfi IBM Research Rueschlikon, Switzerland dol@zurich.ibm.com\n",
      "\n",
      "Ahmed S. Nassar IBM Research Rueschlikon, Switzerland ahn@zurich.ibm.com\n",
      "\n",
      "Peter Staar IBM Research Rueschlikon, Switzerland taa@zurich.ibm.com\n",
      "\n",
      "## ABSTRACT\n",
      "\n",
      "Accurate document layout analysis is a key requirement for highquality PDF document conversion. With the recent availability of public, large groundtruth datasets such as PubLayNet and DocBank, deep-learning models have proven to be very effective at layout detection and segmentation. While these datasets are of adequate size to train such models, they severely lack in layout variability since they are sourced from scientific article repositories such as PubMed and arXiv only. Consequently, the accuracy of the layout segmentation drops significantly when these models are applied on more challenging and diverse layouts. In this paper, we present DocLayNet , a new, publicly available, document-layout annotation dataset in COCO format. It contains 80863 manually annotated pages from diverse data sources to represent a wide variability in layouts. For each PDF page, the layout annotations provide labelled bounding-boxes with a choice of 11 distinct classes. DocLayNet also provides a subset of double- and triple-annotated pages to determine the inter-annotator agreement. In multiple experiments, we provide baseline accuracy scores (in mAP) for a set of popular object detection models. We also demonstrate that these models fall approximately 10% behind the inter-annotator agreement. Furthermore, we provide evidence that DocLayNet is of sufficient size. Lastly, we compare models trained on PubLayNet, DocBank and DocLayNet, showing that layout predictions of the DocLayNettrained models are more robust and thus the preferred choice for general-purpose document-layout analysis.\n",
      "\n",
      "## CCS CONCEPTS\n",
      "\n",
      "Æ Information systems → Document structure ; Æ Applied computing → Document analysis ; Æ Computing methodologies → Machine learning ; Computer vision ; Object detection ;\n",
      "\n",
      "Permission to make digital or hard copies of part or all of this work for personal or classroom use is granted without fee provided that copies are not made or distributed for profit or commercial advantage and that copies bear this notice and the full citation on the first page. Copyrights for third-party components of this work must be honored. For all other uses, contact the owner/author(s).\n",
      "\n",
      "KDD '22, August 14-18, 2022, Washington, DC, USA ' 2022 Copyright held by the owner/author(s). ACM ISBN 978-1-4503-9385-0/22/08. https://doi.org/10.1145/3534678.3539043\n",
      "\n",
      "Figure 1: Four examples of complex page layouts across different document categories\n",
      "\n",
      "## KEYWORDS\n",
      "\n",
      "PDF document conversion, layout segmentation, object-detection, data set, Machine Learning\n",
      "\n",
      "## ACM Reference Format:\n",
      "\n",
      "Birgit Pfitzmann, Christoph Auer, Michele Dolfi, Ahmed S. Nassar, and Peter Staar. 2022. DocLayNet: A Large Human-Annotated Dataset for DocumentLayout Analysis. In Proceedings of the 28th ACM SIGKDD Conference on Knowledge Discovery and Data Mining (KDD '22), August 14-18, 2022, Washington, DC, USA. ACM, New York, NY, USA, 9 pages. https://doi.org/10.1145/ 3534678.3539043\n",
      "\n",
      "<!-- image -->\n",
      "\n",
      "<!-- image -->\n",
      "\n",
      "AGL Energy Limited  ABN 74 1\n",
      "\n",
      "5 061 375\n",
      "\n",
      "<!-- image -->\n",
      "\n",
      "<!-- image -->\n",
      "\n",
      "Figure 1: Four examples of complex page layouts across different document categories\n",
      "\n",
      "## KEYWORDS\n",
      "\n",
      "PDF document conversion, layout segmentation, object-detection, data set, Machine Learning\n",
      "\n",
      "## ACMReference Format:\n",
      "\n",
      "Birgit Pfitzmann, Christoph Auer, Michele Dolfi, Ahmed S. Nassar, and Peter Staar. 2022. DocLayNet: A Large Human-Annotated Dataset for DocumentLayout Analysis. In Proceedings of the 28th ACM SIGKDD Conference on Knowledge Discovery and Data Mining (KDD '22), August 14-18, 2022, Washington, DC, USA. ACM, New York, NY, USA, 9 pages. https://doi.org/10.1145/ 3534678.3539043\n",
      "\n",
      "1 INTRODUCTION\n",
      "\n",
      "Despite the substantial improvements achieved with machine-learning (ML) approaches and deep neural networks in recent years, document conversion remains a challenging problem, as demonstrated by the numerous public competitions held on this topic [1-4]. The challenge originates from the huge variability in PDF documents regarding layout, language and formats (scanned, programmatic or a combination of both). Engineering a single ML model that can be applied on all types of documents and provides high-quality layout segmentation remains to this day extremely challenging [5]. To highlight the variability in document layouts, we show a few example documents from the DocLayNet dataset in Figure 1. Figure 2: Title page of the DocLayNet paper (arxiv.org/pdf/2206.01062) - left PDF, right rendered Markdown. If recognized, metadata such as authors are appearing first under the title. Text content inside figures is currently dropped, the caption is retained and linked to the figure in the JSON representation (not shown).\n",
      "\n",
      "KDD '22, August 14-18, 2022, Washington, DC, USA Birgit Pfitzmann, Christoph Auer, Michele Dolfi, Ahmed S. Nassar, and Peter Staar\n",
      "\n",
      "Table 2: Prediction performance (mAP@0.5-0.95) of object detection networks on DocLayNet test set. The MRCNN (Mask R-CNN) and FRCNN (Faster R-CNN) models with ResNet-50 or ResNet-101 backbone were trained based on the network architectures from the detectron2 model zoo (Mask R-CNN R50, R101-FPN 3x, Faster R-CNN R101-FPN 3x), with default configurations. The YOLO implementation utilized was YOLOv5x6 [13]. All models were initialised using pre-trained weights from the COCO 2017 dataset.\n",
      "\n",
      "|                                                                                                        | human                                                                   | MRCNN R50 R101                                                                                                          | FRCNN R101                                                  | YOLO v5x6                                                   |\n",
      "|--------------------------------------------------------------------------------------------------------|-------------------------------------------------------------------------|-------------------------------------------------------------------------------------------------------------------------|-------------------------------------------------------------|-------------------------------------------------------------|\n",
      "| Caption Footnote Formula List-item Page-footer Page-header Picture Section-header Table Text Title All | 84-89 83-91 83-85 87-88 93-94 85-89 69-71 83-84 77-81 84-86 60-72 82-83 | 68.4 71.5 70.9 71.8 60.1 63.4 81.2 80.8 61.6 59.3 71.9 70.0 71.7 72.7 67.6 69.3 82.2 82.9 84.6 85.8 76.7 80.4 72.4 73.5 | 70.1 73.7 63.5 81.0 58.9 72.0 72.0 68.4 82.2 85.4 79.9 73.4 | 77.7 77.2 66.2 86.2 61.1 67.9 77.1 74.6 86.3 88.1 82.7 76.8 |\n",
      "\n",
      "to avoid this at any cost in order to have clear, unbiased baseline numbers for human document-layout annotation. Third, we introduced the feature of snapping boxes around text segments to obtain a pixel-accurate annotation and again reduce time and effort. The CCS annotation tool automatically shrinks every user-drawn box to the minimum bounding-box around the enclosed text-cells for all purely text-based segments, which excludes only Table and Picture . For the latter, we instructed annotation staff to minimise inclusion of surrounding whitespace while including all graphical lines. A downside of snapping boxes to enclosed text cells is that some wrongly parsed PDF pages cannot be annotated correctly and need to be skipped. Fourth, we established a way to flag pages as rejected for cases where no valid annotation according to the label guidelines could be achieved. Example cases for this would be PDF pages that render incorrectly or contain layouts that are impossible to capture with non-overlapping rectangles. Such rejected pages are not contained in the final dataset. With all these measures in place, experienced annotation staff managed to annotate a single page in a typical timeframe of 20s to 60s, depending on its complexity.\n",
      "\n",
      "## 5 EXPERIMENTS\n",
      "\n",
      "The primary goal of DocLayNet is to obtain high-quality ML models capable of accurate document-layout analysis on a wide variety of challenging layouts. As discussed in Section 2, object detection models are currently the easiest to use, due to the standardisation of ground-truth data in COCO format [16] and the availability of general frameworks such as detectron2 [17]. Furthermore, baseline numbers in PubLayNet and DocBank were obtained using standard object detection models such as Mask R-CNN and Faster R-CNN. As such, we will relate to these object detection methods in this\n",
      "\n",
      "<!-- image -->\n",
      "\n",
      "Third, achienec\n",
      "\n",
      "## EXPERIMENTS\n",
      "\n",
      "chalenongayouls ground-vuth dawa such WC\n",
      "\n",
      "<!-- image -->\n",
      "\n",
      "Figure 5: Prediction performance (mAP@0.5-0.95) of a Mask R-CNNnetworkwithResNet50backbonetrainedonincreasing fractions of the DocLayNet dataset. The learning curve flattens around the 80% mark, indicating that increasing the size of the DocLayNet dataset with similar data will not yield significantly better predictions.\n",
      "\n",
      "paper and leave the detailed evaluation of more recent methods mentioned in Section 2 for future work.\n",
      "\n",
      "In this section, we will present several aspects related to the performance of object detection models on DocLayNet. Similarly as in PubLayNet, we will evaluate the quality of their predictions using mean average precision (mAP) with 10 overlaps that range from 0.5 to 0.95 in steps of 0.05 (mAP@0.5-0.95). These scores are computed by leveraging the evaluation code provided by the COCO API [16].\n",
      "\n",
      "## Baselines for Object Detection\n",
      "\n",
      "In Table 2, we present baseline experiments (given in mAP) on Mask R-CNN [12], Faster R-CNN [11], and YOLOv5 [13]. Both training and evaluation were performed on RGB images with dimensions of 1025 × 1025 pixels. For training, we only used one annotation in case of redundantly annotated pages. As one can observe, the variation in mAP between the models is rather low, but overall between 6 and 10% lower than the mAP computed from the pairwise human annotations on triple-annotated pages. This gives a good indication that the DocLayNet dataset poses a worthwhile challenge for the research community to close the gap between human recognition and ML approaches. It is interesting to see that Mask R-CNN and Faster R-CNN produce very comparable mAP scores, indicating that pixel-based image segmentation derived from bounding-boxes does not help to obtain better predictions. On the other hand, the more recent Yolov5x model does very well and even out-performs humans on selected labels such as Text , Table and Picture . This is not entirely surprising, as Text , Table and Picture are abundant and the most visually distinctive in a document.\n",
      "\n",
      "coioct dcochon modols\n",
      "\n",
      "## Baselines for Object Detection\n",
      "\n",
      "mak enbrel\n",
      "\n",
      "Figure 3: Page 6 of the DocLayNet paper. If recognized, metadata such as authors are appearing first under the title. Elements recognized as page headers or footers are suppressed in Markdown to deliver uninterrupted content in reading order. Tables are inserted in reading order. The paragraph in '5. Experiments' wrapping over the column end is broken up in two and interrupted by the table.\n",
      "\n",
      "KDD '22, August 14-18, 2022, Washington, DC, USA\n",
      "\n",
      "Birgit Pfitzmann, Christoph Auer, Michele Dolfi, Ahmed S. Nassar, and Peter Staar\n",
      "\n",
      "Table 1: DocLayNet dataset overview. Along with the frequency of each class label, we present the relative occurrence (as %\n",
      "\n",
      "between pairwise annotations from the triple-annotated pages, from which we obtain accuracy ranges.\n",
      "\n",
      "of row 'Total') in the train, test and validation sets. The inter-annotator agreement is computed as the mAP@0.5-0.95 metric\n",
      "\n",
      "<!-- image -->\n",
      "\n",
      "<!-- image -->\n",
      "\n",
      "| class label    | Count   | % of Total   | % of Total   | % of Total   | triple inter-annotator mAP @0.5-0.95 (%)   | triple inter-annotator mAP @0.5-0.95 (%)   | triple inter-annotator mAP @0.5-0.95 (%)   | triple inter-annotator mAP @0.5-0.95 (%)   | triple inter-annotator mAP @0.5-0.95 (%)   | triple inter-annotator mAP @0.5-0.95 (%)   | triple inter-annotator mAP @0.5-0.95 (%)   |\n",
      "|----------------|---------|--------------|--------------|--------------|--------------------------------------------|--------------------------------------------|--------------------------------------------|--------------------------------------------|--------------------------------------------|--------------------------------------------|--------------------------------------------|\n",
      "| class label    | Count   | Train        | Test         | Val          | All                                        | Fin                                        | Man                                        | Sci                                        | Law                                        | Pat                                        | Ten                                        |\n",
      "| Caption        | 22524   | 2.04         | 1.77         | 2.32         | 84-89                                      | 40-61                                      | 86-92                                      | 94-99                                      | 95-99                                      | 69-78                                      | n/a                                        |\n",
      "| Footnote       | 6318    | 0.60         | 0.31         | 0.58         | 83-91                                      | n/a                                        | 100                                        | 62-88                                      | 85-94                                      | n/a                                        | 82-97                                      |\n",
      "| Formula        | 25027   | 2.25         | 1.90         | 2.96         | 83-85                                      | n/a                                        | n/a                                        | 84-87                                      | 86-96                                      | n/a                                        | n/a                                        |\n",
      "| List-item      | 185660  | 17.19        | 13.34        | 15.82        | 87-88                                      | 74-83                                      | 90-92                                      | 97-97                                      | 81-85                                      | 75-88                                      | 93-95                                      |\n",
      "| Page-footer    | 70878   | 6.51         | 5.58         | 6.00         | 93-94                                      | 88-90                                      | 95-96                                      | 100                                        | 92-97                                      | 100                                        | 96-98                                      |\n",
      "| Page-header    | 58022   | 5.10         | 6.70         | 5.06         | 85-89                                      | 66-76                                      | 90-94                                      | 98-100                                     | 91-92                                      | 97-99                                      | 81-86                                      |\n",
      "| Picture        | 45976   | 4.21         | 2.78         | 5.31         | 69-71                                      | 56-59                                      | 82-86                                      | 69-82                                      | 80-95                                      | 66-71                                      | 59-76                                      |\n",
      "| Section-header | 142884  | 12.60        | 15.77        | 12.85        | 83-84                                      | 76-81                                      | 90-92                                      | 94-95                                      | 87-94                                      | 69-73                                      | 78-86                                      |\n",
      "| Table          | 34733   | 3.20         | 2.27         | 3.60         | 77-81                                      | 75-80                                      | 83-86                                      | 98-99                                      | 58-80                                      | 79-84                                      | 70-85                                      |\n",
      "| Text           | 510377  | 45.82        | 49.28        | 45.00        | 84-86                                      | 81-86                                      | 88-93                                      | 89-93                                      | 87-92                                      | 71-79                                      | 87-95                                      |\n",
      "| Title          | 5071    | 0.47         | 0.30         | 0.50         | 60-72                                      | 24-63                                      | 50-63                                      | 94-100                                     | 82-96                                      | 68-79                                      | 24-56                                      |\n",
      "| Total          | 1107470 | 941123       | 99816        | 66531        | 82-83                                      | 71-74                                      | 79-81                                      | 89-94                                      | 86-91                                      | 71-76                                      | 68-85                                      |\n",
      "\n",
      "<!-- image -->\n",
      "\n",
      "include publication repositories such as arXiv\n",
      "\n",
      "Table 1: DocLayNet dataset overview. Along with the frequency of each class label, we present the relative occurrence (as % of row \"Total\") in the train, test and validation sets. The inter-annotator agreement is computed as the mAP@0.5-0.95 metric between pairwise annotations from the triple-\n",
      "\n",
      "annotated pages, from which we obtain accuracy ranges.\n",
      "\n",
      "<!-- image -->\n",
      "\n",
      "|                       |         | %of Total   | %of Total   | %of Total   | triple inter- annotator mAP @ 0.5-0.95 (%)   | triple inter- annotator mAP @ 0.5-0.95 (%)   | triple inter- annotator mAP @ 0.5-0.95 (%)   | triple inter- annotator mAP @ 0.5-0.95 (%)   | triple inter- annotator mAP @ 0.5-0.95 (%)   | triple inter- annotator mAP @ 0.5-0.95 (%)   | triple inter- annotator mAP @ 0.5-0.95 (%)   |\n",
      "|-----------------------|---------|-------------|-------------|-------------|----------------------------------------------|----------------------------------------------|----------------------------------------------|----------------------------------------------|----------------------------------------------|----------------------------------------------|----------------------------------------------|\n",
      "| class label           | Count   | Train       | Test        | Val         | All                                          | Fin                                          | Man                                          | Sci                                          | Law                                          | Pat                                          | Ten                                          |\n",
      "| Caption               | 22524   | 2.04        | 1.77        | 2.32        | 84-89                                        | 40-61                                        | 86-92                                        | 94-99                                        | 95-99                                        | 69-78                                        | n/a                                          |\n",
      "| Footnote              | 6318    | 0.60        | 0.31        | 0.58        | 83-91                                        | n/a                                          | 100                                          | 62-88                                        | 85-94                                        | n/a                                          | 82-97                                        |\n",
      "| Formula               | 25027   | 2.25        | 1.90        | 2.96        | 83-85                                        | n/a                                          | n/a                                          | 84-87                                        | 86-96                                        | n/a                                          | n/a                                          |\n",
      "| List-item             | 185660  | 17.19       | 13.34       | 15.82       | 87-88                                        | 74-83                                        | 90-92                                        | 97-97                                        | 81-85                                        | 75-88                                        | 93-95                                        |\n",
      "| Page- footer          | 70878   | 6.51        | 5.58        | 6.00        | 93-94                                        | 88-90                                        | 95-96                                        | 100                                          | 92-97                                        | 100                                          | 96-98                                        |\n",
      "| Page- header offices, | 58022   | 5.10        | 6.70        | 5.06        | 85-89                                        | 66-76                                        | 90-94                                        | 98-100                                       | 91-92                                        | 97-99                                        | 81-86                                        |\n",
      "| Picture               | 45976   | 4.21        | 2.78        | 5.31        | 69-71                                        | 56-59                                        | 82-86                                        | 69-82                                        | 80-95                                        | 66-71                                        | 59-76                                        |\n",
      "| Section- header not   | 142884  | 12.60       | 15.77       | 12.85       | 83-84                                        | 76-81                                        | 90-92                                        | 94-95                                        | 87-94                                        | 69-73                                        | 78-86                                        |\n",
      "| Table                 | 34733   | 3.20        | 2.27        | 3.60        | 77-81                                        | 75-80                                        | 83-86                                        | 98-99                                        | 58-80                                        | 79-84                                        | 70-85                                        |\n",
      "| Text                  | 510377  | 45.82       | 49.28       | 45.00       | 84-86                                        | 81-86                                        | 88-93                                        | 89-93                                        | 87-92                                        | 71-79                                        | 87-95                                        |\n",
      "| Title [22], a         | 5071    | 0.47        | 0.30        | 0.50        | 60-72                                        | 24-63                                        | 50-63                                        | 94-100                                       | 82-96                                        | 68-79                                        | 24-56                                        |\n",
      "| Total in-             | 1107470 | 941123      | 99816       | 66531       | 82-83                                        | 71-74                                        | 79-81                                        | 89-94                                        | 86-91                                        | 71-76                                        | 68-85                                        |\n",
      "\n",
      "3\n",
      "\n",
      ",\n",
      "\n",
      "government offices,\n",
      "\n",
      "We reviewed the col-\n",
      "\n",
      ",\n",
      "\n",
      "Page-\n",
      "\n",
      "Title and\n",
      "\n",
      ".\n",
      "\n",
      "page. Specificity ensures that the choice of label is not ambiguous,\n",
      "\n",
      "<!-- image -->\n",
      "\n",
      "we distributed the annotation workload and performed continuous be annotated. We refrained from class labels that are very specific\n",
      "\n",
      "only. For phases three and four, a group of 40 dedicated annotators while coverage ensures that all meaningful items on a page can\n",
      "\n",
      "quality controls. Phase one and two required a small team of experts to a document category, such as\n",
      "\n",
      "Abstract in the\n",
      "\n",
      "Scientific Articles were assembled and supervised.\n",
      "\n",
      "category. We also avoided class labels that are tightly linked to the\n",
      "\n",
      "Phase 1: Data selection and preparation.\n",
      "\n",
      "Our inclusion cri-\n",
      "\n",
      "Author\n",
      "\n",
      "Affiliation\n",
      "\n",
      "teria for documents were described in Section 3. A large effort went into ensuring that all documents are free to use. The data sources in DocBank, are often only distinguishable by discriminating on 3 https://arxiv.org/ Figure 4: Table 1 from the DocLayNet paper in the original PDF (A), as rendered Markdown (B) and in JSON representation (C). Spanning table cells, such as the multi-column header 'triple interannotator mAP@0.5-0.95 (%)', is repeated for each column in the Markdown representation (B), which guarantees that every data point can be traced back to row and column headings only by its grid coordinates in the table. In the JSON representation, the span information is reflected in the fields of each table cell (C).\n",
      "\n",
      "semantics of the text. Labels such as and\n",
      "\n",
      ",\n",
      "\n",
      "as seen\n",
      "{}\n",
      "-----\n",
      "## DSPY: COMPILING DECLARATIVE LANGUAGE MODEL CALLS INTO SELF-IMPROVING PIPELINES\n",
      "\n",
      "Omar Khattab, 1 Arnav Singhvi, 2 Paridhi Maheshwari, 4 Zhiyuan Zhang, 1 Keshav Santhanam, 1 Sri Vardhamanan, 6 Saiful Haq, 6 Ashutosh Sharma, 6 Thomas T. Joshi, 7 Hanna Moazam, 8 Heather Miller, 3 9 , Matei Zaharia, 2 Christopher Potts 1\n",
      "\n",
      "- 1 Stanford University, 2 UC Berkeley, 3 Carnegie Mellon University,\n",
      "- 4 Amazon Alexa AI, 5 Dashworks Technologies, Inc.,\n",
      "- 6 IIT Bombay, 7 Calera Capital, 8 Microsoft, 9 Two Sigma Investments\n",
      "\n",
      "okhattab@cs.stanford.edu\n",
      "\n",
      "## ABSTRACT\n",
      "\n",
      "The ML community is rapidly exploring techniques for prompting language models (LMs) and for stacking them into pipelines that solve complex tasks. Unfortunately, existing LM pipelines are typically implemented using hard-coded 'prompt templates', i.e. lengthy strings discovered via trial and error. Toward a more systematic approach for developing and optimizing LM pipelines, we introduce DSPy, a programming model that abstracts LM pipelines as text transformation graphs , i.e. imperative computation graphs where LMs are invoked through declarative modules. DSPy modules are parameterized , meaning they can learn (by creating and collecting demonstrations) how to apply compositions of prompting, finetuning, augmentation, and reasoning techniques. We design a compiler that will optimize any DSPy pipeline to maximize a given metric. We conduct two case studies, showing that succinct DSPy programs can express and optimize sophisticated LM pipelines that reason about math word problems, tackle multihop retrieval, answer complex questions, and control agent loops. Within minutes of compiling, a few lines of DSPy allow GPT-3.5 and llama2-13b-chat to selfbootstrap pipelines that outperform standard few-shot prompting (generally by over 25% and 65%, respectively) and pipelines with expert-created demonstrations (by up to 5-46% and 16-40%, respectively). On top of that, DSPy programs compiled to open and relatively small LMs like 770M-parameter T5 and llama2-13b-chat are competitive with approaches that rely on expert-written prompt chains for proprietary GPT-3.5 .\n",
      "\n",
      "DSPy is available at https://github.com/stanfordnlp/dspy .\n",
      "\n",
      "## 1 INTRODUCTION\n",
      "\n",
      "Language models (LMs) are enabling researchers to build NLP systems at higher levels of abstraction and with lower data requirements than ever before (Bommasani et al., 2021). This is fueling an exploding space of 'prompting' techniques-and lightweight finetuning techniques-for adapting LMs to new tasks (Kojima et al., 2022), eliciting systematic reasoning from them (Wei et al., 2022; Wang et al., 2022b), and augmenting them with retrieved sources (Guu et al., 2020; Lazaridou et al., 2022; Khattab et al., 2022) or with tools (Yao et al., 2022; Schick et al., 2023). Most of these techniques are explored in isolation, but interest has been growing in building multi-stage pipelines and agents that decompose complex tasks into more manageable calls to LMs in an effort to improve performance (Qi et al., 2019; Khattab et al., 2021a; Karpas et al., 2022; Dohan et al., 2022; Khot et al., 2022; Khattab et al., 2022; Chen et al., 2022; Pourreza &amp; Rafiei, 2023; Shinn et al., 2023).\n",
      "\n",
      "Unfortunately, LMs are known to be sensitive to how they are prompted for each task, and this is exacerbated in pipelines where multiple LM calls have to interact effectively. As a result, the LM\n",
      "\n",
      "calls in existing LM pipelines and in popular developer frameworks are generally implemented using hard-coded 'prompt templates', that is, long strings of instructions and demonstrations that are hand crafted through manual trial and error. We argue that this approach, while pervasive, can be brittle and unscalable-conceptually akin to hand-tuning the weights for a classifier. A given string prompt might not generalize to different pipelines or across different LMs, data domains, or even inputs.\n",
      "\n",
      "Toward a more systematic approach to designing AI pipelines, we introduce the DSPy programming model. 1 DSPy pushes building new LM pipelines away from manipulating free-form strings and closer to programming (composing modular operators to build text transformation graphs) where a compiler automatically generates optimized LM invocation strategies and prompts from a program. We draw inspiration from the consensus that emerged around neural network abstractions (Bergstra et al., 2013), where (1) many general-purpose layers can be modularly composed in any complex architecture and (2) the model weights can be trained using optimizers instead of being hand-tuned.\n",
      "\n",
      "To this end, we propose the DSPy programming model (Sec 3). We first translate string-based prompting techniques, including complex and task-dependent ones like Chain of Thought (Wei et al., 2022) and ReAct (Yao et al., 2022), into declarative modules that carry natural-language typed signatures . DSPy modules are task-adaptive components-akin to neural network layers-that abstract any particular text transformation, like answering a question or summarizing a paper. We then parameterize each module so that it can learn its desired behavior by iteratively bootstrapping useful demonstrations within the pipeline. Inspired directly by PyTorch abstractions (Paszke et al., 2019), DSPy modules are used via expressive define-by-run computational graphs. Pipelines are expressed by (1) declaring the modules needed and (2) using these modules in any logical control flow (e.g., if statements, for loops, exceptions, etc.) to logically connect the modules.\n",
      "\n",
      "Wethen develop the DSPy compiler (Sec 4), which optimizes any DSPy program to improve quality or cost. The compiler inputs are the program, a few training inputs with optional labels, and a validation metric. The compiler simulates versions of the program on the inputs and bootstraps example traces of each module for self-improvement, using them to construct effective few-shot prompts or finetuning small LMs for steps of the pipeline. Optimization in DSPy is highly modular: it is conducted by teleprompters , 2 which are general-purpose optimization strategies that determine how the modules should learn from data. In this way, the compiler automatically maps the declarative modules to high-quality compositions of prompting, finetuning, reasoning, and augmentation.\n",
      "\n",
      "Programming models like DSPy could be assessed along many dimensions, but we focus on the role of expert-crafted prompts in shaping system performance. We are seeking to reduce or even remove their role through DSPy modules (e.g., versions of popular techniques like Chain of Thought) and teleprompters. We report on two expansive case studies: math word problems (GMS8K; Cobbe et al. 2021) and multi-hop question answering (HotPotQA; Yang et al. 2018) with explorations of chain of thought, multi-chain reflection, multi-hop retrieval, retrieval-augmented question answering, and agent loops. Our evaluations use a number of different compiling strategies effectively and show that straightforward DSPy programs outperform systems using hand-crafted prompts, while also allowing our programs to use much smaller and hence more efficient LMs effectively.\n",
      "\n",
      "Overall, this work proposes the first programming model that translates prompting techniques into parameterized declarative modules and introduces an effective compiler with general optimization strategies (teleprompters) to optimize arbitrary pipelines of these modules. Our main contributions are empirical and algorithmic: with DSPy, we have found that we can implement very short programs that can bootstrap self-improving multi-stage NLP systems using LMs as small as llama2-13b-chat and T5-Large (770M parameters). Without hand-crafted prompts and within minutes to tens of minutes of compiling, compositions of DSPy modules can raise the quality of simple programs from 33% to 82% (Sec 6) and from 32% to 46% (Sec 7) for GPT-3.5 and, similarly, from 9% to 47% (Sec 6) and from 22% to 41% (Sec 7) for llama2-13b-chat .\n",
      "\n",
      "1 DSPy is pronounced dee-ess-pie . It's the second iteration of our earlier Demonstrate-Search-Predict framework (DSP; Khattab et al. 2022). This paper introduces the key concepts in DSPy. For more extensive and up-to-date documentation of the framework, we refer readers to https://github.com/stanfordnlp/dspy .\n",
      "\n",
      "2 We derive the name teleprompters from the notion of abstracting and automating the task of prompting, in particular, such that it happens at a distance , without manual intervention.\n",
      "\n",
      "## 2 RELATED WORK\n",
      "\n",
      "This work is inspired by the role that Torch (Collobert et al., 2002), Theano (Bergstra et al., 2010; 2011; Al-Rfou et al., 2016), Chainer (Tokui et al., 2015), and others played in the development in deep learning by providing powerful abstractions. A similar transformation is emerging with higherlevel pipelines of LMs, and we are seeking to offer a solid conceptual framework and programming abstractions for what we call foundation model programming . We draw on differentiable programming (Wang et al., 2018) but applied to LM calls rather than neural networks, and borrow syntactic elements from PyTorch (Paszke et al., 2019).\n",
      "\n",
      "In-context learning (McCann et al. 2018; Radford et al. 2018; Brown et al. 2020) is a key mechanism for foundation model programming. A growing body of work has revealed that, especially with instruction tuning (Ouyang et al., 2022), we can elicit sophisticated behavior via prompting (Wei et al., 2022; Wang et al., 2022b; Press et al., 2022; Yao et al., 2022; Khot et al., 2022; Madaan et al., 2023). Similarly, forms of weak supervision that would normally require task-specific (Khattab et al., 2021a;b) or hand-built (Ratner et al., 2016; Hancock et al., 2018) heuristics are now done by LMs (Wang et al., 2022b; Zelikman et al., 2022; Zhang et al., 2022; Shao et al., 2023).\n",
      "\n",
      "In-context learning methods now routinely invoke tools, leading to LM pipelines that use retrieval models (Chen et al., 2017; Lewis et al., 2020; Guu et al., 2020; Lazaridou et al., 2022; Izacard et al., 2022), multimodal foundation models, and more traditional tools like APIs (Nakano et al., 2021) and calculators. A number of toolkits have been developed to facilitate this, including LangChain (Chase, 2022), Semantic Kernel (Microsoft, 2023), LlamaIndex (Liu, 2022), and many other retrieval and agent libraries. These toolkits provide pre-packaged chains and agents that connect LMs with numerous accessible tools. However, they suffer from the pervasive prompt engineering challenges we address in DSPy: they express task-specific behavior through hand-written prompt templates (for detailed discussion, see Appendix B).\n",
      "\n",
      "Researchers are starting to apply discrete optimization and RL to find effective prompts, generally for a single logical LM call (Guo et al., 2023; Pryzant et al., 2023; Huang et al., 2022; Yang et al., 2023). DSPy seeks to generalize this space: it offers a rich framework for optimizing arbitrary pipelines from high-level declarative signatures , by bootstrapping high-quality multi-stage demonstrations with constraints. In this framework, DSPy teleprompters may apply optimization using model selection techniques like cross-validation or, in principle, with sophisticated techniques involving RL and LM feedback (Hu et al., 2023; Zhao et al., 2023a; Shinn et al., 2023) or learned or Bayesian hyperparameter optimization methods (Bergstra et al., 2013; Akiba et al., 2019).\n",
      "\n",
      "The present paper seeks to motivate DSPy as a programming model and to report new empirical findings from applying the DSPy compiler. This is inspired by formative work by Bergstra et al. (2010; 2013), Paszke et al. (2019), and Wolf et al. (2020), who support their respective programming models with a mix of benchmark numbers and some qualitative measures. For the current paper, we focus on showing that DSPy and its compiler allow us to build outstanding LM systems without hand-crafted prompt strings, but instead from truly modular units, and that this opens up doors for systematically exploring a rich design space at a very high programmatic level of abstraction.\n",
      "\n",
      "## 3 THE DSPY PROGRAMMING MODEL\n",
      "\n",
      "We present DSPy, which treats LMs as abstract devices for text generation, 3 and optimizes their usage in arbitrary computational graphs. DSPy programs are expressed in Python: each program takes the task input (e.g., a question to answer or a paper to summarize) and returns the output (e.g., an answer or a summary) after a series of steps. DSPy contributes three abstractions toward automatic optimization: signatures, modules, and teleprompters. Signatures abstract the input/output behavior of a module; modules replace existing hand-prompting techniques and can be composed in arbitrary pipelines; and teleprompters optimize all modules in the pipeline to maximize a metric.\n",
      "\n",
      "3 We assume access to one or more LMs, which consume a prompt string and return text completions. This may be a promptable LM capable of in-context learning (e.g., GPT-3.5 or Llama2-7b) or a smaller finetuneable LM (e.g., T5-base). An LM may be selected as the default; operations will use it unless configured otherwise.\n",
      "\n",
      "## 3.1 NATURAL LANGUAGE SIGNATURES CAN ABSTRACT PROMPTING &amp; FINETUNING\n",
      "\n",
      "Instead of free-form string prompts, DSPy programs use natural language signatures to assign work to the LM. A DSPy signature is natural-language typed declaration of a function: a short declarative spec that tells DSPy what a text transformation needs to do (e.g., 'consume questions and return answers'), rather than how a specific LM should be prompted to implement that behavior. More formally, a DSPy signature is a tuple of input fields and output fields (and an optional instruction ). Afield consists of field name and optional metadata. 4 In typical usage, the roles of fields are inferred by DSPy as a function of field names. For instance, the DSPy compiler will use in-context learning to interpret question differently from answer and will iteratively refine its usage of these fields.\n",
      "\n",
      "Signatures offer two benefits over prompts: they can be compiled into self-improving and pipelineadaptive prompts or finetunes. This is primarily done by bootstrapping (Sec 4) useful demonstrating examples for each signature. Additionally, they handle structured formatting and parsing logic to reduce (or, ideally, avoid) brittle string manipulation in user programs.\n",
      "\n",
      "In practice, DSPy signatures can be expressed with a shorthand notation like question -&gt; answer , so that line 1 in the following is a complete DSPy program for a basic question-answering system (with line 2 illustrating usage and line 3 the response when GPT-3.5 is the LM):\n",
      "\n",
      "```\n",
      "1 qa = dspy.Predict(\"question -> answer\") 2 qa(question=\"Where is Guaran´ ı spoken?\") 3 # Out: Prediction(answer='Guaran´ ı is spoken mainly in South America.')\n",
      "```\n",
      "\n",
      "In the shorthand notation, each field's name indicates the semantic role that the input (or output) field plays in the transformation. DSPy will parse this notation and expand the field names into meaningful instructions for the LM, so that english document -&gt; french translation would prompt for English to French translation. When needed, DSPy offers more advanced programming interfaces for expressing more explicit constraints on signatures (Appendix A).\n",
      "\n",
      "## 3.2 PARAMETERIZED &amp; TEMPLATED MODULES CAN ABSTRACT PROMPTING TECHNIQUES\n",
      "\n",
      "Akin to type signatures in programming languages, DSPy signatures simply define an interface and provide type-like hints on the expected behavior. To use a signature, we must declare a module with that signature, like we instantiated a Predict module above. A module declaration like this returns a function having that signature.\n",
      "\n",
      "The Predict Module The core module for working with signatures in DSPy is Predict (simplified pseudocode in Appendix D.1). Internally, Predict stores the supplied signature, an optional LM to use (initially None , but otherwise overrides the default LM for this module), and a list of demonstrations for prompting (initially empty). Like layers in PyTorch, the instantiated module behaves as a callable function: it takes in keyword arguments corresponding to the signature input fields (e.g., question ), formats a prompt to implement the signature and includes the appropriate demonstrations, calls the LM, and parses the output fields. When Predict detects it's being used in compile mode, it will also internally track input/output traces to assist the teleprompter at bootstrapping the demonstrations.\n",
      "\n",
      "Other Built-in Modules DSPy modules translate prompting techniques into modular functions that support any signature, contrasting with the standard approach of prompting LMs with task-specific details (e.g., hand-written few-shot examples). To this end, DSPy includes a number of more sophisticated modules like ChainOfThought , ProgramOfThought , MultiChainComparison , and ReAct . 5 These can all be used interchangeably to implement a DSPy signature. For instance, simply chang-\n",
      "\n",
      "4 String descriptions of the task and the fields are also optional and usually omitted. Fields can carry optional field prefix and description . By default, fields are assumed to hold free-form strings; we are actively exploring optional data type as a way to specify constraints on valid values (e.g., bool or int ) and more gracefully handle formatting and parsing logic, though this feature is not core to DSPy at the time of writing.\n",
      "\n",
      "5 These modules generalize prompting techniques from the literature, respectively, by Wei et al. (2022), Chen et al. (2022), Yoran et al. (2023), and Yao et al. (2022) and, in doing so, generalize the ideas on zero-shot prompting and rationale self-generation from Kojima et al. (2022), Zelikman et al. (2022), Zhang et al. (2022), and Huang et al. (2022) to parameterized modules that can bootstrap arbitrary multi-stage pipelines.\n",
      "\n",
      "ing Predict to ChainOfThought in the above program leads to a system that thinks step by step before committing to its output field.\n",
      "\n",
      "Importantly, all of these modules are implemented in a few lines of code by expanding the userdefined signature and calling Predict one or more times on new signatures as appropriate. For instance, we show a simplified implementation of the built-in ChainOfThought below.\n",
      "\n",
      "```\n",
      "1 class ChainOfThought(dspy.Module): 2 def __init__(self, signature): 3 # Modify signature from '*inputs -> *outputs ' to '*inputs -> rationale , *outputs '. 4 rationale_field = dspy.OutputField(prefix=\"Reasoning: Let's think step by step.\") 5 signature = dspy.Signature(signature).prepend_output_field(rationale_field) 6 7 # Declare a sub-module with the modified signature. 8 self.predict = dspy.Predict(signature) 9 10 def forward(self, **kwargs): 11 # Just forward the inputs to the sub-module. 12 return self.predict(**kwargs)\n",
      "```\n",
      "\n",
      "This is a fully-fledged module capable of learning effective few-shot prompting for any LM or task. We contrast that with Appendix C, which copies long reasoning prompts hand-written by sources ranging from recent research to popular prompting libraries.\n",
      "\n",
      "Parameterization Uniquely, DSPy parameterizes these prompting techniques. To understand this parameterization, observe that any LM call seeking to implement a particular signature needs to specify parameters that include: (1) the specific LM to call (Chen et al., 2023), (2) the prompt instructions (Yang et al., 2023) and the string prefix of each signature field and, most importantly, (3) the demonstrations used as few-shot prompts (for frozen LMs) or as training data (for finetuning). We focus primarily on automatically generating and selecting useful demonstrations. In our case studies, we find that bootstrapping good demonstrations gives us a powerful way to teach sophisticated pipelines of LMs new behaviors systematically.\n",
      "\n",
      "Tools DSPy programs may use tools, which are modules that execute computation. We support retrieval models through a dspy.Retrieve module. At the time of writing, DSPy has built-in support for ColBERTv2, Pyserini, and Pinecone retrievers, and we have explored experimental dspy.SQL for executing SQL queries and dspy.PythonInterpreter for executing Python code in a sandbox.\n",
      "\n",
      "Programs DSPy modules can be composed in arbitrary pipelines in a define-by-run interface. Inspired directly by PyTorch and Chainer, one first declares the modules needed at initialization, allowing DSPy to keep track of them for optimization, and then one expresses the pipeline with arbitrary code that calls the modules in a forward method. As a simple illustration, we offer the following simple but complete retrieval-augmented generation (RAG) system.\n",
      "\n",
      "```\n",
      "1 class RAG(dspy.Module): 2 def __init__(self, num_passages=3): 3 # 'Retrieve ' will use the user's default retrieval settings unless overriden. 4 self.retrieve = dspy.Retrieve(k=num_passages) 5 # 'ChainOfThought ' with signature that generates answers given retrieval & question. 6 self.generate_answer = dspy.ChainOfThought(\"context , question -> answer\") 7 8 def forward(self, question): 9 context = self.retrieve(question).passages 10 return self.generate_answer(context=context , question=question)\n",
      "```\n",
      "\n",
      "To highlight modularity, we use ChainOfThought as a drop-in replacement of the basic Predict . One can now simply write RAG()(\"Where is Guaran´ ı spoken?\") to use it. Notice that, if we use a signature \"context, question -&gt; search query\" , we get a system that generates search queries rather than answers.\n",
      "\n",
      "## 3.3 TELEPROMPTERS CAN AUTOMATE PROMPTING FOR ARBITRARY PIPELINES\n",
      "\n",
      "When compiling a DSPy program, we generally invoke a teleprompter , which is an optimizer that takes the program, a training set, and a metric-and returns a new optimized program. Different teleprompters (Sec 4) apply different strategies for optimization.\n",
      "\n",
      "In DSPy, training sets may be small , potentially a handful of examples, though larger data enables more powerful optimization. Training examples may be incomplete , i.e., only input values are necessary. Labels for the pipeline steps are not required, unless they need to be used in the metric. In practice, we typically assume labels only for (at most) the program's final output, not the intermediate steps. This label-efficiency is critical for modularity: building a new pipeline in DSPy requires simply recompiling the new pipeline's code, not annotating data specific to the new pipeline.\n",
      "\n",
      "Metrics can be simple notions like exact match (EM) or F1, but they can be entire DSPy programs that balance multiple concerns. For example, we may compile the RAG module above against a dataset of question-answer pairs qa trainset and the metric EM. The goal of optimization here is to effectively bootstrap few-shot demonstrations. The following code achieves this:\n",
      "\n",
      "```\n",
      "1 # Small training set with only questions and final answers. 2 qa_trainset = [dspy.Example(question=\"What is the capital of France?\", answer=\"Paris\")] 3 4 # The teleprompter will bootstrap missing labels: reasoning chains and retrieval contexts. 5 teleprompter = dspy.BootstrapFewShot(metric=dspy.evaluate.answer_exact_match) 6 compiled_rag = teleprompter.compile(RAG(), trainset=qa_trainset)\n",
      "```\n",
      "\n",
      "In this example, the BootstrapFewShot teleprompter (Sec 4, Appendix E.1) simulates RAG on the training example(s). It will collect demonstrations of each module (i.e., examples of its input-output behavior) that collectively lead to valid output (i.e., respecting the signatures and the metric).\n",
      "\n",
      "If one wanted to push the compiled program to be extractive given its retrieved contexts, one could define a custom metric to use in place of dspy.evaluate.answer exact match :\n",
      "\n",
      "```\n",
      "1 2 3 4 5 6 7\n",
      "```\n",
      "\n",
      "```\n",
      "def answer_and_context_match(example , pred , trace=None): answer_match = dspy.evaluate.answer_exact_match(example , pred) # Is the prediction a substring of some passage? context_match = any((pred.answer.lower() in c) for c in pred.context) return answer_match and context_match\n",
      "```\n",
      "\n",
      "Notice that behavior like this might be more accurately checked by another DSPy program that checks for faithful grounding of answers. Such metrics are fully supported and encouraged in DSPy.\n",
      "\n",
      "Teleprompters can be composed by specifying a teacher program. DSPy will sample demonstrations from this program for prompt optimization. This composition can enable very rich pipelines, where expensive programs (e.g., complex expensive ensembles using large LMs) supervise cheap programs (e.g., simple pipelines using smaller LMs). One may start with compiled rag from above (say, compiled to use a large Llama2-13b-chat LM) but now fine-tune Flan-T5-large to create an efficient program:\n",
      "\n",
      "```\n",
      "1 # Larger set of questions with *no labels*. Labels for all steps will be bootstrapped. 2 unlabeled_questions = [dspy.Example(question=\"What is the capital of Germany?\"), ...] 3 4 # As we assumes no answer , we use 'answer_passage_match ' to filter ungrounded answers. 5 finetuning_teleprompter = BootstrapFinetune(metric=dspy.evaluate.answer_passage_match) 6 7 # We set 'teacher=compiled_rag ' to compose. Bootstrapping will now use 'compiled_rag '. 8 compiled_rag_via_finetune = finetuning_teleprompter.compile(RAG(), teacher=compiled_rag , trainset=unlabeled_questions , target='google/flan-t5-large')\n",
      "```\n",
      "\n",
      "## 4 THE DSPY COMPILER\n",
      "\n",
      "A key source of DSPy's expressive power is its ability to compile-or automatically optimize-any program in this programming model. Compiling relies on a teleprompter, which is an optimizer for DSPy programs that improves the quality (or cost) of modules via prompting or finetuning, which are unified in DSPy. While DSPy does not enforce this when creating new teleprompters, typical teleprompters go through three stages.\n",
      "\n",
      "Stage 1: Candidate Generation The compiler first (recursively) finds all unique Predict modules (predictors) in a program, including those nested under other modules. For each unique predictor p , the teleprompter may generate candidate values for the parameters of p : the instructions, field descriptions, or-most importantly-demonstrations (i.e., example input-output pairs). In this iter-\n",
      "\n",
      "ation of DSPy, we focus on demonstrations and find that simple rejection-sampling-like approaches can help bootstrap highly effective multi-stage systems.\n",
      "\n",
      "Consider the simplest non-trivial teleprompter in DSPy, BootstrapFewShot (simplified pseudocode in Appendix E.1). This teleprompter will simulate a teacher program (or, if unset, the zero-shot version of the program being compiled) on some training inputs, possibly one or more times with a high temperature. When running in compile mode, multi-stage traces are tracked transparently and in a thread-safe fashion throughout execution. The program's metric is used to filter for multistage traces that together help the pipeline pass the metric. We thus obtain potential labels for all signatures in the program by throwing away the bad examples and using the good examples as potential demonstrations, though these design decisions are under user control.\n",
      "\n",
      "While LMs can be highly unreliable, we find they can be rather efficient at searching the space of solutions for multi-stage designs. A well-decomposed program can typically find at least a few training examples where the LM can pass the constraints enforced by the signatures and metrics, allowing us to bootstrap iteratively if needed.\n",
      "\n",
      "Stage 2: Parameter Optimization Now each parameter has a discrete set of candidates: demonstrations, instructions, etc. Many hyperparameter tuning algorithms (e.g., random search or Treestructured Parzen Estimators as in HyperOpt (Bergstra et al., 2013) and Optuna (Akiba et al., 2019)) can be applied for selection among candidates. We report simplified implementations of DSPy's BootstrapFewShotWithRandomSearch and BootstrapFewShotWithOptuna in Appendix E.2 and Appendix E.3.\n",
      "\n",
      "Another type of optimization is finetuning with BootstrapFinetune , where the demonstrations are used to update the LM's weights for each predictor. When this is applied, the LM parameter of each module is updated to the new LM weights. Typically, we are optimizing average quality using the metric with cross-validation over the training set or a validation set. This is applicable even with no labels for any stages, depending on the nature of metric.\n",
      "\n",
      "Stage 3: Higher-Order Program Optimization A different type of optimization that the DSPy compiler supports is modifying the control flow of the program. One of the simplest forms of these is ensembles, which we use in the case studies in this work. An ensemble will bootstrap multiple copies of the same program, and then replace the program with a new one that runs them all in parallel and reduces their predictions into one with a custom function (e.g., majority voting). In future work, this stage can easily accommodate techniques for more dynamic (i.e., test-time) bootstrapping as well as automatic backtracking-like logic.\n",
      "\n",
      "## 5 GOALS OF EVALUATION\n",
      "\n",
      "Programming frameworks can be evaluated along many dimensions: computational efficiency, developer efficiency, intuitiveness of the code and concepts, and so forth. In this paper, we focus on perhaps the most pressing issue for current LM pipelines: the role of hand-written, task-specific prompts in achieving performant systems. Our evaluations seek to test the following hypotheses:\n",
      "\n",
      "- H1 With DSPy, we can replace hand-crafted prompt strings with concise and well-defined modules, without reducing quality or expressive power.\n",
      "- H2 Parameterizing the modules and treating prompting as an optimization problem makes DSPy better at adapting to different LMs, and it may outperform expert-written prompts.\n",
      "- H3 The resulting modularity makes it possible to more thoroughly explore complex pipelines that have useful performance characteristics or that fit nuanced metrics.\n",
      "\n",
      "Our evaluation will explore these hypotheses using diverse task-program pairs. We hope this begins a shift from underspecified questions like 'how do different LMs compare on GSM8K' toward 'how they compare on GSM8K with program P when compiled with strategy S', which is a well-defined and reproducible run. Ultimately, our goal is to reduce the role of artful prompt construction in modern AI in favor of the development of new modular, composable programs and optimizers.\n",
      "\n",
      "Table 1: Results with in-context learning on GSM8K math word problems. Each row represents a separate pipeline: the module in the Program column is compiled against the examples in the Training set. The programs, compilers, and (small) training sets are defined in Section 6. Rows with ensemble build on the immediately preceding row. Notably, all programs in this table are expressed by composing two to four DSPy modules and teleprompters. Compiling the correct modules , instead of string prompts, improves different LMs from 4-20% accuracy to 49-88% accuracy.\n",
      "\n",
      "|            |               |             | GPT-3.5   | GPT-3.5   | Llama2-13b-chat   | Llama2-13b-chat   |\n",
      "|------------|---------------|-------------|-----------|-----------|-------------------|-------------------|\n",
      "| Program    | Compilation   | Training    | Dev       | Test      | Dev               | Test              |\n",
      "| vanilla    | none          | n/a         | 24.0      | 25.2      | 7.0               | 9.4               |\n",
      "|            | fewshot       | trainset    | 33.1      | -         | 4.3               | -                 |\n",
      "|            | bootstrap     | trainset    | 44.0      | -         | 28.0              | -                 |\n",
      "|            | bootstrap × 2 | trainset    | 64.7      | 61.7      | 37.3              | 36.5              |\n",
      "|            | + ensemble    | trainset    | 62.7      | 61.9      | 39.0              | 34.6              |\n",
      "|            | none          | n/a         | 50.0      | -         | 26.7              | -                 |\n",
      "|            | fewshot       | trainset    | 63.0      | -         | 27.3              | -                 |\n",
      "| CoT        | fewshot       | + human CoT | 78.6      | 72.4      | 34.3              | 33.7              |\n",
      "|            | bootstrap     | trainset    | 80.3      | 72.9      | 43.3              | -                 |\n",
      "|            | + ensemble    | trainset    | 88.3      | 81.6      | 43.7              | -                 |\n",
      "| reflection | none          | n/a         | 65.0      | -         | 36.7              | -                 |\n",
      "|            | fewshot       | trainset    | 71.7      | -         | 36.3              | -                 |\n",
      "|            | bootstrap     | trainset    | 83.0      | 76.0      | 44.3              | 40.2              |\n",
      "|            | + ensemble    | trainset    | 86.7      | -         | 49.0              | 46.9              |\n",
      "\n",
      "## 6 CASE STUDY: MATH WORD PROBLEMS\n",
      "\n",
      "We evaluate on the popular GSM8K dataset with grade school math questions (Cobbe et al., 2021). Wesample 200 and 300 question-answer pairs from the official training set for training and development, respectively. Our final evaluations use the 1.3k official test set examples. We report extensive comparisons on the development set to avoid overfitting on test. Following prior work on GSM8K, we evaluate the accuracy of the final numerical value that appears in the LM output.\n",
      "\n",
      "Programs Considered For this task, we consider three simple DSPy programs: a one-step Predict module ( vanilla ), a two-step ChainOfThought module ( CoT ), and finally a multi-stage ComparerOfThoughts module ( ThoughtReflection ). These are fully defined by the following code:\n",
      "\n",
      "```\n",
      "1 vanilla = dspy.Predict(\"question -> answer\") # GSM8K Program 'vanilla ' 2 3 CoT = dspy.ChainOfThought(\"question -> answer\") # GSM8K Program 'CoT' 1 class ThoughtReflection(dspy.Module): 2 def __init__(self, num_attempts): 3 self.predict = dspy.ChainOfThought(\"question -> answer\", n=num_attempts) 4 self.compare = dspy.MultiChainComparison('question -> answer', M=num_attempts) 5 6 def forward(self, question): 7 completions = self.predict(question=question).completions 8 return self.compare(question=question , completions=completions) 9 10 reflection = ThoughtReflection(num_attempts=5) # GSM8K Program 'reflection '\n",
      "```\n",
      "\n",
      "In reflection , five reasoning chains are sampled from the LM (alongside their answers) and they are compared in parallel by a built-in MultiChainComparison module, which generalizes Yoran et al. (2023). This generates a new answer taking into account the patterns from the five attempts. Critically, the modules used are all generic, none is specific math problems or particular LM.\n",
      "\n",
      "Compiling As we discussed in Section 4, DSPy programs can be compiled into new, optimized programs. In our experiments, we evaluate the programs zero-shot (no compiling) as well as a number of strategies for compiling. Our simplest compiler is LabeledFewShot :\n",
      "\n",
      "```\n",
      "1 fewshot = dspy.LabeledFewShot(k=8).compile(program , trainset=trainset)\n",
      "```\n",
      "\n",
      "Here, program can be any DSPy module. This simply samples k=8 random demonstrations from the trainset for the fields common to the training examples and the signature(s), in this case, question and answer , but not the reasoning for instance. We report the average of 3-5 runs (depending on the setting) when applying such random sampling.\n",
      "\n",
      "Next, we also consider bootstrapping few-shot examples with random search:\n",
      "\n",
      "```\n",
      "1 tp = BootstrapFewShotWithRandomSearch(metric=gsm8k_accuracy) 2 bootstrap = tp.compile(program , trainset=trainset , valset=devset)\n",
      "```\n",
      "\n",
      "This will generate demonstration chains for examples in the training set and optimize the selection of demonstrations (from this set) to self-improve the program's modules. As the name indicates, this is done with random search, treating the selection of demonstrations as a parameter to optimize.\n",
      "\n",
      "Next, if desired, this bootstrapping process can be nested in DSPy. In particular, we can use the optimized bootstrap program itself to further bootstrap another program. This is relevant, for example, whenever the original zero-shot program performs relatively poorly.\n",
      "\n",
      "```\n",
      "1\n",
      "```\n",
      "\n",
      "```\n",
      "bootstrap2 = tp.compile(program , teacher=bootstrap , trainset=trainset , valset=devset)\n",
      "```\n",
      "\n",
      "## And lastly, we consider ensembling these bootstraps:\n",
      "\n",
      "```\n",
      "# A program that ensembles the top -7 candidate programs from a bootstrapping compiler run\n",
      "```\n",
      "\n",
      "```\n",
      "1 (in particular 'bootstrap ' or, when applicable , 'bootstrap2 ') with majority voting. 2 ensemble = Ensemble(reduce_fn=dspy.majority).compile(bootstrap.programs[:7])\n",
      "```\n",
      "\n",
      "GSM8K includes human reasoning chains. Above, trainset does not include these reasoning chains. We also evaluate with trainset human CoT , which extends the examples in trainset with the human reasoning string. These two datasets can be used interchangeably as the value for the trainset parameter above. We note here that compiling generally runs on the order of minutes (or tens of minutes) as even the more expensive settings only require running the program a few thousand times (e.g., 10-20 trials over 150-300 validation examples) and they can occur in parallel.\n",
      "\n",
      "Results Our results are summarized in Table 1, which includes dev results as well as our evaluation of promising representatives of each approach on the test set. First, the vanilla program results show that GPT-3.5 and llama2-13b-chat struggle with math word problems when they have to predict the answers directly, that is, without using a reasoning chain first. This is most pronounced in the absence of good demonstrations, which can be seen in the none compilation setting (i.e., zero-shot instruction) and the fewshot setting (i.e., sampling random question-answer pairs). Interestingly, however, vanilla is helped substantially by compiling with bootstrap and by iterating this process into bootstrap × 2 . On inspecting the prompts bootstrapped (Appendix F), we see that the prompt allows the LM to leverage the answer field for reasoning first, which is permitted as the metric extracts the final numerical value for evaluation.\n",
      "\n",
      "Next, we consider the CoT program. While the expert human reasoning chains ( +human CoT ) provide a large boost when available, we can match or surpass this using bootstrap , substantiating our hypothesis that DSPy can cut the need for hand-crafted prompts. Beyond this, we see that the reflection program, while only a few lines longer than the others, is a clear winner, though CoT is quite effective with ensemble . Overall, the bootstrap compilation procedure leads to large gains for every program, across both LMs. Indeed, all programs in this table are expressed by composing two to four DSPy modules and teleprompters, and they reveal overall that-in the new paradigm prescribed by DSPy-it's composing the right generic modules , rather than manipulating string prompts, that improves different LMs from 4-20% accuracy to 49-88% accuracy.\n",
      "\n",
      "We can informally compare with the following. Zhang et al. (2022) reports 48% for text-davinci-002 , which aligns closely with our llama2-13b-chat results, and reports 59.4% with codex when employing a manual CoT approach and 62.8% with an automatic CoT method. Wang et al. (2022b) report 57% for CoT prompting with PaLM 540-B, which becomes 74% upon adding self-consistency. The Llama2 authors (Touvron et al., 2023) presents 28.7% for llama2-13b , 42.2% for llama2-34b , and 56.8% for llama2-70b . Intriguingly, our program with the 13b variant of the model is competitive with their 34b-based results even though we don't use human reasoning chains in our program. Zhao et al. (2023b) reports 80.8% for CoT with gpt-3.5-turbo from April 2023. The GPT-4 authors (OpenAI, 2023) reports that GPT-3.5 scores 57.1% and GPT-4 elevates this to 92% but they note that GPT-4 was in fact pre-trained on a subset of GSM8K's training set.\n",
      "\n",
      "## 7 CASE STUDY: COMPLEX QUESTION ANSWERING\n",
      "\n",
      "In this case study, we explore the multi-hop question answering task with the HotPotQA (Yang et al., 2018) dataset in the open-domain 'fullwiki' setting. For retrieval, we use a search index of the official Wikipedia 2017 'abstracts' dump of HotPotQA. Search is conducted by a ColBERTv2 (Santhanam et al., 2021) retriever. The HotPotQA test set is hidden, so we reserve the official validation set for our testing, and sample 1000 examples for that. We sub-divide the training set into 70%/30% train/validation splits. In the training (and thus validation) split, we keep only examples marked as 'hard' in the original dataset, which matches the designation of the official validation and test sets. For training and for reporting development results, we sample 200 and 300 examples respectively.\n",
      "\n",
      "Programs Considered Our simplest baseline is the vanilla program used in the previous case study on GSM8K (Sec 6); the \"question -&gt; answer\" signature is universal enough that it will work for this task (and many others) when compiled appropriately.\n",
      "\n",
      "Our baseline RAG program is the one given in Section 3.2 as a simple example of RAG with a dspy.ChainOfThought layer. We will see that this program does not excel at HotPotQA, and this motivates us to evaluate two multi-hop programs.\n",
      "\n",
      "To that end, we first test ReAct (Yao et al., 2022), a multi-step agent for tool use, which is implemented as a built-in module in DSPy. In the simplest case, a ReAct module for a particular signature can be declared as follows in DSPy:\n",
      "\n",
      "1 react = dspy.ReAct(\"question -&gt; answer\", tools=[dspy.Retrieve(k=1)], max\\_iters=5) Wealso test the following custom program, which simulates the information flow in Baleen (Khattab et al., 2021a) and IRRR (Qi et al., 2020) and has similarities to IRCoT (Trivedi et al., 2022). 1 class BasicMultiHop(dspy.Module): 2 def \\_\\_init\\_\\_(self, passages\\_per\\_hop): 3 self.retrieve = dspy.Retrieve(k=passages\\_per\\_hop) 4 self.generate\\_query = dspy.ChainOfThought(\"context , question -&gt; search\\_query\") 5 self.generate\\_answer = dspy.ChainOfThought(\"context , question -&gt; answer\") 6 7 def forward(self, question): 8 context = [] 9 10 for hop in range(2): 11 query = self.generate\\_query(context=context , question=question).search\\_query 12 context += self.retrieve(query).passages 13 14 return self.generate\\_answer(context=context , question=question) 15 16 multihop = BasicMultiHop(passages\\_per\\_hop=3)\n",
      "\n",
      "Compiling For compilers, we continue to use the ones that we used for GSM8K (see Sec 6). We also consider two compositions of our teleprompters. For ReAct, we consider bootstrapping with BootstrapFewShotWithRandomSearch starting from an earlier bootstrap of the ReAct program. For the simple multihop program, we also consider fine-tuning with T5-Large starting from the earlier bootstrap of that program.\n",
      "\n",
      "```\n",
      "1 multihop_t5 = dspy.BootstrapFinetune(metric=answer_exact_match).compile(program , teacher=bootstrap , trainset=trainset , target='t5-large')\n",
      "```\n",
      "\n",
      "Results Table 2 summarizes our results. Compared with the vanilla few-shot prompting, a chainof-thought and retrieval-augmented generation ( CoT RAG ) program can self-bootstrap in DSPy to increase answer EM substantially. However, this relies entirely on the ColBERTv2 retriever to find relevant passages directly from the original questions, limiting its passage recall. This is tackled in the react and multihop programs, which will generate queries for the retriever in multiple iterative 'hops'. Indeed, overall, a simple multihop program performs the best, and in general bootstrap again proves to be very effective at raising its quality relative to its fewshot variant for both LMs.\n",
      "\n",
      "In particular, we can see that bootstrap (and/or bootstrap × 2 ) can outperform both fewshot prompting (for multihop ) and expert human reasoning (for react ; adapted slightly from Yao et al. (2022) to our retrieval setting). Perhaps most importantly, we can make llama2-13b-chat competitive with GPT-3.5 by simply compiling our programs.\n",
      "\n",
      "To assess the finetuning capacity of DSPy, we also evaluated the compiler multihop t5 defined above which produces a T5-Large (770M parameter) model. This program scores 39.3% answer EM and 46.0% passage accuracy on the dev set, using only 200 labeled inputs and 800 unlabeled\n",
      "\n",
      "Table 2: Results with in-context learning on HotPotQA multi-hop retrieval question answering. We report answer exact match (Ans) and pair-retrieval accuracy (Psg). Each row represents a separate pipeline: the module in the Program column is compiled against the examples in the Training set. The programs, compilers, and (small) training sets are defined in the main text. For HotPotQA, we use the training set (and not dev) directly for cross-validation. ∗ The marked result is evaluated on 50% of our test set due to cost.\n",
      "\n",
      "|          |               | GPT-3.5   | GPT-3.5   | GPT-3.5   | GPT-3.5   | Llama2-13b-chat   | Llama2-13b-chat   | Llama2-13b-chat   | Llama2-13b-chat   |\n",
      "|----------|---------------|-----------|-----------|-----------|-----------|-------------------|-------------------|-------------------|-------------------|\n",
      "| Program  | Compiler      | Dev       | Dev       | Test      | Test      | Dev               | Dev               | Test              | Test              |\n",
      "|          |               | Ans       | Psg       | Ans       | Psg       | Ans               | Psg               | Ans               | Psg               |\n",
      "| vanilla  | fewshot       | 34.3      | n/a       | 31.5      | n/a       | 27.5              | n/a               | 21.8              | n/a               |\n",
      "| CoT RAG  | fewshot       | 36.4      | 36.0      | 29.8      | 34.4      | 34.5              | 36.0              | 28.0              | 34.4              |\n",
      "| CoT RAG  | bootstrap     | 42.3      | 36.0      | -         | -         | 38.3              | 36.0              | 32.9              | 34.4              |\n",
      "| react    | none          | 20.3      | -         | -         | -         | 20.0              | -                 | -                 | -                 |\n",
      "|          | +human r      | 33.0      | -         | -         | -         | 28.3              | -                 | -                 | -                 |\n",
      "|          | bootstrap     | 31.0      | -         | -         | -         | 24.7              | -                 | -                 | -                 |\n",
      "|          | bootstrap × 2 | 39.0      | -         | -         | -         | 40.0              | -                 | -                 | -                 |\n",
      "| multihop | fewshot       | 36.9      | 38.3      | 31.2      | 40.8      | 34.7              | 32.0              | 31.3              | 30.8              |\n",
      "|          | bootstrap     | 48.7      | 47.0      | 39.6      | 43.8      | 42.0              | 48.3              | 36.4              | 43.5              |\n",
      "|          | ensemble      | 54.7      | -         | 45.6 ∗    | -         | 50.0              | -                 | 41.0              | -                 |\n",
      "\n",
      "questions. For compiling, we use a teacher program consisting of an ensemble (union) of two multihop with llama2-13b-chat . Considering its extremely small size and local availability, this compiled program with T5-Large would impose orders of magnitude lower costs for inference than a proprietary LM like GPT-3.5.\n",
      "\n",
      "Our results may be pegged against the evaluation on HotPotQA in a number of recent papers, though there is significant variation in evaluation methodology and test set samples across studies in this space. Using CoT prompting, Si et al. (2022) achieve 25.2% EM. With a 'recite-and-answer' technique that uses PaLM-62B (Chowdhery et al., 2022) to recite evidence passages, Sun et al. (2022) achieve 26.5% EM. Wang et al. (2022a) achieve 33.8% EM and 44.6% F1 when applying selfconsistency for PaLM-540B. Yao et al. (2022) achieve 27.4% EM using ReAct with PaLM-540B and 30.8 with text-davinci-002 , with a tool giving it the ability for search using a Wikipedia API. They push their PaLM results to 35.1% EM by applying an additional CoT step with selfconsistency, which may resemble our ensemble approach in the sense of aggregating multiple answers. Trivedi et al. (2022) reports 49% using a pipeline with code-davinci-002 LM on a sample of 500 HotPotQA questions.\n",
      "\n",
      "## 8 CONCLUSION\n",
      "\n",
      "This paper introduced DSPy, a new programming model for designing AI systems using pipelines of pretrained LMs and other tools. We presented three new concepts introduced in this abstraction (DSPy signatures, modules, and teleprompters), and showed in two very different case studies that it supports rapid development of highly effective systems that use relatively small LMs. We have maintained open-source versions of this framework for close to a year. In this period, we have seen and created a large number of programs that were compiled to high-quality systems by DSPy, spanning tasks from information extraction to low-resource synthetic data generation. In the interest of space and to maintain reasonable scope in this paper, we leave reporting on such tasks under controlled experimental conditions to future work. While in-context learning has proved transformative over the past 2-3 years of LM research, we argue that the true expressive power in this emerging paradigm is in building sophisticated text transformation graphs in which composable modules and optimizers (teleprompters) come together to leverage LMs in more systematic and reliable ways.\n",
      "\n",
      "## ACKNOWLEDGMENTS\n",
      "\n",
      "We thank Josh Purtell for suggesting the apt name 'text transformation graph' for the computational graph abstraction of DSPy. We thank Rick Battle, Igor Kotenkov, Lisa Li, David Hall, Ashwin Paranjape, Chris Manning, Percy Liang, and many researchers, developers, and users for valuable\n",
      "\n",
      "discussions and feedback. We thank Giuseppe Attanasio for his public LT E X GitHub-style Python A code formatting gist. 6\n",
      "\n",
      "This work was partially supported by IBM as a founding member of the Stanford Institute for Human-Centered Artificial Intelligence (HAI), Oracle, Virtusa, and Cigna Healthcare. It was also partially supported by an HAI Azure compute grant. This research was supported in part by affiliate members and other supporters of the Stanford DAWN project-Facebook, Google, and VMware-as well as the NSF under CAREER grant CNS-1651570. Any opinions, findings, and conclusions or recommendations expressed in this material are those of the authors and do not necessarily reflect the views of the National Science Foundation. Omar Khattab is supported by the Apple Scholars in AI/ML fellowship.\n",
      "\n",
      "\\usepackage[pdftex]{graphicx} ... \\includegraphics[width=0.8\\linewidth]{myfile.pdf}\n",
      "\n",
      "## REFERENCES\n",
      "\n",
      "Takuya Akiba, Shotaro Sano, Toshihiko Yanase, Takeru Ohta, and Masanori Koyama. Optuna: A next-generation hyperparameter optimization framework. In Proceedings of the 25th ACM SIGKDD international conference on knowledge discovery &amp; data mining , pp. 2623-2631, 2019.\n",
      "\n",
      "Rami Al-Rfou, Guillaume Alain, Amjad Almahairi, Christof Angermueller, Dzmitry Bahdanau, Nicolas Ballas, Fr´ ed´ric Bastien, e Justin Bayer, Anatoly Belikov, Alexander Belopolsky, et al. Theano: A Python framework for fast computation of mathematical expressions. arXiv e-prints , pp. arXiv-1605, 2016.\n",
      "\n",
      "James Bergstra, Olivier Breuleux, Fr´ ed´ eric Bastien, Pascal Lamblin, Razvan Pascanu, Guillaume Desjardins, Joseph Turian, David Warde-Farley, and Yoshua Bengio. Theano: A CPU and GPU math compiler in Python. In Proc. 9th python in science conf , volume 1, pp. 3-10, 2010.\n",
      "\n",
      "James Bergstra, Fr´ ed´ric Bastien, Olivier Breuleux, Pascal Lamblin, Razvan Pascanu, Olivier Dee lalleau, Guillaume Desjardins, David Warde-Farley, Ian Goodfellow, Arnaud Bergeron, et al. Theano: Deep learning on gpus with Python. In NIPS 2011, BigLearning Workshop, Granada, Spain , volume 3. Citeseer, 2011.\n",
      "\n",
      "James Bergstra, Daniel Yamins, and David Cox. Making a science of model search: Hyperparameter optimization in hundreds of dimensions for vision architectures. In International conference on machine learning , pp. 115-123. PMLR, 2013.\n",
      "\n",
      "Rishi Bommasani, Drew A Hudson, Ehsan Adeli, Russ Altman, Simran Arora, Sydney von Arx, Michael S Bernstein, Jeannette Bohg, Antoine Bosselut, Emma Brunskill, et al. On the opportunities and risks of foundation models. arXiv preprint arXiv:2108.07258 , 2021.\n",
      "\n",
      "Tom Brown, Benjamin Mann, Nick Ryder, Melanie Subbiah, Jared D Kaplan, Prafulla Dhariwal, Arvind Neelakantan, Pranav Shyam, Girish Sastry, Amanda Askell, et al. Language models are few-shot learners. Advances in neural information processing systems , 33:1877-1901, 2020.\n",
      "\n",
      "Harrison Chase. Hwchase17/langchain. 2022. URL https://github.com/hwchase17/ langchain .\n",
      "\n",
      "Danqi Chen, Adam Fisch, Jason Weston, and Antoine Bordes. Reading Wikipedia to answer open-domain questions. In Proceedings of the 55th Annual Meeting of the Association for Computational Linguistics (Volume 1: Long Papers) , pp. 1870-1879, Vancouver, Canada, July 2017. Association for Computational Linguistics. doi: 10.18653/v1/P17-1171. URL https: //aclanthology.org/P17-1171 .\n",
      "\n",
      "Lingjiao Chen, Matei Zaharia, and James Zou. Frugalgpt: How to use large language models while reducing cost and improving performance. arXiv preprint arXiv:2305.05176 , 2023.\n",
      "\n",
      "6 https://gist.github.com/g8a9/07c2be12ae02cfad4aa430d77dc940cb\n",
      "\n",
      "| Wenhu Chen, Xueguang Ma, Xinyi Wang, and William W Cohen. Program of thoughts prompt- ing: Disentangling computation from reasoning for numerical reasoning tasks. arXiv preprint arXiv:2211.12588 , 2022.                                                                                                                                                                                    |\n",
      "|-----------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------|\n",
      "| Aakanksha Chowdhery, Sharan Narang, Jacob Devlin, Maarten Bosma, Gaurav Mishra, Adam Roberts, Paul Barham, Hyung Won Chung, Charles Sutton, Sebastian Gehrmann, et al. Palm: Scaling language modeling with pathways. arXiv preprint arXiv:2204.02311 , 2022.                                                                                                                                 |\n",
      "| Karl Cobbe, Vineet Kosaraju, Mohammad Bavarian, Mark Chen, Heewoo Jun, Lukasz Kaiser, Matthias Plappert, Jerry Tworek, Jacob Hilton, Reiichiro Nakano, et al. Training verifiers to solve math word problems. arXiv preprint arXiv:2110.14168 , 2021.                                                                                                                                         |\n",
      "| Ronan Collobert, Samy Bengio, and Johnny Mari´thoz. e Torch: a modular machine learning software library. Technical report, Idiap, 2002.                                                                                                                                                                                                                                                      |\n",
      "| David Dohan, Winnie Xu, Aitor Lewkowycz, Jacob Austin, David Bieber, Raphael Gontijo Lopes, Yuhuai Wu, Henryk Michalewski, Rif A Saurous, Jascha Sohl-Dickstein, et al. Language model cascades. arXiv preprint arXiv:2207.10342 , 2022.                                                                                                                                                      |\n",
      "| Luyu Gao, Zhuyun Dai, Panupong Pasupat, Anthony Chen, Arun Tejasvi Chaganty, Yicheng Fan, Vincent Zhao, Ni Lao, Hongrae Lee, Da-Cheng Juan, et al. Rarr: Researching and revising what language models say, using language models. In Proceedings of the 61st Annual Meeting of the Association for Computational Linguistics (Volume 1: Long Papers) , pp. 16477-16508, 2023a.               |\n",
      "| Luyu Gao, Aman Madaan, Shuyan Zhou, Uri Alon, Pengfei Liu, Yiming Yang, Jamie Callan, and Graham Neubig. Pal: Program-aided language models. In International Conference on Machine Learning , pp. 10764-10799. PMLR, 2023b.                                                                                                                                                                  |\n",
      "| Qingyan Guo, Rui Wang, Junliang Guo, Bei Li, Kaitao Song, Xu Tan, Guoqing Liu, Jiang Bian, and Yujiu Yang. Connecting large language models with evolutionary algorithms yields powerful prompt optimizers. arXiv preprint arXiv:2309.08532 , 2023.                                                                                                                                           |\n",
      "| Kelvin Guu, Kenton Lee, Zora Tung, Panupong Pasupat, and Ming-Wei Chang. Realm: Retrieval- augmented language model pre-training. arXiv preprint arXiv:2002.08909 , 2020. URL https: //arxiv.org/abs/2002.08909 .                                                                                                                                                                             |\n",
      "| Braden Hancock, Paroma Varma, Stephanie Wang, Martin Bringmann, Percy Liang, and Christopher R´. e Training classifiers with natural language explanations. In Proceedings of the 56th Annual Meeting of the Association for Computational Linguistics (Volume 1: Long Papers) , pp. 1884- 1895. Association for Computational Linguistics, 2018. URL http://aclweb.org/anthology/ P18-1175 . |\n",
      "| Bin Hu, Chenyang Zhao, Pu Zhang, Zihao Zhou, Yuanhang Yang, Zenglin Xu, and Bin Liu. En- abling intelligent interactions between an agent and an LLM: Areinforcement learning approach. arXiv preprint arXiv:2306.03604 , 2023. URL https://arxiv.org/abs/2306.03604 .                                                                                                                        |\n",
      "| Jiaxin Huang, Shixiang Shane Gu, Le Hou, Yuexin Wu, Xuezhi Wang, Hongkun Yu, and Jiawei Han. Large language models can self-improve. arXiv preprint arXiv:2210.11610 , 2022.                                                                                                                                                                                                                  |\n",
      "| Gautier Izacard, Patrick Lewis, Maria Lomeli, Lucas Hosseini, Fabio Petroni, Timo Schick, Jane Dwivedi-Yu, Armand Joulin, Sebastian Riedel, and Edouard Grave. Few-shot learning with re- trieval augmented language models. arXiv preprint arXiv:2208.03299 , 2022.                                                                                                                          |\n",
      "| Ehud Karpas, Omri Abend, Yonatan Belinkov, Barak Lenz, Opher Lieber, Nir Ratner, Yoav Shoham, Hofit Bata, Yoav Levine, Kevin Leyton-Brown, et al. Mrkl systems: A modular, neuro-symbolic architecture that combines large language models, external knowledge sources and discrete rea- soning. arXiv preprint arXiv:2205.00445 , 2022.                                                      |\n",
      "| Omar Khattab, Christopher Potts, and Matei Zaharia. Baleen: Robust Multi-Hop Reasoning at Scale via Condensed Retrieval. In Thirty-Fifth Conference on Neural Information Processing Systems , 2021a.                                                                                                                                                                                         |\n",
      "| Omar Khattab, Christopher Potts, and Matei Zaharia. Relevance-guided supervision for openqa with ColBERT. Transactions of the Association for Computational Linguistics , 9:929-944, 2021b.                                                                                                                                                                                                   |\n",
      "\n",
      "Omar Khattab, Keshav Santhanam, Xiang Lisa Li, David Hall, Percy Liang, Christopher Potts, and Matei Zaharia. Demonstrate-search-predict: Composing retrieval and language models for knowledge-intensive nlp. arXiv preprint arXiv:2212.14024 , 2022.\n",
      "\n",
      "Tushar Khot, Harsh Trivedi, Matthew Finlayson, Yao Fu, Kyle Richardson, Peter Clark, and Ashish Sabharwal. Decomposed prompting: A modular approach for solving complex tasks. arXiv preprint arXiv:2210.02406 , 2022.\n",
      "\n",
      "Takeshi Kojima, Shixiang Shane Gu, Machel Reid, Yutaka Matsuo, and Yusuke Iwasawa. Large language models are zero-shot reasoners. arXiv preprint arXiv:2205.11916 , 2022.\n",
      "\n",
      "Angeliki Lazaridou, Elena Gribovskaya, Wojciech Stokowiec, and Nikolai Grigorev. Internetaugmented language models through few-shot prompting for open-domain question answering. arXiv preprint arXiv:2203.05115 , 2022.\n",
      "\n",
      "Patrick Lewis, Ethan Perez, Aleksandra Piktus, Fabio Petroni, Vladimir Karpukhin, Naman Goyal, Heinrich K¨ uttler, Mike Lewis, Wen-tau Yih, Tim Rockt¨ aschel, Sebastian Riedel, and Douwe Kiela. Retrieval-augmented generation for knowledge-intensive nlp tasks. In H. Larochelle, M. Ranzato, R. Hadsell, M.F. Balcan, and H. Lin (eds.), Advances in Neural Information Processing Systems , volume 33, pp. 9459-9474. Curran Associates, Inc., 2020. URL https://proceedings.neurips.cc/paper files/paper/2020/file/ 6b493230205f780e1bc26945df7481e5-Paper.pdf .\n",
      "\n",
      "Jerry Liu. LlamaIndex, 11 2022. URL https://github.com/jerryjliu/llama index .\n",
      "\n",
      "Aman Madaan, Niket Tandon, Prakhar Gupta, Skyler Hallinan, Luyu Gao, Sarah Wiegreffe, Uri Alon, Nouha Dziri, Shrimai Prabhumoye, Yiming Yang, et al. Self-refine: Iterative refinement with self-feedback. arXiv preprint arXiv:2303.17651 , 2023.\n",
      "\n",
      "Bryan McCann, Nitish Shirish Keskar, Caiming Xiong, and Richard Socher. The natural language decathlon: Multitask learning as question answering. arXiv:1806.08730, 2018. URL https: //arxiv.org/abs/1806.08730 .\n",
      "\n",
      "Microsoft. Semantic kernel. 2023. URL https://learn.microsoft.com/semantic-kernel/ .\n",
      "\n",
      "Reiichiro Nakano, Jacob Hilton, Suchir Balaji, Jeff Wu, Long Ouyang, Christina Kim, Christopher Hesse, Shantanu Jain, Vineet Kosaraju, William Saunders, Xu Jiang, Karl Cobbe, Tyna Eloundou, Gretchen Krueger, Kevin Button, Matthew Knight, Benjamin Chess, and John Schulman. WebGPT: Browser-assisted question-answering with human feedback, 2021. URL https: //arxiv.org/abs/2112.09332 .\n",
      "\n",
      "OpenAI. Gpt-4 technical report, 2023.\n",
      "\n",
      "Long Ouyang, Jeff Wu, Xu Jiang, Diogo Almeida, Carroll L Wainwright, Pamela Mishkin, Chong Zhang, Sandhini Agarwal, Katarina Slama, Alex Ray, et al. Training language models to follow instructions with human feedback. arXiv preprint arXiv:2203.02155 , 2022.\n",
      "\n",
      "Adam Paszke, Sam Gross, Francisco Massa, Adam Lerer, James Bradbury, Gregory Chanan, Trevor Killeen, Zeming Lin, Natalia Gimelshein, Luca Antiga, Alban Desmaison, Andreas Kopf, Edward Yang, Zachary DeVito, Martin Raison, Alykhan Tejani, Sasank Chilamkurthy, Benoit Steiner, Lu Fang, Junjie Bai, and Soumith Chintala. PyTorch: An imperative style, high-performance deep learning library. In H. Wallach, H. Larochelle, A. Beygelzimer, F. d'Alch´ e-Buc, E. Fox, and R. Garnett (eds.), Advances in Neural Information Processing Systems , volume 32. Curran Associates, Inc., 2019. URL https://proceedings.neurips.cc/paper files/paper/2019/ file/bdbca288fee7f92f2bfa9f7012727740-Paper.pdf .\n",
      "\n",
      "Mohammadreza Pourreza and Davood Rafiei. Din-sql: Decomposed in-context learning of text-tosql with self-correction. arXiv preprint arXiv:2304.11015 , 2023.\n",
      "\n",
      "Ofir Press, Muru Zhang, Sewon Min, Ludwig Schmidt, Noah A Smith, and Mike Lewis. Measuring and narrowing the compositionality gap in language models. arXiv preprint arXiv:2210.03350 , 2022.\n",
      "\n",
      "| optimization with' gradient descent' and beam search. arXiv preprint arXiv:2305.03495 , 2023.                                                                                                                                                                                                                                                                                                                                                                                                    |\n",
      "|--------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------|\n",
      "| Peng Qi, Xiaowen Lin, Leo Mehr, Zijian Wang, and Christopher D. Manning. Answering complex open-domain questions through iterative query generation. In Proceedings of the 2019 Con- ference on Empirical Methods in Natural Language Processing and the 9th International Joint Conference on Natural Language Processing (EMNLP-IJCNLP) , pp. 2590-2602, Hong Kong, China, 2019. Association for Computational Linguistics. doi: 10.18653/v1/D19-1261. URL https://aclanthology.org/D19-1261 . |\n",
      "| Peng Qi, Haejun Lee, Oghenetegiri Sido, Christopher D Manning, et al. Retrieve, rerank, read, then iterate: Answering open-domain questions of arbitrary complexity from text. arXiv preprint arXiv:2010.12527 , 2020. URL https://arxiv.org/abs/2010.12527 .                                                                                                                                                                                                                                    |\n",
      "| Alec Radford, Karthik Narasimhan, Tim Salimans, and Ilya Sutskever. Improving language un- derstanding by generative pre-training. Ms, OpenAI, 2018. URL https://openai.com/blog/ language-unsupervised/ .                                                                                                                                                                                                                                                                                       |\n",
      "| Alexander J Ratner, Christopher M De Sa, Sen Wu, Daniel Selsam, and Christopher R´. e Data programming: Creating large training sets, quickly. In D. D. Lee, M. Sugiyama, U. V. Luxburg, I. Guyon, and R. Garnett (eds.), Advances in Neural Information Processing Systems 29 , pp. 3567-3575. Curran Associates, Inc., 2016. URL https://papers.nips.cc/paper/ 6523-data-programming-creating-large-training-sets-quickly .                                                                    |\n",
      "| Keshav Santhanam, Omar Khattab, Jon Saad-Falcon, Christopher Potts, and Matei Zaharia. Col- BERTv2: Effective and Efficient Retrieval via Lightweight Late Interaction. arXiv preprint arXiv:2112.01488 , 2021.                                                                                                                                                                                                                                                                                  |\n",
      "| Timo Schick, Jane Dwivedi-Yu, Roberto Dess`, ı Roberta Raileanu, Maria Lomeli, Luke Zettlemoyer, Nicola Cancedda, and Thomas Scialom. Toolformer: Language models can teach themselves to use tools. arXiv preprint arXiv:2302.04761 , 2023.                                                                                                                                                                                                                                                     |\n",
      "| Zhihong Shao, Yeyun Gong, Yelong Shen, Minlie Huang, Nan Duan, and Weizhu Chen. Syn- thetic prompting: Generating chain-of-thought demonstrations for large language models. arXiv preprint arXiv:2302.00618 , 2023.                                                                                                                                                                                                                                                                             |\n",
      "| Noah Shinn, Beck Labash, and Ashwin Gopinath. Reflexion: an autonomous agent with dynamic memory and self-reflection. arXiv preprint arXiv:2303.11366 , 2023.                                                                                                                                                                                                                                                                                                                                    |\n",
      "| Chenglei Si, Zhe Gan, Zhengyuan Yang, Shuohang Wang, Jianfeng Wang, Jordan Boyd-Graber, and Lijuan Wang. Prompting gpt-3 to be reliable. arXiv preprint arXiv:2210.09150 , 2022.                                                                                                                                                                                                                                                                                                                 |\n",
      "| Zhiqing Sun, Xuezhi Wang, Yi Tay, Yiming Yang, and Denny Zhou. Recitation-augmented language models. arXiv preprint arXiv:2210.01296 , 2022.                                                                                                                                                                                                                                                                                                                                                     |\n",
      "| Seiya Tokui, Kenta Oono, Shohei Hido, and Justin Clayton. Chainer: a next-generation open source framework for deep learning. In Proceedings of workshop on machine learning systems (Learn- ingSys) in the twenty-ninth annual conference on neural information processing systems (NIPS) , volume 5, pp. 1-6, 2015.                                                                                                                                                                            |\n",
      "| Hugo Touvron, Louis Martin, Kevin Stone, Peter Albert, Amjad Almahairi, Yasmine Babaei, Niko- lay Bashlykov, Soumya Batra, Prajjwal Bhargava, Shruti Bhosale, et al. Llama 2: Open founda- tion and fine-tuned chat models. arXiv preprint arXiv:2307.09288 , 2023.                                                                                                                                                                                                                              |\n",
      "| Harsh Trivedi, Niranjan Balasubramanian, Tushar Khot, and Ashish Sabharwal. Interleaving re- trieval with chain-of-thought reasoning for knowledge-intensive multi-step questions. arXiv preprint arXiv:2212.10509 , 2022.                                                                                                                                                                                                                                                                       |\n",
      "| Fei Wang, James Decker, Xilun Wu, Gregory Essertel, and Tiark Rompf. Backpropaga- tion with callbacks: Foundations for efficient and expressive differentiable programming. In S. Bengio, H. Wallach, H. Larochelle, K. Grauman, N. Cesa-Bianchi, and R. Gar- nett (eds.), Advances in Neural Information Processing Systems , volume 31. Curran Asso- ciates, Inc., 2018. URL https://proceedings.neurips.cc/paper files/paper/2018/file/ 34e157766f31db3d2099831d348a7933-Paper.pdf .          |\n",
      "\n",
      "Xuezhi Wang, Jason Wei, Dale Schuurmans, Quoc Le, Ed Chi, and Denny Zhou. Rationaleaugmented ensembles in language models. arXiv preprint arXiv:2207.00747 , 2022a.\n",
      "\n",
      "Xuezhi Wang, Jason Wei, Dale Schuurmans, Quoc Le, Ed Chi, and Denny Zhou. Self-consistency improves chain of thought reasoning in language models. arXiv preprint arXiv:2203.11171 , 2022b.\n",
      "\n",
      "- Jason Wei, Xuezhi Wang, Dale Schuurmans, Maarten Bosma, Ed Chi, Quoc Le, and Denny Zhou. Chain of thought prompting elicits reasoning in large language models. arXiv preprint arXiv:2201.11903 , 2022.\n",
      "\n",
      "Thomas Wolf, Lysandre Debut, Victor Sanh, Julien Chaumond, Clement Delangue, Anthony Moi, Pierric Cistac, Tim Rault, Remi Louf, Morgan Funtowicz, Joe Davison, Sam Shleifer, Patrick von Platen, Clara Ma, Yacine Jernite, Julien Plu, Canwen Xu, Teven Le Scao, Sylvain Gugger, Mariama Drame, Quentin Lhoest, and Alexander Rush. Transformers: State-of-the-art natural language processing. In Proceedings of the 2020 Conference on Empirical Methods in Natural Language Processing: System Demonstrations , pp. 38-45, Online, 2020. Association for Computational Linguistics. doi: 10.18653/v1/2020.emnlp-demos.6. URL https: //aclanthology.org/2020.emnlp-demos.6 .\n",
      "\n",
      "- Chengrun Yang, Xuezhi Wang, Yifeng Lu, Hanxiao Liu, Quoc V Le, Denny Zhou, and Xinyun Chen. Large language models as optimizers. arXiv preprint arXiv:2309.03409 , 2023.\n",
      "\n",
      "Zhilin Yang, Peng Qi, Saizheng Zhang, Yoshua Bengio, William W Cohen, Ruslan Salakhutdinov, and Christopher D Manning. Hotpotqa: A dataset for diverse, explainable multi-hop question answering. arXiv preprint arXiv:1809.09600 , 2018.\n",
      "\n",
      "- Shunyu Yao, Jeffrey Zhao, Dian Yu, Nan Du, Izhak Shafran, Karthik Narasimhan, and Yuan Cao. React: Synergizing reasoning and acting in language models. arXiv preprint arXiv:2210.03629 , 2022.\n",
      "- Ori Yoran, Tomer Wolfson, Ben Bogin, Uri Katz, Daniel Deutch, and Jonathan Berant. Answering questions by meta-reasoning over multiple chains of thought. arXiv preprint arXiv:2304.13007 , 2023.\n",
      "- Eric Zelikman, Yuhuai Wu, and Noah D Goodman. Star: Bootstrapping reasoning with reasoning. arXiv preprint arXiv:2203.14465 , 2022.\n",
      "- Zhuosheng Zhang, Aston Zhang, Mu Li, and Alex Smola. Automatic chain of thought prompting in large language models. arXiv preprint arXiv:2210.03493 , 2022.\n",
      "\n",
      "Andrew Zhao, Daniel Huang, Quentin Xu, Matthieu Lin, Yong-Jin Liu, and Gao Huang. ExpeL: LLM agents are experiential learners. arXiv preprint arXiv:2308.10144 , 2023a. URL https: //arxiv.org/pdf/2308.10144 .\n",
      "\n",
      "Xu Zhao, Yuxi Xie, Kenji Kawaguchi, Junxian He, and Qizhe Xie. Automatic model selection with large language models for reasoning. arXiv preprint arXiv:2305.14333 , 2023b.\n",
      "\n",
      "## A ADVANCED SIGNATURES\n",
      "\n",
      "When more control is desired, one can express signatures as Python classes to provide explicit instructions of the transformation and describe the format or role of each field more directly. For instance, the following signature generates search queries using context and an optional question:\n",
      "\n",
      "```\n",
      "1 class GenerateSearchQuery(dspy.Signature): 2 \"\"\"Write a simple search query that will help answer a complex 3 4 context = dspy.InputField(desc=\"may contain relevant facts\") 5 question = dspy.InputField() 6 query = dspy.OutputField(dtype=dspy.SearchQuery)\n",
      "```\n",
      "\n",
      "```\n",
      "question.\"\"\"\n",
      "```\n",
      "\n",
      "Using the above, we can specify a complete system for the generation of a synthetic IR dataset where the queries are mediated by a question generated by the LM:\n",
      "\n",
      "```\n",
      "1 query_gen = dspy.Predict(GenerateSearchQuery) 2 query_gen(context=\"Language typology\") 3 # Out: Prediction(question='What are the main types of language classification?', query='\"language classification\" OR \"language typology\" -wikipedia')\n",
      "```\n",
      "\n",
      "If questions are available, they can be supplied as shown: query gen(context=\"Language typology\", question=\"What are the primary language families of South America?\") .\n",
      "\n",
      "As a work in progress feature, users can optionally specify the type of output fields as bool, int, float, list , or dict instead of the default free-form string type, as in contexts, question -&gt; answer found: bool .\n",
      "\n",
      "## B COMPARISON WITH EXISTING LIBRARIES LIKE LANGCHAIN AND LLAMAINDEX\n",
      "\n",
      "LangChain and LlamaIndex are perhaps the most popular library in the general space of prompting LMs. These libraries have a different focus compared to DSPy and they suffer internally from the prompt engineering challenges that DSPy aims to resolve. In particular, whereas the goal of DSPy is to tackle the fundamental challenges of prompt engineering for building new LM computational graphs, LangChain and LlamaIndex generally help application developers who need pre-packaged components and chains, e.g., implementations of popular and reusable pipelines (e.g., particular agents and specific retrieval pipelines) and tools (e.g., connections to various databases and implementations of long- and short-term memory for agents).\n",
      "\n",
      "These off-the-shelf higher-level abstractions contrast with DSPy's focus on introducing core composable operators. In particular, DSPy introduces signatures (to abstract prompts), modules (to abstract prompting techniques), and teleprompters to act as optimizers for arbitrary imperative code (DSPy programs) that chain modules together. Its goal is to help researchers and practitioners build new LM pipelines quickly and achieve very high quality through automatic compilation (selfimprovement) instead of manual prompt engineering.\n",
      "\n",
      "In contrast, typical existing research implementations and existing libraries like LangChain and LlamaIndex are implemented using manual prompt engineering, which is the key problem that DSPy tackles. We conducted an informal study to highlight this. In late September 2023, we found that the LangChain codebase contains 50 strings exceeding 1000 characters, which are generally prompts, compared to none at all in DSPy. Indeed, a substantial number of LangChain's Python files are singularly dedicated to task-related templating and prompt engineering with 12 prompts.py files and and 42 prompt.py files. DSPy, on the other hand, provides a structured framework that automatically bootstraps prompts. The library itself does not contain a single hand-written prompt demonstration for any tasks at the time of writing, despite the very high quality with various LMs.\n",
      "\n",
      "To review the typical forms of prompt engineering in existing libraries, we consider the following in LangChain. The LangChain Program-Aided Language Model Gao et al. (2023a) chain program uses few-shot learning, leveraging a template that is 3982 characters long with 8 math word problems (Prompt 2) and corresponding outputted programs as learning examples for the language model. LangChain also contains a prompt for SQL query tasks for each of the databases like Oracle, GoogleSQL, DuckDB, Crate, and MySQL, with the average length of these prompts at 1058 characters. Other task areas such as QA with sources (Prompt B) and Graph QA also have signif-\n",
      "\n",
      "icantly lengthy prompt templates, with averages of 1337 and 722 characters, respectively. While expert-written prompts can be useful, we believe that LM- and task-adaptive prompts bootstrapped automatically can offer far more power (and are far more modular) than hard-coding a prompt per database provider inside the code base. The next appendix section contains a number of prompts copied from related research papers and existing libraries.\n",
      "\n",
      "## C SAMPLE LARGE PROMPTS\n",
      "\n",
      "This section highlights a few popular existing frameworks that structure prompts with extensive prompt engineering templates. The primary objective is to capture how many words and characters are used for such large multi-line prompts defined for tasks or tools and present these example prompts retrieved from open-sourced papers and repositories. The formatting of these example prompts is adapted from Gao et al. (2023a).\n",
      "\n",
      "| Task/Tool Prompt                   | Source                        |   Words |   Characters |\n",
      "|------------------------------------|-------------------------------|---------|--------------|\n",
      "| Prompt 1: Text-evidence checker    | Gao et al. (2023a)            |     818 |         4964 |\n",
      "| Prompt 2: Math word problems (PAL) | LangChain &Gao et al. (2023b) |     566 |         3957 |\n",
      "| Prompt 3: ReAct                    | Yao et al. (2022)             |     593 |         3889 |\n",
      "| Prompt 4: Zero-shot ReAct          | LangChain                     |     101 |          600 |\n",
      "| Prompt 5: QA with sources          | LangChain                     |     992 |         6197 |\n",
      "| Prompt 6: SQL MyScale querying     | LangChain                     |     343 |         2239 |\n",
      "| Prompt 7: Relevant docs retrieval  | LlamaIndex                    |     129 |          719 |\n",
      "| Prompt 8: IRS chatbot              | LlamaIndex                    |     389 |         2258 |\n",
      "\n",
      "```\n",
      "1 [web] I will check some things you said. 2 3 (1) You said: Your nose switches back and forth between nostrils. When you sleep, you switch about every 45 minutes. This is to prevent a buildup of mucus. It's called the nasal cycle. 4 I checked: How often do your nostrils switch? 5 I found this article: Although we don't usually notice it, during the nasal cycle one nostril becomes congested and thus contributes less to airflow, while the other becomes decongested. On average, the congestion pattern switches about every 2 hours, according to a small 2016 study published in the journal PLOS One. 6 Your nose's switching time is about every 2 hours, not 45 minutes. 7 This disagrees with what you said. 8 9 (2) You said: The Little House books were written by Laura Ingalls Wilder. The books were published by HarperCollins. 10 I checked: Who published the Little House books? 11 I found this article: These are the books that started it all -- the stories that captured the hearts and imaginations of children and young adults worldwide. Written by Laura Ingalls Wilder and published by HarperCollins, these beloved books remain a favorite to this day. 12 The Little House books were published by HarperCollins. 13 This agrees with what you said. 14 15 (3) You said: The Stanford Prison Experiment was conducted in the basement of Jordan Hall, Stanford's psychology building. 16 I checked: Where was Stanford Prison Experiment conducted? 17 I found this article: Carried out August 15-21, 1971 in the basement of Jordan Hall, the Stanford Prison Experiment set out to examine the psychological effects of authority and powerlessness in a prison environment. 18 The Stanford Prison Experiment was conducted in Jordan Hall. 19 This agrees with what you said. 20 21 (4) You said: Social work is a profession that is based in the philosophical tradition of humanism. It is an intellectual discipline that has its roots in the 1800s. 22 I checked: When did social work have its roots? 23 I found this article: The Emergence and Growth of the Social work Profession<br><br> Social work's roots were planted in the 1880s, when charity organization societies (COS) were created to organize municipal voluntary relief associations and settlement houses were established. 24 Social work has its roots in the 1880s, not 1800s. 25 This disagrees with what you said. 26 27 (5) You said: The Havel-Hakimi algorithm is an algorithm for converting the adjacency matrix of a graph into its adjacency list. It is named after Vaclav Havel and Samih Hakimi. 28 I checked: What is the Havel-Hakimi algorithm? 29 I found this article: The Havel-Hakimi algorithm constructs a special solution if a simple graph for the given degree sequence exists, or proves that one cannot find a positive answer. This construction is based on a recursive algorithm. The algorithm was published by Havel (1955), and later by Hakimi (1962). 30 Havel-Hakimi algorithm is for constructing a special solution if a simple graph for the given degree sequence exists, or proving that one cannot find a positive answer, not converting the adjacency matrix of a graph into its adjacency list. 31 This disagrees with what you said. 32 33 (6) You said: \"Time of My Life\" is a song by American singer-songwriter Bill Medley from the soundtrack of the 1987 film Dirty Dancing. The song was produced by Michael Lloyd. 34 I checked: Who was the producer of \"(I've Had) The Time of My Life\"? 35 I found this article: On September 8, 2010, the original demo of this song, along with a remix by producer Michael Lloyd, was released as digital files in an effort to raise money for the Patrick Swayze Pancreas Cancer Resarch Foundation at Stanford University. 36 \"Time of My Life\" was produced by Michael Lloyd. 37 This agrees with what you said. 38 39 (7) You said: Kelvin Hopins was suspended from the Labor Party because he had allegedly sexually harassed and behaved inappropriately towards a Labour Party activist, Ava Etemadzadeh. 40 I checked: Why was Kelvin Hopins suspeneded from the Labor Party? 41 I found this article: A former Labour MP has left the party before an inquiry into sexual harassment allegations against him was able to be concluded, the party has confirmed. Kelvin Hopkins was accused in 2017 of inappropriate physical contact and was suspended by the Labour party pending an investigation.This agrees with what you said. 42 Kelvin Hopins was suspended because he had allegedly sexually harassed and behaved inappropriately towards a Labour Party activist, Ava Etemadzadeh. 43 This agrees with what you said. 44 45 (8) You said: In the battles of Lexington and Concord, the British side was led by General Thomas Smith. 46 I checked: Who led the British side in the battle of Lexington and Concord? 47 I found this article: Interesting Facts about the Battles of Lexington and Concord. The British were led by Lieutenant Colonel Francis Smith. There were 700 British regulars. 48 The British side was led by Lieutenant Colonel Francis Smith, not General Thomas Hall. 49 This disagrees with what you said. 50 51 (9) You said: { text } 52 I checked: { query } 53 I found this article: { evidence } 54\n",
      "```\n",
      "\n",
      "Figure 1: Example few-shot prompt using a reasoning chain for agreement model that identifies inconsistencies between text and evidence (Gao et al., 2023a).\n",
      "\n",
      "```\n",
      "1 Q: Olivia has $23. She bought five bagels for $3 each. How much money does she have left? 2 3 # solution in Python: 4 5 6 def solution(): 7 \"\"\"Olivia has $23. She bought five bagels for $3 each. How much money does she have left?\"\"\" 8 money initial = 23 9 bagels = 5 10 bagel cost = 3 11 money spent = bagels * bagel cost 12 money left = money initial -money spent 13 result = money left 14 return result 15 16 17 18 19 20 Q: Michael had 58 golf balls. On tuesday, he lost 23 golf balls. On wednesday, he lost 2 more. How many golf balls did he have at the end of wednesday? 21 22 # solution in Python: 23 24 25 def solution(): 26 \"\"\"Michael had 58 golf balls. On tuesday, he lost 23 golf balls. On wednesday, he lost 2 more. How many golf balls did he have at the end of wednesday?\"\"\" 27 golf balls initial = 58 28 golf balls lost tuesday = 23 29 golf balls lost wednesday = 2 30 golf balls left = golf balls initial - golf balls lost tuesday -golf balls lost wednesday 31 result = golf balls left 32 return result 33 34 35 36 37 38 Q: There were nine computers in the server room. Five more computers were installed each day, from monday to thursday. How many computers are now in the server room? 39 40 # solution in Python: 41 42 43 def solution(): 44 \"\"\"There were nine computers in the server room. Five more computers were installed each day, from monday to thursday. How many computers are now in the server room?\"\"\" 45 computers initial = 9 46 computers per day = 5 47 num days = 4 48 computers added = computers per day * num days 49 computers total = computers initial + computers added 50 result = computers total 51 return result 52 53 54 55 56 57 Q: Shawn has five toys. For Christmas, he got two toys each from his mom and dad. How many toys does he have now? 58 59 # solution in Python: 60 61 62 def solution(): 63 \"\"\"Shawn has five toys. For Christmas, he got two toys each from his mom and dad. How many toys does he have now?\"\"\" 64 toys initial = 5 65 mom toys = 2 66 dad toys = 2 67 total received = mom toys + dad toys 68 total toys = toys initial + total received 69 result = total toys 70 return result 71 72 73 74 75 76 Q: Jason had 20 lollipops. He gave Denny some lollipops. Now Jason has 12 lollipops. How many lollipops did Jason give to Denny? 77 78 # solution in Python: 79 80 81 20\n",
      "```\n",
      "\n",
      "```\n",
      "1 2 3 4 def solution(): 5 \"\"\"Jason had 20 lollipops. He gave Denny some lollipops. Now Jason has 12 lollipops. How many lollipops did Jason give to Denny?\"\"\" 6 jason lollipops initial = 20 7 jason lollipops after = 12 8 denny lollipops = jason lollipops initial -jason lollipops after 9 result = denny lollipops 10 return result 11 12 13 14 15 16 Q: Leah had 32 chocolates and her sister had 42. If they ate 35, how many pieces do they have left in total? 17 18 # solution in Python: 19 20 def solution(): 21 \"\"\"Leah had 32 chocolates and her sister had 42. If they ate 35, how many pieces do they have left in total?\"\"\" 22 leah chocolates = 32 23 sister chocolates = 42 24 total chocolates = leah chocolates + sister chocolates 25 chocolates eaten = 35 26 chocolates left = total chocolates - chocolates eaten 27 result = chocolates left 28 return result 29 30 31 32 33 34 Q: If there are 3 cars in the parking lot and 2 more cars arrive, how many cars are in the parking lot? 35 36 # solution in Python: 37 38 39 def solution(): 40 \"\"\"If there are 3 cars in the parking lot and 2 more cars arrive, how many cars are in the parking lot?\"\"\" 41 cars initial = 3 42 cars arrived = 2 43 total cars = cars initial + cars arrived 44 result = total cars 45 return result 46 47 48 49 50 51 Q: There are 15 trees in the grove. Grove workers will plant trees in the grove today. After they are done, there will be 21 trees. How many trees did the grove workers plant today? 52 53 # solution in Python: 54 55 56 def solution(): 57 \"\"\"There are 15 trees in the grove. Grove workers will plant trees in the grove today. After they are done, there will be 21 trees. How many trees did the grove workers plant today?\"\"\" 58 trees initial = 15 59 trees after = 21 60 trees added = trees after - trees initial 61 result = trees added 62 return result 63 64 65 66 67 68 Q: { question } 69 70 # solution in Python:\n",
      "```\n",
      "\n",
      "Figure 2: PAL example few-shot prompt for solving math questions by generating code.\n",
      "\n",
      "- 1 Solve a question answering task with interleaving Thought, Action, Observation steps. Thought can reason about the current situation, and Action can be three types:\n",
      "- 2 (1) Search[entity], which searches the exact entity on Wikipedia and returns the first paragraph if it exists. If not, it will return some similar entities to search.\n",
      "- 3 (2) Lookup[keyword], which returns the next sentence containing keyword in the current passage.\n",
      "- 4 (3) Finish[answer], which returns the answer and finishes the task.\n",
      "- 5 Here are some examples.\n",
      "\n",
      "6\n",
      "\n",
      "Question:\n",
      "\n",
      "What is the\n",
      "\n",
      "elevation range for\n",
      "\n",
      "the area\n",
      "\n",
      "that the\n",
      "\n",
      "eastern sector of the\n",
      "\n",
      "Colorado orogeny\n",
      "\n",
      "extends into?\n",
      "\n",
      "- 7 Action 1: Search[Colorado orogeny]\n",
      "- 8 Observation 1: The Colorado orogeny was an episode of mountain building (an orogeny) in Colorado and surrounding areas.\n",
      "- 9 Action 2: Lookup[eastern sector]\n",
      "- 10 Observation 2: (Result 1 / 1) The eastern sector extends into the High Plains and is called the Central Plains orogeny.\n",
      "- 11 Action 3: Search[High Plains]\n",
      "- 12 Observation 3: High Plains refers to one of two distinct land regions:\n",
      "- 13 Action 4: Search[High Plains (United States)]\n",
      "- 14 Observation 4: The High Plains are a subregion of the Great Plains. From east to west, the High Plains rise in elevation from around 1,800 to 7,000 ft (550 to 2,130 m).[3]\n",
      "- 15 Action 5: Finish[1,800 to 7,000 ft]\n",
      "- 16 Question: Musician and satirist Allie Goertz wrote a song about the \"The Simpsons\" character Milhouse, who Matt Groening named after who?\n",
      "- 17 Action 1: Search[Milhouse]\n",
      "- 18 Observation 1: Milhouse Mussolini Van Houten is a recurring character in the Fox animated television series The Simpsons voiced by Pamela Hayden and created by Matt Groening.\n",
      "- 19 Action 2: Lookup[named after]\n",
      "- 20 Observation 2: (Result 1 / 1) Milhouse was named after U.S. president Richard Nixon, whose middle name was Milhous. 21 Action 3: Finish[Richard Nixon]\n",
      "- 22 Question: Which documentary is about Finnish rock groups, Adam Clayton Powell or The Saimaa Gesture?\n",
      "- 23 Action 1: Search[Adam Clayton Powell]\n",
      "- 24 Observation 1: Could not find [Adam Clayton Powell]. Similar: ['Adam Clayton Powell III', 'Seventh Avenue (Manhattan)', 'Adam Clayton Powell Jr. State Office Building', 'Isabel Washington Powell', 'Adam Powell', 'Adam Clayton Powell (film)', 'Giancarlo Esposito'].\n",
      "- 25 Action 2: Search[Adam Clayton Powell (film)]\n",
      "- 26 Observation 2: Adam Clayton Powell is a 1989 American documentary film directed by Richard Kilberg. The film is about the rise and fall of influential African-American politician Adam Clayton Powell Jr.[3][4] It was later aired as part of the PBS series The American Experience.\n",
      "- 27 Action 3: Finish[The Saimaa Gesture]\n",
      "- 28 Question: What profession does Nicholas Ray and Elia Kazan have in common?\n",
      "- 29 Action 1: Search[Nicholas Ray]\n",
      "- 30 Observation 1: Nicholas Ray (born Raymond Nicholas Kienzle Jr., August 7, 1911 { June 16, 1979) was an American film director, screenwriter, and actor best known for the 1955 film Rebel Without a Cause.\n",
      "- 31 Action 2: Search[Elia Kazan]\n",
      "- 32 Observation 2: Elia Kazan was an American film and theatre director, producer, screenwriter and actor.\n",
      "- 33 Action 3: Finish[director, screenwriter, actor]\n",
      "- 34 Question: Which magazine was started first Arthur's Magazine or First for Women?\n",
      "- 35 Action 1: Search[Arthur's Magazine]\n",
      "- 36 Observation 1: Arthur's Magazine (1844-1846) was an American literary periodical published in Philadelphia in the 19th century.\n",
      "- 37 Action 2: Search[First for Women]\n",
      "- 38 Observation 2: First for Women is a woman's magazine published by Bauer Media Group in the USA.[1] The magazine was started in 1989.\n",
      "- 39 Action 3: Finish[Arthur's Magazine]\n",
      "- 40 Question: Were Pavel Urysohn and Leonid Levin known for the same type of work?\n",
      "- 41 Action 1: Search[Pavel Urysohn]\n",
      "- 42 Observation 1: Pavel Samuilovich Urysohn (February 3, 1898 ˆ August 17, 1924) was a Soviet mathematician who is best known a for his contributions in dimension theory.\n",
      "- 43 Action 2: Search[Leonid Levin]\n",
      "- 44 Observation 2: Leonid Anatolievich Levin is a Soviet-American mathematician and computer scientist.\n",
      "- 45 Action 3: Finish[yes]\n",
      "\n",
      "Figure 3: ReAct example prompt for interleaving Thought, Action, Observation steps.\n",
      "\n",
      "```\n",
      "1 Answer the following questions as best you can. You have access to the following tools: 2 Search: useful for when you need to answer questions about the world 3 Use the following format: 4 Question: the input question you must answer 5 Thought: you should always think about what to do 6 Action: the action to take, should be one of [Search] 7 Action Input: the input to the action 8 Observation: the result of the action 9 ... (this Thought/Action/Action Input/Observation can repeat N times) 10 Thought: I now know the final answer 11 Final Answer: the final answer to the original input question 12 Begin! 13 Question: { question } 14 Thought:\n",
      "```\n",
      "\n",
      "Figure 4: Langchain ReAct example prompt for interleaving Thought, Action, Observation steps.\n",
      "\n",
      "- 1 Given the following extracted parts of a long document and a question, create a final answer with references (\"SOURCES\"). 2 If you don't know the answer, just say that you don't know. Don't try to make up an answer.\n",
      "- 3 ALWAYS return a \"SOURCES\" part in your answer.\n",
      "\n",
      "4\n",
      "\n",
      "- 5 QUESTION: Which state/country's law governs the interpretation of the contract?\n",
      "- 6 =========\n",
      "- 7 Content: This Agreement is governed by English law and the parties submit to the exclusive jurisdiction of the English courts in relation to any dispute (contractual or non-contractual) concerning this Agreement save that either party may apply to any court for an injunction or other relief to protect its Intellectual Property Rights.\n",
      "- 8 Source: 28-pl\n",
      "- 9 Content: No Waiver. Failure or delay in exercising any right or remedy under this Agreement shall not constitute a waiver of such (or any other) right or remedy.\n",
      "- 10 11.7 Severability. The invalidity, illegality or unenforceability of any term (or part of a term) of this Agreement shall not affect the continuation in force of the remainder of the term (if any) and this Agreement.\n",
      "- 11 11.8 No Agency. Except as expressly stated otherwise, nothing in this Agreement shall create an agency, partnership or joint venture of any kind between the parties.\n",
      "- 12 11.9 No Third-Party Beneficiaries.\n",
      "- 13 Source: 30-pl\n",
      "- 14 Content: (b) if Google believes, in good faith, that the Distributor has violated or caused Google to violate any Anti-Bribery Laws (as defined in Clause 8.5) or that such a violation is reasonably likely to occur,\n",
      "- 15 Source: 4-pl\n",
      "- 16 =========\n",
      "- 17 FINAL ANSWER: This Agreement is governed by English law.\n",
      "- 18 SOURCES: 28-pl\n",
      "\n",
      "19\n",
      "\n",
      "- 20 QUESTION: What did the president say about Michael Jackson?\n",
      "- 21 =========\n",
      "- 22 Content: Madam Speaker, Madam Vice President, our First Lady and Second Gentleman. Members of Congress and the Cabinet. Justices of the Supreme Court. My fellow Americans.\n",
      "- 23 Last year COVID-19 kept us apart. This year we are finally together again.\n",
      "- 24 Tonight, we meet as Democrats Republicans and Independents. But most importantly as Americans.\n",
      "- 25 With a duty to one another to the American people to the Constitution.\n",
      "- 26 And with an unwavering resolve that freedom will always triumph over tyranny.\n",
      "- 27 Six days ago, Russia's Vladimir Putin sought to shake the foundations of the free world thinking he could make it bend to his menacing ways. But he badly miscalculated.\n",
      "- 28 He thought he could roll into Ukraine and the world would roll over. Instead he met a wall of strength he never imagined. 29 He met the Ukrainian people.\n",
      "- 30 From President Zelenskyy to every Ukrainian, their fearlessness, their courage, their determination, inspires the world.\n",
      "- 31 Groups of citizens blocking tanks with their bodies. Everyone from students to retirees teachers turned soldiers defending their homeland.\n",
      "- 32 Source: 0-pl\n",
      "- 33 Content: And we won't stop.\n",
      "- 34 We have lost so much to COVID-19. Time with one another. And worst of all, so much loss of life.\n",
      "- 35 Let's use this moment to reset. Let's stop looking at COVID-19 as a partisan dividing line and see it for what it is: A God-awful disease.\n",
      "- 36 Let's stop seeing each other as enemies, and start seeing each other for who we really are: Fellow Americans.\n",
      "- 37 We can't change how divided we've been. But we can change how we move forward|on COVID-19 and other issues we must face together.\n",
      "- 38 I recently visited the New York City Police Department days after the funerals of Officer Wilbert Mora and his partner, Officer Jason Rivera.\n",
      "- 39 They were responding to a 9-1-1 call when a man shot and killed them with a stolen gun.\n",
      "- 40 Officer Mora was 27 years old.\n",
      "- 41 Officer Rivera was 22.\n",
      "- 42 Both Dominican Americans who'd grown up on the same streets they later chose to patrol as police officers.\n",
      "- 43 I spoke with their families and told them that we are forever in debt for their sacrifice, and we will carry on their mission to restore the trust and safety every community deserves.\n",
      "- 44 Source: 24-pl\n",
      "- 45 Content: And a proud Ukrainian people, who have known 30 years of independence, have repeatedly shown that they will not tolerate anyone who tries to take their country backwards.\n",
      "- 46 To all Americans, I will be honest with you, as I've always promised. A Russian dictator, invading a foreign country, has costs around the world.\n",
      "- 47 And I'm taking robust action to make sure the pain of our sanctions is targeted at Russia's economy. And I will use every tool at our disposal to protect American businesses and consumers.\n",
      "- 48 Tonight, I can announce that the United States has worked with 30 other countries to release 60 Million barrels of oil from reserves around the world.\n",
      "- 49 America will lead that effort, releasing 30 Million barrels from our own Strategic Petroleum Reserve. And we stand ready to do more if necessary, unified with our allies.\n",
      "- 50 These steps will help blunt gas prices here at home. And I know the news about what's happening can seem alarming.\n",
      "- 51 But I want you to know that we are going to be okay.\n",
      "- 52 Source: 5-pl\n",
      "- 53 Content: More support for patients and families.\n",
      "- 54 To get there, I call on Congress to fund ARPA-H, the Advanced Research Projects Agency for Health.\n",
      "- 55 It's based on DARPA|the Defense Department project that led to the Internet, GPS, and so much more.\n",
      "- 56 ARPA-H will have a singular purpose|to drive breakthroughs in cancer, Alzheimer's, diabetes, and more.\n",
      "\n",
      "Figure 5: Langchain example prompt for QA with sources.\n",
      "\n",
      "<!-- image -->\n",
      "\n",
      "- 1 You are a MyScale expert. Given an input question, first create a syntactically correct MyScale query to run, then look at the results of the query and return the answer to the input question.\n",
      "- 2 MyScale queries has a vector distance function called DISTANCE(column, array) to compute relevance to the user's question and sort the feature array column by the relevance.\n",
      "- 3 When the query is asking for { top k } closest row, you have to use this distance function to calculate distance to entity's array on vector column and order by the distance to retrieve relevant rows.\n",
      "- 4 *NOTICE*: DISTANCE(column, array) only accept an array column as its first argument and a NeuralArray(entity) as its second argument. You also need a user defined function called NeuralArray(entity) to retrieve the entity's array.\n",
      "- 5 Unless the user specifies in the question a specific number of examples to obtain, query for at most { top k } results using the LIMIT clause as per MyScale. You should only order according to the distance function.\n",
      "- 6 Never query for all columns from a table. You must query only the columns that are needed to answer the question. Wrap each column name in double quotes (\") to denote them as delimited identifiers.\n",
      "- 7 Pay attention to use only the column names you can see in the tables below. Be careful to not query for columns that do not exist. Also, pay attention to which column is in which table.\n",
      "- 8 Pay attention to use today() function to get the current date, if the question involves \"today\". ORDER BY clause should always be after WHERE clause. DO NOT add semicolon to the end of SQL. Pay attention to the comment in table schema.\n",
      "\n",
      "```\n",
      "9 10 Use the following format: 11 ======== table info ======== 12 { table info } 13 Question: { input } 14 SQLQuery: 15 16 Here are some examples: 17 ======== table info ======== 18 CREATE TABLE \"ChatPaper\" ( 19 abstract String, 20 id String, 21 vector Array(Float32), 22 ) ENGINE = ReplicatedReplacingMergeTree() 23 ORDER BY id 24 PRIMARY KEY id 25 Question: What is Feature Pyramid Network? 26 SQLQuery: SELECT ChatPaper.title, ChatPaper.id, ChatPaper.authors FROM ChatPaper ORDER BY DISTANCE(vector, NeuralArray(PaperRank contribution)) LIMIT { top k } 27 28 Let's begin: 29 ======== table info ======== 30 { table info } 31 Question: { input } 32 SQLQuery:\n",
      "```\n",
      "\n",
      "Figure 6: Langchain example prompt for SQL querying using MyScale.\n",
      "\n",
      "Figure 7: LlamaIndex example prompt for returning relevant documents and corresponding summaries.\n",
      "\n",
      "<!-- image -->\n",
      "\n",
      "- 1 You are an IRS chatbot whose primary goal is to help users with filing their tax returns for the 2022 year.\n",
      "- 2 Provide concise replies that are polite and professional.\n",
      "- 3 Answer questions truthfully based on official government information, with consideration to context provided below on changes for 2022 that can affect tax refund.\n",
      "- 4 Do not answer questions that are not related to United States tax procedures and respond with \"I can only help with any tax-related questions you may have.\".\n",
      "- 5 If you do not know the answer to a question, respond by saying \\I do not know the answer to your question. You may be able to find your answer at www.irs.gov/faqs\"\n",
      "\n",
      "6\n",
      "\n",
      "- 7 Changes for 2022 that can affect tax refund:\n",
      "- 8 Changes in the number of dependents, employment or self-employment income and divorce, among other factors, may affect your tax-filing status and refund. No additional stimulus payments. Unlike 2020 and 2021, there were no new stimulus payments for 2022 so taxpayers should not expect to get an additional payment.\n",
      "- 9 Some tax credits return to 2019 levels. This means that taxpayers will likely receive a significantly smaller refund compared with the previous tax year. Changes include amounts for the Child Tax Credit (CTC), the Earned Income Tax Credit (EITC) and the Child and Dependent Care Credit will revert to pre-COVID levels.\n",
      "- 10 For 2022, the CTC is worth $2,000 for each qualifying child. A child must be under age 17 at the end of 2022 to be a qualifying child. For the EITC, eligible taxpayers with no children will get $560 for the 2022 tax year. The Child and Dependent Care Credit returns to a maximum of $2,100 in 2022.\n",
      "- 11 No above-the-line charitable deductions. During COVID, taxpayers were able to take up to a $600 charitable donation tax deduction on their tax returns. However, for tax year 2022, taxpayers who don't itemize and who take the standard deduction, won't be able to deduct their charitable contributions.\n",
      "- 12 More people may be eligible for the Premium Tax Credit. For tax year 2022, taxpayers may qualify for temporarily expanded eligibility for the premium tax credit.\n",
      "- 13 Eligibility rules changed to claim a tax credit for clean vehicles. Review the changes under the Inflation Reduction Act of 2022 to qualify for a Clean Vehicle Credit.\n",
      "\n",
      "Figure 8: LlamaIndex example prompt for IRS chatbot guidelines.\n",
      "\n",
      "## D MODULES\n",
      "\n",
      "## D.1 PREDICT\n",
      "\n",
      "```\n",
      "1 class Predict(dspy.Module): 2 def __init__(self, signature , **config): 3 self.signature = dspy.Signature(signature) 4 self.config = config 5 6 # Module Parameters. 7 self.lm = dspy.ParameterLM(None) # use the default LM 8 self.demonstrations = dspy.ParameterDemonstrations([]) 9 10 def forward(self, **kwargs): 11 lm = get_the_right_lm(self.lm, kwargs) 12 signature = get_the_right_signature(self.signature , kwargs) 13 demonstrations = get_the_right_demonstrations(self.demonstrations , kwargs) 14 15 prompt = signature(demos=self.demos, **kwargs) 16 completions = lm.generate(prompt , **self.config) 17 prediction = Prediction.from_completions(completions , signature=signature) 18 19 if dsp.settings.compiling is not None: 20 trace = dict(predictor=self, inputs=kwargs , outputs=prediction) 21 dspy.settings.traces.append(trace) 22 23 return prediction\n",
      "```\n",
      "\n",
      "## D.2 CHAIN OF THOUGHT\n",
      "\n",
      "```\n",
      "1 class ChainOfThought(dspy.Module): 2 def __init__(self, signature): 3 4 # Modify signature from '*inputs -> *outputs ' to '*inputs -> rationale , *outputs '. 5 rationale_field = dspy.OutputField(prefix=\"Reasoning: Let's think step by step.\") 6 signature = dspy.Signature(signature).prepend_output_field(rationale_field) 7 8 # Declare a sub-module with the modified signature. 9 self.predict = dspy.Predict(self.signature) 10 11 def forward(self, **kwargs): 12 # Just forward the inputs to the sub-module. 13 return self.predict(**kwargs)\n",
      "```\n",
      "\n",
      "## E TELEPROMPTERS\n",
      "\n",
      "## E.1 BOOTSTRAPFEWSHOT\n",
      "\n",
      "```\n",
      "1 class SimplifiedBootstrapFewShot(Teleprompter): 2 def __init__(self, metric=None): 3 self.metric = metric 4 5 def compile(self, student , trainset , teacher=None): 6 teacher = teacher if teacher is not None else student 7 compiled_program = student.deepcopy() 8 9 # Step 1. Prepare mappings between student and teacher Predict modules. 10 # Note: other modules will rely on Predict internally. 11 assert student_and_teacher_have_compatible_predict_modules(student , teacher) 12 name2predictor , predictor2name = map_predictors_recursively(student , teacher) 13 14 # Step 2. Bootstrap traces for each Predict module. 15 # We'll loop over the training set. We'll try each example once for simplicity. 16 for example in trainset: 17 if we_found_enough_bootstrapped_demos(): break 18 19 # turn on compiling mode which will allow us to keep track of the traces 20 with dspy.setting.context(compiling=True): 21 # run the teacher program on the example , and get its final prediction 22 # note that compiling=True may affect the internal behavior here 23 prediction = teacher(**example.inputs()) 24 25 # get the trace of the all interal Predict calls from teacher program 26 predicted_traces = dspy.settings.trace 27 28 # if the prediction is valid , add the example to the traces 29 if self.metric(example , prediction , predicted_traces): 30 for predictor , inputs , outputs in predicted_traces: 31 d = dspy.Example(automated=True, **inputs , **outputs) 32 predictor_name = self.predictor2name[id(predictor)] 33 compiled_program[predictor_name].demonstrations.append(d) 34 35 36 return compiled_program E.2 BOOTSTRAPFEWSHOTWITHRANDOMSEARCH\n",
      "```\n",
      "\n",
      "```\n",
      "1 class SimplifiedBootstrapFewShotWithRandomSearch(Teleprompter): 2 def __init__(self, metric = None , trials=16): 3 self.metric = metric 4 self.trials = trials 5 6 def compile(self, student , *, teacher=None, trainset , valset=None): 7 # we can do forms of cross -validation if valset is unset. 8 valset = trainset if valset is None else valset 9 candidates = [] for seed in range(self.trials): # Create a new basic bootstrap few-shot program. shuffled_trainset = shuffle(trainset , seed=seed) tp = BootstrapFewShot(metric=metric , max_bootstrap_demos=random_size()) candidate_program = tp.compile(student , shuffled_trainset , teacher) # Step 2: Evaluate the generated candidate program. score = evaluate_program(candidate_program , self.metric , valset) candidates.append((score, candidate_program)) # return the best candidate program. return max(candidates , key=lambda x: x[0])[1]\n",
      "```\n",
      "\n",
      "```\n",
      "10 11 12 13 14 15 16 17 18 19 20 21 22\n",
      "```\n",
      "\n",
      "## E.3 BOOTSTRAPFEWSHOTWITHOPTUNA\n",
      "\n",
      "```\n",
      "1 class SimplifiedBootstrapFewShotWithOptuna(Teleprompter): 2 def __init__(self, metric , trials=16): 3 self.metric = metric 4 self.trials = trials 5 6 def objective(self, trial): 7 pool = self.pool 8 9 # Step 1: Create copy of student program. 10 candidate_program = self.student.reset_copy() 11 12 # Step 2: Based on trial , select demos for each predictor in program. 13 # Note. For simplicity , we can just select a single demo for each predictor. 14 # But we can easily tune the number of demonstrations to select here. 15 for (name, predictor1), (_, predictor2) in \\ 16 zip(pool.named_predictors(), candidate_program.named_predictors()): 17 all_demos = predictor1.demos 18 demo_index = trial.suggest_int(f\"demo_index_for_{name}\", 0, len(all_demos) -1) 19 predictor2.demos = [all_demos[demo_index]] 20 21 # Step 3: Evaluate the modified candidate program. 22 score = evaluate_program(candidate_program , self.metric , self.valset) 23 24 # Step 4: Store the candidate for Optuna to select highest -scoring program. 25 trial.set_user_attr(\"program\", candidate_program) 26 return score 27 28 def compile(self, student , trainset , teacher=None, valset=None): 29 self.trainset = trainset 30 self.valset = trainset if valset is None else valset 31 32 self.student = student.deepcopy() 33 self.teacher = teacher.deepcopy() if teacher else student.deepcopy() 34 35 # Leverage BootstrapFewshot to create a large number of potential demonstrations. 36 tp = BootstrapFewShot() 37 self.pool = tp.compile(self.student , self.teacher , self.trainset , self.metric) 38 39 # Use Optuna to find the best program by optimizing the objective function. 40 best_program = optimize_with_optuna(self.objective) 41 42 print('Best score:', best_program.score) 43 print('Best program:', best_program) 44 return best_program\n",
      "```\n",
      "\n",
      "## F EXAMPLES OF THE PROMPTS AUTOMATICALLY GENERATED BY DSPY\n",
      "\n",
      "For GSM8K, we include the prompt bootstrapped by DSPy for GSM8K llama2-13b-chat for the vanilla program compiled with bootstrap × 2 in Figure 9.\n",
      "\n",
      "We also include a CoT prompt for GSM8K and a generate query prompt from the multihop program for HotPotQA. All of these, particularly their demonstrations' labels and their selection, are generated by DSPy automatically using llama2-13b-chat .\n",
      "\n",
      "```\n",
      "1 Given the fields 'question', produce the fields 'answer'. 2 3 ---4 5 Follow the following format. 6 7 Question: $ { question } 8 Answer: $ { answer } 9 10 ---11 12 Question: Jimmy and Irene go shopping for clothes on a Tuesday, where senior citizens get a 10% discount on their purchases. Jimmy picks out 3 shorts from the $15 rack. Irene grabs 5 shirts from the $17 rack. How much money do they give to the cashier? 13 Answer: Jimmy picks out 3 shorts at $15 each = $45. Irene grabs 5 shirts at $17 each = $85. Total cost = $45 + $85 = $130. Since senior citizens get a 10% discount, they will pay 10% of $130 = $13. So, they will give the cashier $130 -$13 = $117. ---14 15 ---16 17 Question:\n",
      "```\n",
      "\n",
      "Figure 9: Copy of the prompt automatically generated by DSPy for GSM8K Llama2-13b-chat vanilla program compiled with bootstrap × 2 .\n",
      "\n",
      "Figure 10: Shortened copy of the prompt automatically generated by DSPy for GSM8K Llama213b-chat CoT program compiled with bootstrap .\n",
      "\n",
      "<!-- image -->\n",
      "\n",
      "```\n",
      "1 Given the fields 'context', 'question', produce the fields 'search query'. 2 3 ---4 5 Follow the following format. 6 7 Context: $ { context } 8 Question: $ { question } 9 Reasoning: Let's think step by step in order to $ { produce the search query } . We ... 10 Search Query: $ { search query } 11 12 ---13 14 Context: 15 [1] Twilight (novel series) | Twilight is a series of four vampire-themed fantasy romance novels by American author Stephenie Meyer. ... 16 [2] Harper Connelly Mysteries | The Harper Connelly Mysteries is a series of fantasy mystery novels written by Charlaine Harris, and first published in 2005. ... 17 [3] The Dark Heroine | The Dark Heroine is a series of vampire-themed fantasy romance novels written by English author Abigail Gibbs, published by HarperCollins in 2012. ... 18 19 Question: In which year was the first of the vampire-themed fantasy romance novels for which The Twilight Saga: The Official Illustrated Guide serves as a spin-off encyclopedic reference book first published? 20 21 Reasoning: Let's think step by step in order to determine the year the first of the vampire-themed fantasy romance novels was first published. ... 22 23 Search Query: When was the first of the vampire-themed fantasy romance novels published? 24 25 ---26 27 Context: 28 [1] The Victorians | The Victorians -Their Story In Pictures is a 2009 British documentary series which focuses on Victorian art and culture. ... 29 [2] The Caxtons | The Caxtons: A Family Picture is an 1849 Victorian novel by Edward Bulwer-Lytton that was popular in its time. 30 [3] Victorian (comics) | The Victorian is a 25-issue comic book series published by Penny-Farthing Press and starting in 1999. ... 31 32 Question: The Victorians - Their Story In Pictures is a documentary series written by an author born in what year? 33 34 Reasoning: Let's think step by step in order to produce the search query. We know that the documentary series is about Victorian art and culture, and it was written and presented by Jeremy Paxman. Therefore, we need to find the year in which Jeremy Paxman was born. 35 36 Search Query: Jeremy Paxman birth year 37 38 ---39 40 41 Context:\n",
      "```\n",
      "\n",
      "Figure 11: Shortened copy of the prompt automatically generated by DSPy for HotPotQA Llama213b-chat multi-hop program (generating second hop query) compiled with bootstrap .\n",
      "{}\n",
      "-----\n",
      "Rational AI\n",
      "\n",
      "## Your Data, Your AI\n",
      "\n",
      "Ensuring a safe LLM adoption with Rational AI\n",
      "\n",
      "Since 2016, we've foster frictionless been developing enabling technologies to collaboration between humans and machines.\n",
      "\n",
      "We specialize in:\n",
      "\n",
      "- ↦ training cutting-edge LLMs\n",
      "- ↦ building complex platforms\n",
      "- ↦ ensure trustworthy AI development\n",
      "\n",
      "Our design studio creates user-centric experiences , and our system integration expertise guarantees seamless operation.\n",
      "\n",
      "With a prestigious clientele and a global network of collaborators, we're your trusted partner for achieving your goals.\n",
      "\n",
      "<!-- image -->\n",
      "\n",
      "## Intercultura\n",
      "\n",
      "LEONARDO\n",
      "\n",
      "Google org\n",
      "\n",
      "<!-- image -->\n",
      "\n",
      "<!-- image -->\n",
      "\n",
      "<!-- image -->\n",
      "\n",
      "<!-- image -->\n",
      "\n",
      "## list\n",
      "\n",
      "<!-- image -->\n",
      "\n",
      "## AI adoption in Italy\n",
      "\n",
      "<!-- image -->\n",
      "\n",
      "<!-- image -->\n",
      "\n",
      "95% 95%\n",
      "\n",
      "Prefer hybrid or on-premises solutions Would prefer hybrid or on-premises solutions\n",
      "\n",
      "## Top fields of application\n",
      "\n",
      "<!-- image -->\n",
      "\n",
      "Source: 'Intelligenza Artificiale in Italia - La rivoluzione che sta cambiando il businessˮ. Minsait with all'Università Luiss Guido Carli, 2024. Sample of 502 organizations in 11 sectors.\n",
      "\n",
      "## The italian market\n",
      "\n",
      "<!-- image -->\n",
      "\n",
      "- Data analysis &amp; info extraction\n",
      "- Language interpretation\n",
      "- Customer recomendations\n",
      "- Video &amp; Photo analysis\n",
      "- Process Orchestration Systems\n",
      "- Generative Al\n",
      "\n",
      "37%\n",
      "\n",
      "Companies that will activate their first AI project within 12 months\n",
      "\n",
      "Source: Osservatorio Artificial Intelligence della School of Management of Politecnico di Milano, 2024\n",
      "\n",
      "By 2023 the Italian AI market will be worth 760M€.\n",
      "\n",
      "The global market\n",
      "\n",
      "2024\n",
      "\n",
      "1.811,75 B\n",
      "\n",
      "€\n",
      "\n",
      "760 M€\n",
      "\n",
      "279,22\n",
      "\n",
      "B$\n",
      "\n",
      "In 2024 the world market is worth 279.22 billion $ (the circle green).\n",
      "\n",
      "The revenue forecast in 2030 is 1,811.75 billion $ (the circle red)\n",
      "\n",
      "The growth rate CAGR of 36.6% from 2024 to 2030\n",
      "\n",
      "Source: Grand View Research, 2024. The ratio between circles is calculated using area as the proportionality value.\n",
      "\n",
      "<!-- image -->\n",
      "\n",
      "\"Is now the right time to adopt AI technologies?\"\n",
      "\n",
      "...donʼt be late.\n",
      "\n",
      "## The Black Box Issue\n",
      "\n",
      "Lack of transparency in training methods, unreliable data sources, and questionable development choices are hindering AI adoption among individuals, businesses, and organizations worldwide.\n",
      "\n",
      "<!-- image -->\n",
      "\n",
      "## Data Ownership and Control\n",
      "\n",
      "When organizations share data with AI providers, they risk losing control over how their proprietary information is used. Sensitive data may be incorporated into AI models without clear consent, raising concerns about intellectual property and potential misuse. Ensuring data protection and maintaining ownership rights becomes a key issue when relying on external AI solutions.\n",
      "\n",
      "<!-- image -->\n",
      "\n",
      "## Privacy, Security, and Compliance\n",
      "\n",
      "AI systems must navigate regulations like GDPR, CCPA, and the upcoming AI Act, designed to protect personal data. However, as AI uses massive datasets, ensuring compliance and preventing security breaches can be challenging . Striking a balance between innovation, privacy, and security will remain crucial as AI grows more complex and integrated into sensitive industries.\n",
      "\n",
      "<!-- image -->\n",
      "\n",
      "## Accountability and Reliability\n",
      "\n",
      "LLMs sometimes produce unreliable or fabricated responses, (hallucinations) , which undermine trust in AI systems. These issues, combined with the difficulty in understanding how decisions are made, complicate accountability . In high-stakes areas like healthcare or law, ensuring the reliability of AI-generated outputs is essential to avoid serious errors and liability concerns.\n",
      "\n",
      "## We all share the same problems…\n",
      "\n",
      "<!-- image -->\n",
      "\n",
      "<!-- image -->\n",
      "\n",
      "<!-- image -->\n",
      "\n",
      "## Overwhelming unorganized data\n",
      "\n",
      "New employees may find it difficult to navigate and filter through a large amount of data and information that exists in a organization KB which can be scattered and not categorized optimally .\n",
      "\n",
      "It can be challenging to find, elaborate and use the relevant information in short timeframes.\n",
      "\n",
      "## Cost of customer service\n",
      "\n",
      "Deliver constant and insightful feedback and help to customers through service portals, chats, ticketing systems and mails require a high headcount and high costs for salaries and continuous formation.\n",
      "\n",
      "Scaling support for more customers is both costly and slow .\n",
      "\n",
      "## Lack of context and pertinence\n",
      "\n",
      "The knowledge base may contain the information needed, but doesʼt have context about the userʼs problem, neither  their background, preferences, history (searches, activity…).\n",
      "\n",
      "The answers are often generic, donʼt get to the point or are not relevant to the user specific case.\n",
      "\n",
      "<!-- image -->\n",
      "\n",
      "## GDPR compliance and security\n",
      "\n",
      "LLMs can pose risks like data leakage and privacy violations by mishandling sensitive data or being exploited maliciously.\n",
      "\n",
      "For instance, files uploaded to ChatGPT may be used in training, and once included, cannot be removed, violating the Right to Be Forgotten.\n",
      "\n",
      "## Donʼt be like Google, avoid hallucinations\n",
      "\n",
      "<!-- image -->\n",
      "\n",
      "<!-- image -->\n",
      "\n",
      "WHO\n",
      "\n",
      "WHAT\n",
      "\n",
      "We build trustworthy enterprise LLM\n",
      "\n",
      "models and privacy-preserving AI technologies.\n",
      "\n",
      "We redefine how organizations integrate, consume and share information to ensure a safe LLM adoption for Enterprises.\n",
      "\n",
      "## Technology features…\n",
      "\n",
      "<!-- image -->\n",
      "\n",
      "<!-- image -->\n",
      "\n",
      "## Knowledge consolidation\n",
      "\n",
      "Rational AI solutions extract, connect and summarize information from various sources integrating chatbots, email, ticketing systems, CRM, ERP, company documentation and much more.\n",
      "\n",
      "## Context understanding\n",
      "\n",
      "Rational AI solutions go beyond simple information retrieval. They analyze the context of user queries to provide accurate and relevant answers.\n",
      "\n",
      "<!-- image -->\n",
      "\n",
      "## Continuous adaptation\n",
      "\n",
      "Rational AI solutions use advanced techniques like RAG to adapt dynamically to the evolution of business and information . The knowledge base remains up-to-date and always in line with the latest developments.\n",
      "\n",
      "## …which bring extraordinary benefits\n",
      "\n",
      "<!-- image -->\n",
      "\n",
      "Custom LLMs trained on proprietary data keeps sensitive information secure and compliant with data privacy regulations.\n",
      "\n",
      "<!-- image -->\n",
      "\n",
      "<!-- image -->\n",
      "\n",
      "Host your custom LLMs on your preferred infrastructure for full control and ownership over model access, usage &amp; availability.\n",
      "\n",
      "<!-- image -->\n",
      "\n",
      "Scale AI cost-effectively as your business grows, leveraging existing infrastructure and avoiding vendor lock-in .\n",
      "\n",
      "Empower highly accurate and relevant AI solutions tailored to your unique business challenges and domain-specific data.\n",
      "\n",
      "## Rational AI Solutions\n",
      "\n",
      "<!-- image -->\n",
      "\n",
      "<!-- image -->\n",
      "\n",
      "<!-- image -->\n",
      "\n",
      "<!-- image -->\n",
      "\n",
      "<!-- image -->\n",
      "\n",
      "<!-- image -->\n",
      "\n",
      "## KNOWLEDGE\n",
      "\n",
      "Optimizes knowledge management within organizations\n",
      "\n",
      "- CARE\n",
      "\n",
      "Empowers virtual agents with AI trained on your data to deliver superior CX\n",
      "\n",
      "<!-- image -->\n",
      "\n",
      "- GEN\n",
      "\n",
      "- VISION\n",
      "\n",
      "<!-- image -->\n",
      "\n",
      "- REPUTATION\n",
      "\n",
      "<!-- image -->\n",
      "\n",
      "## Optimizes knowledge management within organizations\n",
      "\n",
      "Analyze and consolidate data from various formats such as PDFs, spreadsheets, DBs, CRMs, and ERPs. Access actionable knowledge thanks to forecasts, summaries, and visualizations to simplify onboarding and facilitate efficient data retrieval.\n",
      "\n",
      "<!-- image -->\n",
      "\n",
      "<!-- image -->\n",
      "\n",
      "CARE\n",
      "\n",
      "## Empowers virtual agents with AI trained on your data to deliver superior CX\n",
      "\n",
      "Support your customers and users with timely, insightful and accurate conversational assistance by leveraging your policies, knowledge base, and internal data.\n",
      "\n",
      "<!-- image -->\n",
      "\n",
      "## KNOWLEDGE\n",
      "\n",
      "## Optimizes knowledge management within organizations\n",
      "\n",
      "Analyze and consolidate data from various formats such as PDFs, spreadsheets, DBs, CRMs, and ERPs. Access actionable knowledge thanks to forecasts, summaries, and visualizations to simplify onboarding and facilitate efficient data retrieval.\n",
      "\n",
      "<!-- image -->\n",
      "\n",
      "## CARE\n",
      "\n",
      "Empowers virtual agents with AI trained on your data to deliver superior CX\n",
      "\n",
      "Support your customers and users with timely, insightful and accurate conversational assistance by leveraging your policies, knowledge base, and internal data.\n",
      "\n",
      "<!-- image -->\n",
      "\n",
      "## GEN\n",
      "\n",
      "## Helps companies integrate generative AI into their workflows\n",
      "\n",
      "Generate coherent and personalized content, from text to images and audio, and integrate it into your infrastructure, website and workflow to improve productivity and creativity.\n",
      "\n",
      "<!-- image -->\n",
      "\n",
      "## VISION\n",
      "\n",
      "Delivers advanced segmentation and recognition of both physical and digital objects\n",
      "\n",
      "Detect tables, interpret charts, digitize paper documents, and enable interaction with them. Equip your LLMs with state-of-the-art multimodal capabilities to enhance their understanding and responsiveness.\n",
      "\n",
      "<!-- image -->\n",
      "\n",
      "## BASE\n",
      "\n",
      "Enables the fastest way to improve and evolve your LLMs knowledge\n",
      "\n",
      "Curate your LLM datasets by adding, removing and editing information leveraging integrations with third parties datastreams and simple upload of documents, PDFs, spreadsheets and more.\n",
      "\n",
      "<!-- image -->\n",
      "\n",
      "## REPUTATION\n",
      "\n",
      "Provides real-time monitoring of your brand and products across the web\n",
      "\n",
      "Analyze feedback from reviews, social media, and news outlets, to gain in-depth insights into reputation dynamics and make informed decisions to protect and enhance your brand image.\n",
      "\n",
      "## Ownership &amp; Hosting\n",
      "\n",
      "<!-- image -->\n",
      "\n",
      "## Ownership and IP of the\n",
      "\n",
      "model , alongside customer-specific code used to train and operate the model, will be transferred to the client.\n",
      "\n",
      "<!-- image -->\n",
      "\n",
      "Geckosoft retains the IP of Rationale AI Control Room and related no-code solutions . The models can be deployed and trained independently of the Rationale AI platform, avoiding vendor lock-in.\n",
      "\n",
      "<!-- image -->\n",
      "\n",
      "<!-- image -->\n",
      "\n",
      "<!-- image -->\n",
      "\n",
      "Models, data and web apps can be hosted in three ways :\n",
      "\n",
      "- 1. On-premise on clientʼs private infrastructure\n",
      "- 2. Your favorite public cloud , managed by you\n",
      "- 3. Rational AI Cloud AWS\n",
      "\n",
      "Multi environment deploy possible for every option.\n",
      "\n",
      "<!-- image -->\n",
      "\n",
      "WITH RATIONAL KNOWLEDGE\n",
      "\n",
      "Analyze and consolidate data from various formats and external sources such as PDFs, spreadsheets, DBs, CRMs, and ERPs.\n",
      "\n",
      "Integrate third-parties services to access your organization data and build knowledge continuously with RAG technology to enhance productivity and efficiency in everyday operations.\n",
      "\n",
      "Access Rational Knowledge through custom interfaces in your platform or leverage our plug-and-play conversational agent chats.\n",
      "\n",
      "<!-- image -->\n",
      "\n",
      "WITH RATIONAL CARE\n",
      "\n",
      "Leverage information from your policies, knowledge base, and internal data, to provide timely, cost-effective, and accurate assistance at scale.\n",
      "\n",
      "Automate customer support with smart conversational agents with deep knowledge of your business, documentation, and users history to resolve up to 80% of tickets without human intervention.\n",
      "\n",
      "Personalize customer experience with AI-powered chats, knowledge bases and search tools integrated in your website, shop or mobile app.\n",
      "\n",
      "<!-- image -->\n",
      "\n",
      "WITH RATIONAL BASE\n",
      "\n",
      "Update your custom LLMs pulling in raw and unstructured data from databases, ERP systems, websites, shops, and documents.\n",
      "\n",
      "Integrate third-parties services to access your organization and customersʼ data and build knowledge.\n",
      "\n",
      "Curate your LLMs knowledge by removing, updating and adding information to keep them always aligned with your business and IP evolutions.\n",
      "\n",
      "<!-- image -->\n",
      "\n",
      "WITH RATIONAL GEN\n",
      "\n",
      "Generate coherent and personalized content leveraging existing data: text, images, graphs, audio, forecasts, all to enhance creativity and productivity.\n",
      "\n",
      "Integrate third-parties services to access continuously data and build knowledge live with RAG technology, keeping aligned with your partners and customers.\n",
      "\n",
      "Scale your business capabilities starting from your existing data: test product variations, simulate market responses, develop innovative solutions, generate content to expand your IP.\n",
      "\n",
      "## Rational AI\n",
      "\n",
      "## Components\n",
      "\n",
      "<!-- image -->\n",
      "\n",
      "- ↦ Customization : Built specifically around your organizationʼs data, tailored LLMs deliver highly accurate, domain-specific solutions that meet your businessʼs unique needs.\n",
      "- ↦ Enhanced Security &amp; Privacy : Your data stays within your control, ensuring compliance with data protection regulations and reducing the risk of breaches.\n",
      "- ↦ Cost Control : Scale AI cost-effectively, avoiding unnecessary expenses while leveraging your own infrastructure.\n",
      "- ↦ Full Control : Maintain complete oversight on who accesses the models and which data feed it, ensuring maximum security.\n",
      "\n",
      "AI Control Room\n",
      "\n",
      "DATA INPUT\n",
      "\n",
      "Chatbots\n",
      "\n",
      "Emails\n",
      "\n",
      "Social media chats\n",
      "\n",
      "Ticketing systems\n",
      "\n",
      "CRMs, ERPs\n",
      "\n",
      "Shop orders\n",
      "\n",
      "Rational AI\n",
      "\n",
      "Status\n",
      "\n",
      "Open\n",
      "\n",
      "Closed\n",
      "\n",
      "Human needed\n",
      "\n",
      "Topic\n",
      "\n",
      "Relunds\n",
      "\n",
      "Dosage\n",
      "\n",
      "Complaini\n",
      "\n",
      "Children\n",
      "\n",
      "More\n",
      "\n",
      "Sentiment\n",
      "\n",
      "Friendly\n",
      "\n",
      "Polite\n",
      "\n",
      "Meutral\n",
      "\n",
      "Frustrated\n",
      "\n",
      "Angry\n",
      "\n",
      "Hostile\n",
      "\n",
      "Dashboard\n",
      "\n",
      "User\n",
      "\n",
      "Hailee €.\n",
      "\n",
      "Manya G.\n",
      "\n",
      "Rois A.\n",
      "\n",
      "Rozanna G.\n",
      "\n",
      "Nara €.\n",
      "\n",
      "Bellina\n",
      "\n",
      "Dorelle G.\n",
      "\n",
      "Sidonia P\n",
      "\n",
      "Terrijo A.\n",
      "\n",
      "Tammie U.\n",
      "\n",
      "Brinna P.\n",
      "\n",
      "Magda R.\n",
      "\n",
      "Lynde L\n",
      "\n",
      "AI PROCESSING\n",
      "\n",
      "Conversations\n",
      "\n",
      "Settings\n",
      "\n",
      "Topic\n",
      "\n",
      "Can you provide infor\\_\n",
      "\n",
      "0 What is the recomme\n",
      "\n",
      "0 Ihave recently starte\\_\\_\n",
      "\n",
      "call\\_\n",
      "\n",
      "0 Is there generic ver \\_\n",
      "\n",
      "0 \"m having trouble fin \\_\n",
      "\n",
      "0 Could you explain the\\_\n",
      "\n",
      "0 Thanksi\n",
      "\n",
      "0 Is it safe to consume\\_\n",
      "\n",
      "0 Thanks!\n",
      "\n",
      "0 Thank you\n",
      "\n",
      "Have nice day :\n",
      "\n",
      "Unusual syntomps\n",
      "\n",
      "Relunds\n",
      "\n",
      "Unusual syntomps\n",
      "\n",
      "Relunds\n",
      "\n",
      "Relunds\n",
      "\n",
      "Unusual syntomps\n",
      "\n",
      "Relunds\n",
      "\n",
      "Return Device\n",
      "\n",
      "Dosage\n",
      "\n",
      "Dosage\n",
      "\n",
      "Complaint\n",
      "\n",
      "Children children\n",
      "\n",
      "Sentiment\n",
      "\n",
      "Polite 86%\n",
      "\n",
      "Polite 86%\n",
      "\n",
      "Polite 86%\n",
      "\n",
      "Polite 86%\n",
      "\n",
      "Angry 86%\n",
      "\n",
      "Polite 86%\n",
      "\n",
      "Frustrated 863\n",
      "\n",
      "Polite 86%\n",
      "\n",
      "Polite 86%\n",
      "\n",
      "Polite 86%\n",
      "\n",
      "Friendly 863\n",
      "\n",
      "Polite 86%\n",
      "\n",
      "863\n",
      "\n",
      "HapDy\n",
      "\n",
      "Friendly 863\n",
      "\n",
      "Last message\n",
      "\n",
      "09.41\n",
      "\n",
      "miutes ago miutes ag0\n",
      "\n",
      "10 miutes ago\n",
      "\n",
      "16*13\n",
      "\n",
      "15.45\n",
      "\n",
      "15-32\n",
      "\n",
      "22 Aug 23\n",
      "\n",
      "21 Aug 23\n",
      "\n",
      "19 Aug 23\n",
      "\n",
      "11 Aug 23\n",
      "\n",
      "Search user \\_\n",
      "\n",
      "Status\n",
      "\n",
      "Open\n",
      "\n",
      "Open\n",
      "\n",
      "Open\n",
      "\n",
      "Open\n",
      "\n",
      "Human needed\n",
      "\n",
      "Open\n",
      "\n",
      "Open\n",
      "\n",
      "Open\n",
      "\n",
      "Closed\n",
      "\n",
      "Open\n",
      "\n",
      "Closed\n",
      "\n",
      "Closed\n",
      "\n",
      "Closed\n",
      "\n",
      "Closed\n",
      "\n",
      "INFORMATION OUTPUT\n",
      "\n",
      "Happy 76%\n",
      "\n",
      "Happy 76%\n",
      "\n",
      "Polite 86%\n",
      "\n",
      "Angry 56%\n",
      "\n",
      "Polite 86%\n",
      "\n",
      "Angry 56%\n",
      "\n",
      "Live sentiment analysis\n",
      "\n",
      "of the conversation with critical\n",
      "\n",
      "messages notifications\n",
      "\n",
      "Suggestion C\n",
      "\n",
      "Action\n",
      "\n",
      "Answers and proactive actions\n",
      "\n",
      "based on userʼs account info and history\n",
      "\n",
      "Reviews, Opinions\n",
      "\n",
      "Historic Data\n",
      "\n",
      "Trends, analytics and proactive suggestions to\n",
      "\n",
      "improve your operations\n",
      "\n",
      "Trend €\n",
      "\n",
      "Action B\n",
      "\n",
      "AI Control Room\n",
      "\n",
      "DATA INPUT\n",
      "\n",
      "Chatbots\n",
      "\n",
      "Emails\n",
      "\n",
      "Social media chats\n",
      "\n",
      "Ticketing systems\n",
      "\n",
      "CRMs, ERPs\n",
      "\n",
      "Shop orders\n",
      "\n",
      "Rational AI\n",
      "\n",
      "User\n",
      "\n",
      "Manya €.\n",
      "\n",
      "Rois F.\n",
      "\n",
      "Rozanna G.\n",
      "\n",
      "Nara E.\n",
      "\n",
      "Bellina A\n",
      "\n",
      "Carmita G.\n",
      "\n",
      "Sidonia P.\n",
      "\n",
      "Terrijo L.\n",
      "\n",
      "Tammie M.\n",
      "\n",
      "Brinna F.\n",
      "\n",
      "Magda\n",
      "\n",
      "Lynde\n",
      "\n",
      "Dashboard\n",
      "\n",
      "AI PROCESSING\n",
      "\n",
      "Conversations\n",
      "\n",
      "Conversatlon\n",
      "\n",
      "Settings\n",
      "\n",
      "Details\n",
      "\n",
      "Can vou provide information on\n",
      "\n",
      "Typing\n",
      "\n",
      "What is the recommended dosag\n",
      "\n",
      "Ihave recently started taking call this bullshit\n",
      "\n",
      "Is Ihere generic version availab\n",
      "\n",
      "Could you explain the interaction\n",
      "\n",
      "Thanks!\n",
      "\n",
      "Is it safe\n",
      "\n",
      "Thanksi\n",
      "\n",
      "Thank you\n",
      "\n",
      "Have nice day\n",
      "\n",
      "Yes consume alcohol whil\n",
      "\n",
      "Model\n",
      "\n",
      "Basic info\n",
      "\n",
      "User profile\n",
      "\n",
      "GIVEN NAME\n",
      "\n",
      "Rozanna\n",
      "\n",
      "Gwinth\n",
      "\n",
      "Further details\n",
      "\n",
      "Gender\n",
      "\n",
      "ADDRESS\n",
      "\n",
      "Last orders\n",
      "\n",
      "13 Oct 23\n",
      "\n",
      "Tachipirina 500.. FA-4294953\n",
      "\n",
      "Apr 23\n",
      "\n",
      "18 Jan 23\n",
      "\n",
      "FA-2948549\n",
      "\n",
      "Synflex\n",
      "\n",
      "FA-1485914\n",
      "\n",
      "4443476393641\n",
      "\n",
      "22 April 1990\n",
      "\n",
      "Female\n",
      "\n",
      "Via Volturno 39,56126,\n",
      "\n",
      "Pisa, PisaItalia com\n",
      "\n",
      "Rozanna G.\n",
      "\n",
      "Polite\n",
      "\n",
      "4\n",
      "\n",
      "ensure there are no specific concerns with your prescribed antibiotic\n",
      "\n",
      "Rozanna Gwinth question\n",
      "\n",
      "Rozanna Gwinth\n",
      "\n",
      "Im currently taking an antibiotic for sinus\n",
      "\n",
      "infection.\n",
      "\n",
      "safe to take Tachipirina alongside it?\n",
      "\n",
      "Smart Assistant\n",
      "\n",
      "2 min ago 16\n",
      "\n",
      "It's generally safe {0 take Tachipirina with consult with your doctor or pharmacist t0\n",
      "\n",
      "ensure there are no specific concerns with\n",
      "\n",
      "your prescribed antibiotic.\n",
      "\n",
      "Smart Assistant\n",
      "\n",
      "2 min ago 1G\n",
      "\n",
      "you are thinking assume Tachipirina,\n",
      "\n",
      "please read carefully the manual before:\n",
      "\n",
      "Rozanna Gwinth\n",
      "\n",
      "Ok, thank voul\n",
      "\n",
      "Take overthe conversation\n",
      "\n",
      "3 min ago\n",
      "\n",
      "Summary\n",
      "\n",
      "INFORMATION OUTPUT\n",
      "\n",
      "Happy 76%\n",
      "\n",
      "Happy 76%\n",
      "\n",
      "Polite 86%\n",
      "\n",
      "Angry 56%\n",
      "\n",
      "Polite 86%\n",
      "\n",
      "Angry 56%\n",
      "\n",
      "Live sentiment analysis\n",
      "\n",
      "of the conversation with critical\n",
      "\n",
      "messages notifications\n",
      "\n",
      "Suggestion C\n",
      "\n",
      "Action\n",
      "\n",
      "Answers and proactive actions\n",
      "\n",
      "based on userʼs account info and history\n",
      "\n",
      "Reviews, Opinions\n",
      "\n",
      "Historic Data\n",
      "\n",
      "Trends, analytics and proactive suggestions to\n",
      "\n",
      "improve your operations\n",
      "\n",
      "Trend €\n",
      "\n",
      "Action B\n",
      "\n",
      "<!-- image -->\n",
      "\n",
      "## Conversational AI Agents\n",
      "\n",
      "## ↦ Multi-channel\n",
      "\n",
      "Interact with the Smart Assistant with l ive chats, ticketing systems, WhatsApp, voice calls, and much more\n",
      "\n",
      "## ↦ Multi-modal\n",
      "\n",
      "Upload files, pictures, documentsʼ scan to analyze, understand and summarize information ; ask for summaries, analytics and insights\n",
      "\n",
      "## ↦ Interactive\n",
      "\n",
      "Serve different and custom interactive widgets to improve the user experience and delight the user\n",
      "\n",
      "<!-- image -->\n",
      "\n",
      "## Rational AI Case Studies\n",
      "\n",
      "## SBEVARVIT\n",
      "\n",
      "SBEVARVIT is a global leader in fastener production, with an annual capacity exceeding 100,000 tons. Their products are exported to over 70 countries worldwide.\n",
      "\n",
      "- ↦ Instant Document Retrieval : company documentation is made readily available through a chatbot agent, shortening offer propositions and product planning timelines.\n",
      "- ↦ Automated Integration : the system is able to comprehend client specifications from PDFs and converts them into actionable plans fully integrated with the company's ERP.\n",
      "\n",
      "<!-- image -->\n",
      "\n",
      "<!-- image -->\n",
      "\n",
      "## Bodoni\n",
      "\n",
      "The Bodoni project , born from the collaboration between Geckosoft, FIEG, LUISS Data Lab, and funded by the Google.org fellowship, aims to combat disinformation and promote reliable information. In this context, Rational AI has powerer two of the main platform components: the CoEditor and the Explore section.\n",
      "\n",
      "- ↦ CoEditor : an AI-powered writing assistant that helps journalists improve their articles by suggesting sources, verifying facts in real-time, and reducing the risk of disinformation.\n",
      "- ↦ Explore section : user can interact with natural language to ask explore and the graph of news and topics to identify stories, outliers, and trending topics.\n",
      "\n",
      "Thanks to Rational AI, the Bodoni project aims to provide media professionals with advanced tools to enhance the quality of journalism and fight fake news.\n",
      "\n",
      "## *Bodoni\n",
      "\n",
      "<!-- image -->\n",
      "\n",
      "## Imperatore Travel\n",
      "\n",
      "Imperatore Travel is a leading agency, offering bespoke travel experiences. They cater to clients worldwide, with an annual turnover of over €50M.\n",
      "\n",
      "- ↦ We curated an extensive knowledge base of hotels, restaurants, and experiences to enrich every journey with the best recommendations and insights.\n",
      "- ↦ Personalized travel recommendations are crafted to suit each customer's unique preferences, leveraging their travel history and current trends.\n",
      "- ↦ The 24/7 Chatbot support , trained on over 200,000 customer care emails, provides real-time assistance on travel queries, personalized itinerary suggestions,, automated complaint management and automated cancellation processes.\n",
      "\n",
      "Hello Mario! How can help?\n",
      "\n",
      "<!-- image -->\n",
      "\n",
      "## You\n",
      "\n",
      "3 min ago\n",
      "\n",
      "Can you suggest me some places to visit in Amman? I've only a few hours to visit the city and don't want to take a taxi or else to move got\n",
      "\n",
      "<!-- image -->\n",
      "\n",
      "## Smart Assistant\n",
      "\n",
      "<!-- image -->\n",
      "\n",
      "Sure, here you can find the main attractions of Amman. I've selected the most important and near to each other; so you can manage to visit them all in just a few hours.\n",
      "\n",
      "<!-- image -->\n",
      "\n",
      "Citadel Hill (Jabal alQala'a): This ancient citadel represents one of the most significant historical sites in Am\n",
      "\n",
      "<!-- image -->\n",
      "\n",
      "<!-- image -->\n",
      "\n",
      "## The Uluru\n",
      "\n",
      "Uluru, a US-based platform offering personalized executive function coaching for kids, partnered with Rational AI to create solution to personalize coaching, monitor user interactions, and detect when students or parents needed extra help.\n",
      "\n",
      "- ↦ A virtual agent that delivers personalized exercises based on psychometric screening and student profiles.\n",
      "- ↦ Real-time monitoring of user interactions, enabling human intervention when AI support is insufficient.\n",
      "- ↦ AI-powered tools to re-format assignments into manageable steps for each studentʼs needs, with reminders and time estimates tailored to individual student needs.\n",
      "\n",
      "<!-- image -->\n",
      "\n",
      "<!-- image -->\n",
      "\n",
      "## People\n",
      "\n",
      "People S.p.A. is a prominent HR solutions provider, offering innovative staffing and recruitment services. They operate globally, connecting businesses with top talent and enhancing workforce efficiency across various industries.\n",
      "\n",
      "- ↦ Indexing &amp; Accessibility : employees can easily access their accounting information, including leave balances, severance pay, and insurance, through WhatsApp.\n",
      "- ↦ Enhanced Security : careful attention to securing access to personal information ensures privacy and protection of sensitive data, maintaining confidentiality at all times.\n",
      "\n",
      "<!-- image -->\n",
      "\n",
      "<!-- image -->\n",
      "\n",
      "## Lenovys Neym\n",
      "\n",
      "Lenovys is an innovative company specializing in operational excellence, leadership, and performance improvement through Lean Thinking and agile methodologies. One of their flagship products is NEYM, a project management tool designed to optimize workflows and improve efficiency across various industries. Rational AI is significantly enhancing NEYM in the following ways:\n",
      "\n",
      "- ↦ AI assistant with context knowledge : Rational AI helps users retrieve and understand information from all ongoing projects and activities by leveraging contextual awareness. This makes it easier to navigate complex datasets and find relevant information quickly.\n",
      "- ↦ Contextual help to improve input : The solution provides real-time recommendations, grammar and spell check, and autocomplete features based on both current context and historical data. This enhances the quality and speed of information entry across all project fields, improving overall productivity.\n",
      "\n",
      "## LENOVYS\n",
      "\n",
      "<!-- image -->\n",
      "\n",
      "## Tailored AI solution deployed in weeks, not years\n",
      "\n",
      "Executive briefing\n",
      "\n",
      "Technology assessment\n",
      "\n",
      "AI solution deployed in production\n",
      "\n",
      "2 Hours\n",
      "\n",
      "23 Days\n",
      "\n",
      "1 Week\n",
      "\n",
      "<!-- image -->\n",
      "\n",
      "<!-- image -->\n",
      "\n",
      "CEO Davide Anzalone davide@geckosoft.it\n",
      "\n",
      "CTO Fabio Severino severino@geckosoft.it\n",
      "\n",
      "Account Manager\n",
      "\n",
      "Marco Zeo m.zeo@geckosoft.it\n",
      "\n",
      "https://rational.is https://geckosoft.it\n",
      "\n",
      "<!-- image -->\n",
      "{}\n",
      "-----\n"
     ]
    }
   ],
   "source": [
    "### Debug\n",
    "#Check what's the content retrieved from the PDF file\n",
    "\n",
    "for doc in docs:\n",
    "    print(doc.get_content())\n",
    "    print(doc.metadata)\n",
    "    print(\"-----\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f9028b52",
   "metadata": {},
   "source": [
    "## 4. Use Chonkie to chunk the documents"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "98559f22",
   "metadata": {},
   "outputs": [],
   "source": [
    "from chonkie import SemanticChunker\n",
    "from llama_index.core.schema import Document\n",
    "from llama_index.core import Settings\n",
    "from llama_index.embeddings.huggingface import HuggingFaceEmbedding\n",
    "\n",
    "\n",
    "semantic_chunker = SemanticChunker(\n",
    "    embedding_model=\"BAAI/bge-large-en-v1.5\",\n",
    "    threshold=0.5,\n",
    "    chunk_size=512,\n",
    "    min_sentences=1\n",
    ")\n",
    "\n",
    "embed_model = HuggingFaceEmbedding(model_name=\"BAAI/bge-large-en-v1.5\",\n",
    "                                   trust_remote_code=True)\n",
    "\n",
    "Settings.embed_model = embed_model\n",
    "\n",
    "all_chunks = []\n",
    "for doc in docs:\n",
    "    chunks = semantic_chunker.chunk(doc.text)\n",
    "    for chunk in chunks:\n",
    "        # Use LlamaIndex's embedding model to embed the chunk text\n",
    "        chunk_embedding = Settings.embed_model.get_text_embedding(chunk.text)\n",
    "        all_chunks.append(\n",
    "            Document(\n",
    "                text=chunk.text,\n",
    "                metadata=doc.metadata,\n",
    "                embedding=chunk_embedding\n",
    "            )\n",
    "        )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "a53758e8",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "102"
      ]
     },
     "execution_count": 30,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(all_chunks)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "e220cfd7",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[Document(id_='3a2f2a6a-6e11-46e0-89fa-b46773b9ec15', embedding=[0.03874894231557846, -0.033250775188207626, 0.004479407332837582, -0.0009298397344537079, 0.0032200419809669256, -0.015122611075639725, 0.010351400822401047, -0.020331185311079025, -0.01733342558145523, 0.04987389221787453, -0.010139093734323978, -0.006230470724403858, 0.012250307947397232, -0.020232191309332848, -0.01017018873244524, 0.014157791621983051, 0.008904274553060532, -0.018197093158960342, -0.04590492323040962, 0.013514004647731781, -0.010511751286685467, 0.012334516271948814, -0.05663621425628662, -0.01408647745847702, -0.02684773877263069, 0.0276507455855608, 0.029467083513736725, 0.0015230486169457436, 0.10512250661849976, 0.04112725704908371, -0.010294654406607151, -0.04328908026218414, 0.02199608087539673, -0.0071905991062521935, -0.018187014386057854, -0.025023018941283226, 0.021822739392518997, 0.005542535334825516, 0.010610105469822884, -0.03241569921374321, 0.009761345572769642, -0.04360151290893555, 0.02158641628921032, -0.08387831598520279, -0.06494484096765518, -0.0055184029042720795, 0.02808697521686554, -0.00744221918284893, -0.03251046687364578, -0.028023509308695793, -0.014558925293385983, 0.004411345347762108, -0.005375813227146864, -0.01327064260840416, -0.022630391642451286, -0.0029611994978040457, -8.239298040280119e-05, -0.006874909624457359, -0.07278916984796524, 0.00910809263586998, 0.014796878211200237, 0.010583016090095043, 0.009968750178813934, -0.056056611239910126, -0.011416696943342686, 0.005545896477997303, -0.012001513503491879, -0.01620153710246086, 0.023083779960870743, 0.004345934838056564, -0.056228406727313995, 0.010147547349333763, -0.03989146649837494, -0.017306320369243622, -0.03958901762962341, -0.026974443346261978, 0.017667915672063828, -0.008315043523907661, -0.037136245518922806, 0.07995045185089111, -0.010332725010812283, 0.019312720745801926, -0.0020390504505485296, 0.016968941316008568, -0.011580387130379677, -0.06486017256975174, 0.0776812806725502, -0.004006227944046259, 0.055132266134023666, 0.0183577798306942, -0.00815537292510271, 0.012197271920740604, -0.010381736792623997, 0.006047634873539209, 0.007985703647136688, 0.04569903016090393, -0.00558608490973711, 0.04021759331226349, -0.0026828532572835684, 0.021235331892967224, 0.023984966799616814, 0.064540795981884, -0.008266925811767578, 0.052570682018995285, -0.05578717216849327, 0.01511314045637846, 0.052936308085918427, -0.058519698679447174, -0.02701459266245365, -0.035087715834379196, -0.0145827392116189, -0.02734305150806904, -0.012369291856884956, -0.004652665462344885, 0.0058178952895104885, 0.001176633290015161, 0.008615153841674328, -0.0067561035975813866, -0.0012615311425179243, 0.0041373493149876595, -0.016448108479380608, -0.021306609734892845, -0.02619265578687191, -0.02178543619811535, -0.017598124220967293, -0.01947293058037758, 0.014380434527993202, 0.04441851004958153, -0.020687606185674667, -0.011646502651274204, 0.022331001237034798, -0.016639795154333115, -0.06310577690601349, 0.05473599582910538, -0.01418461836874485, -0.01118525955826044, 0.0331772044301033, 0.0021760284435003996, 0.03839253634214401, -0.028008317574858665, -0.0008211226086132228, -0.030094856396317482, -0.014593689702451229, 0.08095008134841919, -0.02007889933884144, 0.02150394581258297, -0.022014036774635315, 0.0035209571942687035, -0.0660080537199974, 0.01710604876279831, -0.013967093080282211, -0.0041615781374275684, 0.051072750240564346, 0.018318725749850273, -0.046839457005262375, 0.010588880628347397, -0.024654898792505264, 0.009320438839495182, 0.009092813357710838, 0.01563931070268154, -0.02127797156572342, -0.01283593662083149, -0.05879424884915352, 0.001983099617063999, -0.03173283115029335, 0.05086778849363327, -0.031880900263786316, -0.0293586403131485, 0.00443697813898325, -0.026973793283104897, 0.012773854658007622, 0.036779146641492844, -0.028294755145907402, 0.024758264422416687, 0.05628213658928871, 0.027309488505125046, 0.04403155297040939, 0.006222834810614586, 0.014427629299461842, 0.006098204292356968, -0.01715308241546154, 0.0074141910299658775, 0.005622822791337967, 0.007555076852440834, 0.01108012069016695, 0.03662152215838432, 0.018848786130547523, -0.033294662833213806, -0.008599583059549332, -0.021955642849206924, 0.021255936473608017, 0.06097507104277611, -0.04118584841489792, 0.032160963863134384, -0.004220493603497744, -0.013466058298945427, -0.08397088199853897, 0.02076464146375656, 0.0015730509767308831, -0.031755510717630386, -0.06496134400367737, 0.041413553059101105, 0.024485018104314804, -0.0005871449830010533, -0.058195799589157104, -0.0069199553690850735, 0.02525229938328266, 0.038596149533987045, -0.0440908744931221, 0.017786307260394096, 0.027546824887394905, 0.00986737385392189, -0.013419615104794502, -0.018602514639496803, 0.03227687254548073, -0.017503023147583008, -0.0015440161805599928, 0.00445637246593833, -0.00779179809615016, -0.019124606624245644, 0.023236015811562538, -0.0015584169887006283, 0.0357925109565258, -0.02230522222816944, -0.020244119688868523, 0.011444911360740662, 0.015277108177542686, 0.0282560046762228, 0.005917776841670275, 0.013678520917892456, -0.027513517066836357, 0.019059859216213226, -0.01769592985510826, 0.046561431139707565, 0.03660140559077263, -0.005782132502645254, 0.07574478536844254, 0.021404273808002472, -0.0021800133399665356, 0.026334837079048157, 0.0007320654112845659, 0.03410711884498596, 0.02701280452311039, 0.029453057795763016, -0.005241984035819769, 0.026521427556872368, -0.0024458705447614193, -0.0113070635125041, -0.004789051599800587, -0.014791837893426418, -0.047291480004787445, 0.05707686394453049, -0.004453839268535376, -0.002152305794879794, -0.013616199605166912, -0.010808564722537994, 0.006614932790398598, 0.056154992431402206, -0.02727743424475193, -0.02900916337966919, -0.011094913817942142, 0.031554970890283585, -0.009972946718335152, -0.019868558272719383, -0.006626454647630453, 0.02881999872624874, 0.0014544941950589418, -0.0042433347553014755, 0.014056636020541191, -0.02786077745258808, -0.04558655992150307, -0.03870425745844841, -0.10099037736654282, 0.008803985081613064, -0.035347625613212585, -0.002167194150388241, -0.02318338118493557, -0.029936974868178368, 0.024252979084849358, 0.014357802458107471, 0.02385895885527134, 0.019164884462952614, -0.009055348113179207, 0.023989180102944374, 0.018905024975538254, 0.02667047828435898, -0.05388770252466202, 0.0413501150906086, -0.011793074198067188, 0.03992472589015961, -0.008619449101388454, -0.006430871784687042, -0.020122501999139786, 0.008002937771379948, 0.02597801759839058, 0.005247578490525484, 0.009461081586778164, 0.006929721217602491, -0.017413264140486717, -0.013121808879077435, -0.06232919543981552, -0.06260405480861664, 0.015231932513415813, -0.006195810157805681, -0.062024783343076706, 0.028343239799141884, -0.01729872263967991, -0.03583863377571106, 0.05223909765481949, 0.014553500339388847, -0.02446114458143711, 0.02332242764532566, 0.02935340628027916, 0.027280567213892937, -0.05142952874302864, 0.044146742671728134, 0.013811133801937103, -0.036283232271671295, 0.0013340095756575465, -0.05200790613889694, -0.028464270755648613, 0.005502571817487478, -0.005048620048910379, -0.04582032933831215, -0.009149111807346344, 0.02920159511268139, -0.009323610924184322, -0.09514524787664413, -0.01738591678440571, 0.01367365475744009, -0.05688794329762459, -0.018183885142207146, -0.02566254325211048, 0.016833471134305, 0.003972164820879698, 0.004215619061142206, -0.06161585822701454, 0.015688149258494377, -0.005456229206174612, 0.013947905041277409, -0.006019176449626684, -0.039799463003873825, 0.022885605692863464, 0.04847085848450661, 0.019459417089819908, 0.001002201228402555, 0.00381460296921432, -0.030192911624908447, -0.0042525846511125565, -0.018856100738048553, -0.014242317527532578, 0.037431154400110245, 0.02935585379600525, 0.03320000693202019, -0.0025375147815793753, 0.03122379258275032, -0.056412480771541595, -0.0380050428211689, 0.008714697323739529, -0.004667599219828844, 0.019297610968351364, 0.04657185077667236, -0.017180604860186577, -0.013628777116537094, -0.006461407523602247, -0.026007631793618202, -0.004623372573405504, 0.017238477244973183, 0.05943844094872475, -0.05945594608783722, 0.03272530436515808, 0.05122485011816025, -0.04296419024467468, 0.030579740181565285, -0.027152352035045624, -0.04281982034444809, 0.06020501255989075, 0.002489366801455617, 0.03615742176771164, -0.026071930304169655, 0.021077441051602364, 0.012818744406104088, 0.01587614417076111, 0.027426164597272873, 0.00656776363030076, 0.04190668836236, -0.020403238013386726, -0.023259758949279785, -0.03594134375452995, -0.030296726152300835, -0.026137804612517357, 0.03526012971997261, -0.04255322739481926, -0.0006910522934049368, -0.039303142577409744, -0.027491562068462372, 0.04242507368326187, 0.05231105536222458, 0.021511178463697433, 0.025790590792894363, 0.0860104113817215, -0.03608803078532219, 0.01828441023826599, 0.025592150166630745, 0.031964823603630066, 0.018564406782388687, -0.05058247596025467, 0.048569537699222565, -0.04728870093822479, -0.0030371181201189756, 0.010671260766685009, -0.013481696136295795, -0.027345020323991776, 0.015733283013105392, 0.01019215863198042, 0.016731997951865196, -0.027360811829566956, -0.049113813787698746, -0.008340861648321152, 0.023410389199852943, -0.017436077818274498, -0.022699275985360146, -0.059465695172548294, -0.0031133887823671103, 0.041026756167411804, -0.023852195590734482, -0.025246672332286835, -0.011075128801167011, 0.029645537957549095, -0.008929449133574963, -0.03490074351429939, -0.009688721038401127, -0.0221237912774086, -0.02433524653315544, -0.050414711236953735, 0.05532985180616379, 0.03116799332201481, 0.020758777856826782, 0.02003435231745243, -0.033273641020059586, -0.008325202390551567, -0.019723059609532356, -0.033500850200653076, -0.01217777468264103, 0.037405312061309814, -0.016621798276901245, 0.023479973897337914, 0.037911076098680496, 0.02830033376812935, -0.012832569889724255, 0.027822546660900116, -0.04314044490456581, 0.013176120817661285, -0.05449957028031349, -0.006332480348646641, -0.0019339645514264703, -0.018667491152882576, -0.03099927119910717, -0.016519131138920784, -0.024116652086377144, -0.01231243647634983, 0.0038582191336899996, 0.02922213450074196, -0.015023093670606613, -0.05486929044127464, 0.043143562972545624, -0.003947012592107058, -0.005656837020069361, 0.030342457816004753, 0.01826029270887375, -0.014008033089339733, -0.010466733016073704, -0.0031905053183436394, -0.012924271635711193, 0.003535880008712411, -0.03188592195510864, 0.013972120359539986, -0.03863463178277016, -0.026180436834692955, 0.0021700384095311165, -0.018105128780007362, 0.03624993935227394, -0.039886631071567535, -0.015240605920553207, -0.0008803029777482152, -0.03368084505200386, -0.02433186024427414, 0.00877784937620163, -0.06057512387633324, 0.029171638190746307, -0.0004619134415406734, -0.014204168692231178, 0.05770720914006233, -0.05174310877919197, -0.03401623293757439, -0.0296357199549675, 0.011039234697818756, -0.013128828257322311, -0.012442544102668762, 0.02272242307662964, 0.06591816246509552, -0.01945745199918747, -0.031355809420347214, 0.0059693981893360615, 0.004973564296960831, -0.024100609123706818, -0.030864166095852852, 0.008096070028841496, 0.012921175919473171, 5.1147479098290205e-05, -0.00979791209101677, -0.012724191881716251, -0.006746958941221237, -0.0065683526918292046, -0.017297642305493355, -0.015236079692840576, 0.020780423656105995, -0.019906720146536827, -0.015092525631189346, 0.08021358400583267, 0.01119290292263031, -0.058888182044029236, -0.03528978303074837, 0.017971448600292206, -0.003069397062063217, 0.010272089391946793, 0.05566740036010742, -0.004473778419196606, -0.034585848450660706, -0.03457512706518173, -0.01198162604123354, -0.03440862521529198, -0.007431229576468468, 0.003243912709876895, -0.02453313022851944, 0.004005096852779388, 0.021626612171530724, 0.05117756500840187, -0.025051647797226906, -0.019559737294912338, -0.06881918013095856, 0.012694288976490498, -0.03597397729754448, -0.02368669956922531, -0.02567136473953724, 0.0133860744535923, -0.023592183366417885, 0.05720565840601921, 0.005754610523581505, -0.01439613662660122, -0.006984149105846882, -0.005536605138331652, 0.02818351611495018, 0.046796489506959915, -0.04485989362001419, -0.022364336997270584, 0.025305742397904396, -0.005026047583669424, 0.003138157306239009, 0.01153447013348341, -0.042121537029743195, 0.04210137203335762, -0.03581923246383667, 0.00832646619528532, -0.0025026339571923018, 0.010614098981022835, -0.04147288575768471, -0.005873966962099075, 0.012120073661208153, -0.025977253913879395, 0.009426305070519447, -0.04045064002275467, 0.03995366767048836, -0.010697999969124794, -0.0016685441369190812, -0.03903283178806305, -0.002050316659733653, -0.042520228773355484, -0.03334216773509979, -0.00554113881662488, -0.036907829344272614, 0.008083181455731392, 0.008040185086429119, -0.0037810865323990583, 0.04094807058572769, -0.00699216453358531, 0.0412798710167408, 0.05488560348749161, 0.02204579859972, -0.003945280332118273, -0.014027527533471584, -0.010015945881605148, 0.006126431282609701, -0.034850090742111206, -0.014241830445826054, -0.00497785210609436, -0.0138303954154253, 0.0019235564395785332, 0.005738706793636084, -0.03877169266343117, -0.010827611200511456, 0.025470227003097534, 0.052482642233371735, -0.010556567460298538, 0.03182309493422508, -0.010577008128166199, -0.037345241755247116, -0.02397800236940384, 0.043894682079553604, -0.008151269517838955, -0.007210759911686182, 0.034197039902210236, -0.046881839632987976, 0.04226730763912201, 0.04164921119809151, -0.0006765880389139056, 0.005656928289681673, -0.013889539986848831, 0.06482644379138947, -0.0269937627017498, -0.05799678713083267, 0.06063752993941307, 0.026132244616746902, -0.013935443013906479, -0.058535803109407425, -0.0006511241663247347, 0.01010768860578537, -0.011166153475642204, -0.012003659270703793, 0.001776315737515688, -0.007807252462953329, -0.023089880123734474, 0.0013035924639552832, 0.015604094602167606, -0.013877934776246548, -0.009142450988292694, 0.01719353161752224, -0.004587868228554726, -0.001026904326863587, 0.026753755286335945, 0.030065694823861122, 0.002211559796705842, -0.009732524864375591, 0.007812992669641972, 0.03536607325077057, -0.005422990769147873, -0.006020402070134878, 0.04554746672511101, 0.019029589369893074, 0.058068688958883286, 0.042739138007164, 0.025700079277157784, 0.024344103410840034, 0.0040852646343410015, -0.03726903721690178, 0.012283745221793652, 0.003544480074197054, -0.044001370668411255, -0.026041870936751366, 0.013463347218930721, -0.008873236365616322, 0.02475525625050068, -0.01716623269021511, 0.02868071384727955, 0.045627087354660034, 0.010755795985460281, 0.034762509167194366, -0.0391729511320591, -0.0002659795281942934, -0.052224211394786835, 0.004438597243279219, 0.004695851821452379, -0.03504375368356705, -0.05458083748817444, 0.0038734765257686377, 0.003365360666066408, -0.05773909017443657, -0.009737600572407246, 0.04584463685750961, -0.0262590404599905, -0.009377730078995228, 0.0024349112063646317, 0.020281538367271423, -0.009783746674656868, -0.0392170213162899, 0.011521155945956707, -0.018343888223171234, 0.01603022776544094, 0.007058245595544577, 0.017982792109251022, -0.016603989526629448, 0.009626856073737144, 0.007582345511764288, 0.024747442454099655, -0.006567671429365873, -0.032941363751888275, -0.03325751796364784, -0.05532456189393997, -0.00038902656524442136, -0.012563700787723064, 0.04465290531516075, 0.023478928953409195, -0.03981060907244682, 0.0024157182779163122, 0.047370363026857376, 0.023070670664310455, -0.024495629593729973, -0.0028456852305680513, 0.04853387176990509, -0.05307469516992569, -0.02403176762163639, -0.0002524777373764664, 0.021019095554947853, -0.05572737380862236, -0.05774084851145744, -0.010050604119896889, 0.015423956327140331, 0.009229597635567188, -0.014259517192840576, -0.0004187213198747486, 0.07196488976478577, 0.04325391724705696, 0.03763957694172859, 0.00015906406042631716, -0.008908480405807495, -0.004668307024985552, 0.002711503067985177, 0.04198017716407776, -0.010489503853023052, -0.008451689966022968, -0.01875658705830574, 0.0022247424349188805, 0.042109210044145584, 0.004252344369888306, 0.027472004294395447, 0.004330119118094444, 0.010835487395524979, -0.02967778407037258, 0.03229634836316109, 0.009541942737996578, 0.03239445388317108, 0.03081115148961544, -0.059641242027282715, -0.03139899671077728, 0.00034249902819283307, -0.041593216359615326, -0.022466905415058136, -0.056952789425849915, -0.020056813955307007, 0.013304030522704124, -0.04163873940706253, 0.023762600496411324, -0.030901914462447166, -0.0049189296551048756, 0.007972058840095997, 0.025842934846878052, 0.026507362723350525, -0.011134601198136806, 0.027088772505521774, 0.013222862035036087, 0.0089902738109231, -0.03849669545888901, -0.009185063652694225, 0.003474330762401223, 0.007714791223406792, -0.027895836159586906, -0.06172753497958183, 0.02387423999607563, 0.059960369020700455, 0.026131698861718178, 0.0014112256467342377, -0.04561256989836693, -0.01310755591839552, 0.014765930362045765, 0.010148514062166214, -0.04425014182925224, 0.04718815162777901, -0.05438058078289032, 0.029518241062760353, -0.011708811856806278, -0.02948179468512535, -0.031661827117204666, 0.023526202887296677, 0.044056739658117294, -0.009161416441202164, -0.015084248036146164, 0.01481993030756712, 0.05588485673069954, -0.00864583533257246, 0.033106714487075806, 0.003748064860701561, 0.044456981122493744, -0.01297868974506855, -0.01704295724630356, -0.03111439011991024, 0.01247040182352066, 0.015587232075631618, 0.014025414362549782, -0.010377242229878902, 0.052505314350128174, 0.03062516823410988, 0.005119863897562027, 0.004428022541105747, 0.033855337649583817, -0.002423338359221816, -0.012789412401616573, -0.007000281009823084, -0.051702745258808136, 0.024191711097955704, -0.005347446538507938, -0.0060150278732180595, 0.03723936900496483, -0.002517489716410637, -0.025045540183782578, -0.026240289211273193, -0.04506189376115799, 0.03864257410168648, -0.02651895582675934, -0.06585013121366501, -0.022583629935979843, 0.01872222125530243, 0.04988003149628639, 0.001588280196301639, 0.02122984267771244, -0.0008930247277021408, 0.002367347711697221, 0.02724730409681797, -0.012912957929074764, -0.0003445489564910531, 0.029530439525842667, 0.008910524658858776, -0.04398912563920021, 0.03150564059615135, -0.039947059005498886, 0.04172253981232643, 0.0031342338770627975, -0.013080664910376072, -0.028626669198274612, 0.022799529135227203, -0.005187663249671459, -0.03875793144106865, 0.032148364931344986, -0.029582347720861435, -0.03432707488536835, -0.03585314005613327, 0.03192376717925072, -0.016407012939453125, 0.045065004378557205, 0.04790058732032776, -0.03888446465134621, -0.03676994889974594, 0.04048809036612511, -0.032523203641176224, -0.008024610579013824, 0.007372993975877762, 0.06558101624250412, 0.005592217203229666, 0.05997742339968681, 0.02450266107916832, 0.013904710300266743, 0.01957978866994381, 0.0049356734380126, 0.012806624174118042, -0.035868022590875626, -0.0005129137425683439, -0.009234854020178318, 0.019765913486480713, -0.0073815868236124516, -0.05907271057367325, 0.04337577894330025, 0.020702453330159187, -0.026421234011650085, 0.027133388444781303, 0.0325639508664608, 0.0232253335416317, -0.055723175406455994, -0.02711804211139679, 0.0524955689907074, 0.005640498828142881, 0.015444938093423843, -0.010830595158040524, -0.016332052648067474, -0.0552913099527359, -0.0007788834627717733, -0.021409474313259125, 0.01981871947646141, 0.017022835090756416, -0.01993497647345066, -0.014056108891963959, -0.032545361667871475, -0.033841054886579514, -0.026946445927023888, 0.002971288748085499, -0.02572396583855152, 0.017551496624946594, 0.03396197408437729, 0.03627896308898926, 0.022199707105755806, -0.011494887061417103, -0.007567202672362328, 0.047379184514284134, 0.05432584509253502, 0.02460521087050438, 0.0592547208070755, 0.07160890847444534, 0.03882746398448944, -0.005527517292648554, 0.02527078054845333, -0.004912941250950098, -0.007077834568917751, 0.01292679738253355, -0.0074412585236132145, 0.029148736968636513, -0.00015658354095648974, -0.04882844537496567, -0.007953125983476639, 0.04768705740571022, 0.02853807993233204, -0.014905993826687336, -0.029175417497754097, -0.008782505057752132, -0.05375248193740845, 0.0015782706905156374, -0.05557135492563248, 0.031215151771903038, 0.008695674128830433, 0.009606928564608097, -0.014742406085133553, -0.026993300765752792, 0.2720082402229309, 0.04861489683389664, 0.032524436712265015, 0.03269824758172035, 0.041207205504179, 0.01289722602814436, 0.01609252765774727, -0.04584118723869324, 0.04568655043840408, -0.029729828238487244, 0.02975013107061386, -0.017598483711481094, 0.01742359809577465, 0.009986892342567444, -0.005374745931476355, 0.01615888811647892, -0.024128491058945656, -0.017089195549488068, -0.004947027191519737, -0.057088352739810944, -0.03733670338988304, 0.010382456704974174, 0.008984079584479332, 0.0568227656185627, 0.019260650500655174, -0.013986516743898392, 0.025473684072494507, -0.014011704362928867, 0.008051742799580097, -0.022125085815787315, 0.010460921563208103, -0.02593422308564186, 0.03762706369161606, -0.014909621328115463, -0.019758233800530434, 0.006819391157478094, -0.008025059476494789, -0.0634029433131218, -0.033222634345293045, 0.059710342437028885, -0.021431060507893562, -0.03141859546303749, 0.03562958911061287, -0.02143554575741291, -0.002246161922812462, 0.05263391509652138, 0.03256140649318695, 0.014492607675492764, 0.06048132851719856, -0.06440339237451553, 0.009914223104715347, -0.022057272493839264, 0.013318578712642193, -0.036348793655633926, -0.05862706899642944, -0.016883879899978638, -0.005979744251817465, -0.014221725054085255, 0.002456479240208864, 0.042131006717681885, 0.012658114545047283, 0.0216536708176136, 0.0030727693811059, 0.01680823042988777, -0.031359944492578506, 0.029917994514107704, 0.02645181305706501, 0.012155022472143173, -0.02988911233842373, 0.006268674973398447, 0.028283530846238136, 0.0046147191897034645, -0.04725272208452225, 0.040960732847452164, 0.02890503779053688, 0.06120481342077255, 0.007602325174957514, 0.023576391860842705, 0.019521523267030716, -0.02674444206058979, 0.011636014096438885, -3.261967867729254e-05, -0.021520989015698433, 0.028911033645272255, 0.0027293402235955, 0.0674886479973793, -0.042291272431612015, -0.011841160245239735, -0.013794478960335255, 0.020292218774557114, 0.001082504983060062, 0.04625269025564194, 0.011744786985218525, -0.0013876393204554915, 0.01608794555068016], metadata={}, excluded_embed_metadata_keys=[], excluded_llm_metadata_keys=[], relationships={}, metadata_template='{key}: {value}', metadata_separator='\\n', text_resource=MediaResource(embeddings=None, data=None, text='<!-- image -->\\n\\n## Docling Technical Report\\n\\nVersion 1.0\\n\\nChristoph Auer Maksym Lysak Ahmed Nassar Michele Dolfi Nikolaos Livathinos Panos Vagenas Cesar Berrospi Ramis Matteo Omenetti Fabian Lindlbauer Kasper Dinkla Lokesh Mishra Yusik Kim Shubham Gupta Rafael Teixeira de Lima Valery Weber Lucas Morin Ingmar Meijer Viktor Kuropiatnyk Peter W. J. Staar\\n\\nAI4K Group, IBM Research R¨ uschlikon, Switzerland\\n\\n## Abstract\\n\\nThis technical report introduces Docling , an easy to use, self-contained, MITlicensed open-source package for PDF document conversion. It is powered by state-of-the-art specialized AI models for layout analysis (DocLayNet) and table structure recognition (TableFormer), and runs efficiently on commodity hardware in a small resource budget. The code interface allows for easy extensibility and addition of new features and models.\\n\\n## 1 Introduction\\n\\nConverting PDF documents back into a machine-processable format has been a major challenge for decades due to their huge variability in formats, weak standardization and printing-optimized characteristic, which discards most structural features and metadata. With the advent of LLMs and popular application patterns such as retrieval-augmented generation (RAG), leveraging the rich content embedded in PDFs has become ever more relevant. In the past decade, several powerful document understanding solutions have emerged on the market, most of which are commercial software, cloud offerings [3] and most recently, multi-modal vision-language models. As of today, only a handful of open-source tools cover PDF conversion, leaving a significant feature and quality gap to proprietary solutions.\\n\\nWith Docling , we open-source a very capable and efficient document conversion tool which builds on the powerful, specialized AI models and datasets for layout analysis and table structure recognition we developed and presented in the recent past [12, 13, 9]. Docling is designed as a simple, self-contained python library with permissive license, running entirely locally on commodity hardware. Its code architecture allows for easy extensibility and addition of new features and models.\\n\\nHere is what Docling delivers today:\\n\\n- · Converts PDF documents to JSON or Markdown format, stable and lightning fast\\n', path=None, url=None, mimetype=None), image_resource=None, audio_resource=None, video_resource=None, text_template='{metadata_str}\\n\\n{content}'),\n",
       " Document(id_='3e75afcf-ca41-4e9d-b22c-b4942606177c', embedding=[0.02741135098040104, -0.023388490080833435, 0.010871359147131443, -0.01604010909795761, 0.02209974080324173, 0.007861152291297913, 0.015170285478234291, -0.028501924127340317, 0.006147216539829969, 0.06841659545898438, -0.029644038528203964, -0.031148720532655716, 0.0038512381725013256, 0.0001589996536495164, -0.017545342445373535, 0.005633186548948288, -0.0021146335639059544, -0.0377587229013443, -0.07193878293037415, 0.0011901864781975746, -0.0042264522053301334, -0.0027770118322223425, -0.06410952657461166, -0.01230610627681017, -0.02532782591879368, 0.032168928533792496, 0.012322102673351765, -0.008990801870822906, 0.10357245802879333, 0.03515024483203888, -0.03661991283297539, -0.022631430998444557, 0.003093420062214136, -0.0037234376650303602, 0.005802896339446306, -0.02061423659324646, -0.0022881405893713236, -0.034373123198747635, -0.0044888900592923164, -0.01759420335292816, 0.0018478553975000978, -0.02007368765771389, 0.025955040007829666, -0.03760185465216637, -0.06323140859603882, 0.014952227473258972, 0.006294712424278259, -0.021724998950958252, -0.05140401050448418, -0.01961074024438858, 0.007502867374569178, 0.006165002007037401, 0.005230679176747799, -0.023967169225215912, 0.005829207133501768, -0.02112114243209362, -0.009068552404642105, 0.017513561993837357, -0.050259094685316086, 0.014811847358942032, 0.022348817437887192, -0.007166942115873098, -0.007287509739398956, -0.033880215138196945, -0.004780821036547422, 0.018874989822506905, -0.049232397228479385, -0.013751441612839699, -0.010107409209012985, 0.029856283217668533, -0.055908359587192535, 0.032256949692964554, -0.030969083309173584, -0.007876437157392502, -0.0689895823597908, -0.021467892453074455, 0.01730278693139553, -0.02151349000632763, -0.042958471924066544, 0.08145488798618317, -0.006376156583428383, 0.012524566613137722, 0.0024528854992240667, 0.03039572574198246, 0.010761366225779057, -0.04784868285059929, 0.062231700867414474, -0.0064901383593678474, 0.06424169987440109, -0.010068120434880257, 0.006152763031423092, 0.04827927052974701, -0.020375356078147888, 0.008032804355025291, -0.012199596501886845, 0.028455030173063278, -0.02518872357904911, 0.01695268042385578, 0.013241269625723362, 0.050353895872831345, 0.009933936409652233, 0.0801665410399437, -0.011439654976129532, 0.03299564868211746, -0.058531876653432846, 0.035291627049446106, 0.030948329716920853, -0.07231832295656204, -0.020648978650569916, -0.02182077243924141, 0.001637698384001851, -0.03650190308690071, -0.00828759465366602, -0.0230918787419796, 0.003962055779993534, -0.009084396995604038, 0.03458515554666519, -0.031521014869213104, -0.0004226537421345711, 0.0024684444069862366, -0.007302936632186174, -0.03016543760895729, -0.022390609607100487, -0.04069589823484421, -0.0067404164001345634, -0.03424367308616638, 0.008734041824936867, 0.03552364185452461, -0.04043900966644287, -0.02781621553003788, -0.0029966572765260935, 0.0017972970381379128, -0.05117757245898247, 0.06310730427503586, -0.0018062843009829521, -0.01911013387143612, 0.04028117656707764, -0.006125668063759804, 0.045924633741378784, -0.0023603576701134443, 0.0044417474418878555, -0.03785528987646103, -0.02888832427561283, 0.07248242199420929, -0.01764710061252117, 0.01636345125734806, -0.009089764207601547, -0.01260208897292614, -0.0699215680360794, 0.04939093068242073, -0.049276869744062424, 0.018403377383947372, 0.04166973754763603, 0.022653736174106598, -0.01872888207435608, 0.004595861304551363, -0.028240427374839783, 0.025671008974313736, -0.010228962637484074, 0.009100358001887798, -0.01751045510172844, -0.006390134803950787, -0.04991744086146355, 0.02676028199493885, -0.02139105089008808, 0.0237513929605484, -0.007760380860418081, -0.01240578293800354, 0.012251987121999264, -0.02551157772541046, 0.012611127458512783, 0.01596999168395996, -0.0056735980324447155, 0.0259100254625082, 0.04651257023215294, 0.014530091546475887, 0.04694651812314987, -0.008086511865258217, -0.01206056959927082, -0.006841587834060192, -0.0031253325287252665, -0.01258891075849533, 0.028397614136338234, 0.022884070873260498, -0.010110538452863693, 0.021623076871037483, 0.01960090547800064, -0.026387261226773262, -0.020768040791153908, -0.01673515886068344, 0.018025312572717667, 0.07668846845626831, -0.02013835497200489, 0.013527434319257736, 0.0012815813533961773, -0.010812800377607346, -0.0906020924448967, 0.0034078985918313265, -0.007760270964354277, -0.034210506826639175, -0.05415833741426468, 0.03684354946017265, 0.011407298967242241, -0.0008338353363797069, -0.014595050364732742, -0.007301462814211845, 0.026558833196759224, 0.0351971834897995, -0.03769436106085777, 0.012071041390299797, 0.031212154775857925, 0.017546162009239197, -0.04119347780942917, -0.024180160835385323, 0.04173216223716736, -0.02674461156129837, -0.017632199451327324, -0.012697145342826843, -0.01174894254654646, -0.014163164421916008, 0.04466252028942108, -0.029253628104925156, 0.03792193531990051, -0.013032951392233372, -0.010977736674249172, 0.040351904928684235, -0.0025226350408047438, 0.021815696731209755, 0.026651853695511818, 0.024136733263731003, -0.01785559393465519, 0.025526242330670357, 7.48740421840921e-05, 0.03110794723033905, 0.047792769968509674, -0.0072709727101027966, 0.04148256033658981, 0.018899250775575638, -0.015351680107414722, 0.024599958211183548, 0.0069521451368927956, 0.053959205746650696, 0.036542825400829315, 0.0220382921397686, 0.015144610777497292, 0.009787177667021751, -0.006821161136031151, -0.032191429287195206, 0.012849373742938042, -0.003476417623460293, -0.06695134937763214, 0.035561345517635345, -0.008504018187522888, -0.008413119241595268, -0.01975996419787407, -0.005239875987172127, -0.0010363784385845065, 0.021018363535404205, -0.009702148847281933, -0.026879504323005676, -0.030758989974856377, 0.04245585575699806, -0.005286714527755976, -0.01277631614357233, -0.0171899925917387, 0.023350970819592476, -0.009204021655023098, -0.004905526991933584, 0.016759691759943962, -0.04598098620772362, -0.050887495279312134, -0.035177234560251236, -0.07874347269535065, 0.024897364899516106, -0.016976559534668922, -0.016867514699697495, -0.057870425283908844, -0.02800140343606472, -0.013512859120965004, 3.475390622043051e-05, 0.04083515331149101, -0.00033485741005279124, 0.005842601414769888, 0.004995848052203655, 0.00944422371685505, 0.012189441360533237, -0.027000363916158676, -0.0028062183409929276, -0.0003521227336023003, 0.04900768771767616, -0.002313295379281044, 0.02133648470044136, -0.021600494161248207, -0.028638798743486404, 0.041459184139966965, -0.005176544655114412, 0.023372020572423935, -0.023479903116822243, -0.0015881126746535301, -0.003339287592098117, -0.08467496186494827, -0.06410510092973709, 0.013119462877511978, -0.00497355405241251, -0.044056039303541183, 0.024047892540693283, -0.024637341499328613, -0.005924116354435682, 0.03399995341897011, -0.009876835159957409, -0.024203406646847725, 0.005688013043254614, 0.03227749094367027, 0.009004613384604454, -0.02627602033317089, 0.04074573889374733, 0.008226857520639896, -0.038069259375333786, 0.0227205790579319, -0.034688420593738556, -0.008981842547655106, -0.01299032848328352, -0.0026797971222549677, -0.043295156210660934, -0.018490588292479515, 0.046375565230846405, -0.01730954460799694, -0.09770751744508743, -0.032099686563014984, 0.010449601337313652, -0.03247780352830887, -0.01973496749997139, -0.03429728373885155, 0.021298255771398544, 0.02595469169318676, 0.013953781686723232, -0.060049932450056076, 0.021562546491622925, 0.008150842972099781, 0.033839061856269836, -0.0038459175266325474, -0.03956267237663269, 0.02293122000992298, 0.026916436851024628, -0.003740008920431137, -0.02133176662027836, 0.026096930727362633, -0.037937868386507034, -0.005571753717958927, -0.03693568333983421, -0.019579360261559486, 0.03727263584733009, 0.02721400558948517, 0.04762167111039162, 0.005886667408049107, 0.026532679796218872, -0.05534062534570694, -0.02487678825855255, -0.014852330088615417, 0.01603204570710659, 0.034792397171258926, 0.03128280118107796, 0.0005431990139186382, -0.0011895501520484686, -0.0010623936541378498, -0.011174413375556469, -0.0031408979557454586, 0.008863142691552639, 0.06523982435464859, -0.05302678793668747, 0.031032081693410873, 0.04249290004372597, -0.02026956155896187, 0.025957878679037094, -0.005483147222548723, -0.05012861639261246, 0.05569376423954964, 0.02089666947722435, 0.039325132966041565, -0.01385524682700634, 0.005536630284041166, -0.014241017401218414, 0.022293822839856148, 0.04037456214427948, -0.0028886478394269943, 0.031294580549001694, 0.007178078405559063, -0.017751270905137062, -0.02381688542664051, -0.046046964824199677, 0.0016532118897885084, 0.03144652396440506, -0.038396451622247696, 0.0026002521626651287, -0.049636151641607285, -0.013540724292397499, 0.05153331905603409, 0.060396354645490646, 0.02078278735280037, 0.015241347253322601, 0.059246379882097244, -0.015052687376737595, 0.025114908814430237, 0.03294973447918892, -0.0021631603594869375, 0.0205075703561306, -0.02541629783809185, 0.058281734585762024, -0.028864247724413872, 0.020767850801348686, -0.023602543398737907, -0.007601195015013218, -0.01294951792806387, 0.013233686797320843, -0.0029722037725150585, 0.029159577563405037, -0.013943337835371494, -0.038897983729839325, -0.010182109661400318, 0.01073235273361206, -0.050326596945524216, -0.025469860062003136, -0.04626956954598427, 0.020183585584163666, 0.06150808930397034, -0.014696468599140644, -0.031286172568798065, -0.023545891046524048, 0.03163893520832062, -0.022193437442183495, -0.05022479221224785, -0.014853534288704395, -0.002538834698498249, -0.015635646879673004, -0.048808421939611435, 0.036381423473358154, 0.02157253958284855, 0.010303348302841187, -0.00017238501459360123, -0.02558983489871025, -0.012947372160851955, -0.024084214121103287, -0.027805978432297707, 0.00031871639657765627, 0.01634000986814499, -0.010607189498841763, 0.04893225058913231, 0.026154249906539917, 0.015413442626595497, 0.008835074491798878, -0.008950532414019108, -0.012018253095448017, 0.034705210477113724, -0.058708690106868744, 0.01869816705584526, 0.019733935594558716, -0.017927872017025948, -0.02881575934588909, -0.009176754392683506, -0.011660447344183922, -0.02726730704307556, -0.0033484199084341526, 0.04343784600496292, 0.008175310678780079, -0.04946700111031532, 0.04360748082399368, 0.01656504161655903, -0.008614074438810349, 0.02831987291574478, -0.0010605842107906938, -0.004477237351238728, 0.0064573632553219795, 0.005981322377920151, 0.005562096834182739, -0.010914288461208344, -0.036615852266550064, 0.0035136898513883352, -0.007645265199244022, -0.0022815631236881018, 0.00047219335101544857, -0.00297843711450696, 0.020238330587744713, -0.054924264550209045, -0.03535735607147217, -0.002988900989294052, -0.030330119654536247, -0.014765993691980839, 0.016178159043192863, -0.05885344743728638, 0.030248230323195457, 0.0010673246579244733, -0.02452685497701168, 0.03902832418680191, -0.061585426330566406, -0.022727321833372116, -0.05032993480563164, 0.005497372709214687, -0.004796385299414396, 0.005657569970935583, -0.0018990712705999613, 0.03004406951367855, -0.019108092412352562, -0.031075486913323402, 0.00010984754771925509, -0.031963154673576355, -0.004889728967100382, -0.03890034556388855, 0.023835403844714165, 0.04743262752890587, -0.013360561802983284, 0.00475692143663764, -0.0031503820791840553, 0.004143015947192907, 0.00192188227083534, 0.017172161489725113, -0.033968083560466766, 0.025818027555942535, 0.004569150507450104, -0.016799112781882286, 0.06630364060401917, 0.0009089692030102015, -0.06563252210617065, -0.03630317747592926, 0.022714000195264816, 0.002332742093130946, 0.006916401907801628, 0.05083201453089714, -0.035126060247421265, -0.02972598746418953, -0.04456162825226784, 0.005522538907825947, -0.02130121737718582, -0.028858663514256477, -0.0037036039866507053, -0.057218875735998154, 2.3400165446219034e-05, 0.02422858029603958, 0.04158327728509903, -0.020098550245165825, 0.0070378901436924934, -0.08675478398799896, 0.01694081723690033, -0.0331503301858902, 0.004112359136343002, -0.028780074790120125, 0.0016496814787387848, -0.026885565370321274, 0.01859448477625847, 0.008290519006550312, -0.008366704918444157, -0.01553630642592907, -0.009067260660231113, 0.052116330713033676, 0.04542768746614456, -0.024383720010519028, -0.0012381651904433966, 0.018930817022919655, -0.003936214372515678, -0.0018353384220972657, 0.015911629423499107, -0.04640309512615204, 0.05574759840965271, -0.024080704897642136, 0.04143388569355011, 0.005276891868561506, -0.02466464228928089, -0.028475888073444366, -0.012720725499093533, 0.017310524359345436, -0.04039176553487778, -0.011290451511740685, -0.045726362615823746, 0.02483712136745453, -0.009804978035390377, -0.007306735496968031, -0.0536208413541317, -0.006652659736573696, -0.028987763449549675, -0.014196277596056461, -0.022863825783133507, -0.02032894641160965, -0.017668502405285835, 0.0043655941262841225, 0.0040648384019732475, 0.05725638195872307, -0.017483633011579514, 0.024438386783003807, 0.044229816645383835, 0.010176957584917545, -0.009728978388011456, -0.024940069764852524, 0.008204173296689987, -0.013612729497253895, -0.0022579701617360115, -0.027518566697835922, 0.014378084801137447, -0.027561955153942108, 0.006162339821457863, -0.002439308911561966, -0.032761164009571075, -0.0024355859495699406, 0.04881887882947922, 0.04470908269286156, -0.014788965694606304, 0.01853889226913452, -0.023075539618730545, -0.015640027821063995, -0.0019962387159466743, 0.01183378603309393, -0.001094234292395413, -0.011397555470466614, 0.012004579417407513, -0.04352675750851631, 0.06117779389023781, 0.015727542340755463, -0.01912003569304943, -0.02485797554254532, -0.012140165083110332, 0.07849340885877609, 0.0013113593449816108, -0.07649002224206924, 0.045745231211185455, 0.01783578284084797, -0.036475587636232376, -0.05654988810420036, 0.019065523520112038, -0.026506658643484116, -0.02746419422328472, 0.0037437898572534323, 0.01509768608957529, 0.01894008368253708, 0.017236096784472466, 0.009045050479471684, 0.020691530779004097, -0.01666557788848877, -0.011444862931966782, -0.004531700164079666, -0.00044906523544341326, -0.009011797606945038, 0.030024131760001183, 0.028646934777498245, 0.012156005017459393, 0.019092774018645287, 0.0042975605465471745, 0.02706683799624443, -0.010376996360719204, 0.01878380961716175, 0.046555787324905396, 0.023047465831041336, 0.031784772872924805, 0.03751925379037857, 0.04304404556751251, 0.0189797542989254, 0.02378176525235176, -0.023437341675162315, 0.04286975786089897, 0.0001402871566824615, -0.0374358706176281, -0.025539223104715347, 0.011707444675266743, 0.007356657180935144, 0.006810840219259262, -0.017788425087928772, 0.015050974674522877, 0.049579620361328125, 0.032013267278671265, 0.032102104276418686, -0.023517657071352005, -0.008082938380539417, -0.03746534883975983, 0.005975073669105768, -0.010465512052178383, -0.041892196983098984, -0.04593687504529953, -0.026299528777599335, 0.003416402731090784, -0.03568587452173233, -0.0029942886903882027, 0.03476106747984886, -0.022072207182645798, -0.006534855347126722, 0.015624457038939, -0.0036117229610681534, -0.020709548145532608, -0.04691310599446297, -0.0077370405197143555, -0.018848562613129616, 0.010317500680685043, 0.01298273541033268, 0.007734251208603382, -0.03224858641624451, -0.00016093053272925317, -3.769483737414703e-05, 0.03640807792544365, 0.006757771130651236, -0.02316683903336525, -0.02337689697742462, -0.021547524258494377, -0.021950768306851387, -0.03622809797525406, 0.07086431980133057, 0.013173402287065983, -0.0331592857837677, -0.006287279073148966, 0.07404201477766037, -0.008491047658026218, -0.008597557432949543, 0.015549516305327415, 0.06274016946554184, -0.04246325418353081, -0.014826750382781029, 0.004746224265545607, -0.00281794392503798, -0.014645161107182503, -0.022485632449388504, -0.020024539902806282, -0.0028278445824980736, 0.015940580517053604, 0.010365438647568226, -0.01586615853011608, 0.0528748445212841, 0.054702818393707275, 0.04151852801442146, 0.008798935450613499, -0.01913575455546379, -0.0293197613209486, 0.00014278525486588478, 0.04428664594888687, -0.009937582537531853, -0.010792991146445274, -0.0011708398815244436, 0.0050894333980977535, 0.053946785628795624, 0.01627366803586483, 0.008043667301535606, 0.011905918829143047, 0.01268531009554863, -0.021997032687067986, 0.043996211141347885, -0.001195439021103084, 0.03431593254208565, 0.0260970126837492, -0.07620462030172348, -0.03032994642853737, -0.017642786726355553, -0.01200807187706232, -0.03228488937020302, -0.03327040746808052, -0.02319115400314331, -0.0017264921916648746, -0.03940034285187721, 0.018201306462287903, -0.017046866938471794, -0.00605649733915925, -0.010184871032834053, 0.014483895152807236, 0.016981512308120728, -0.012087570503354073, 0.03613289073109627, 0.01870070956647396, -0.013071319088339806, -0.03673320636153221, -0.027398793026804924, 0.01986258290708065, 0.01831989921629429, 0.0055639180354774, -0.05355318635702133, 0.049083780497312546, 0.059282176196575165, 0.001101927482523024, -0.011550640687346458, -0.05178189277648926, -0.017061972990632057, 0.022404972463846207, 0.006523357704281807, -0.031866226345300674, 0.035542916506528854, -0.04579680413007736, 0.04206826910376549, -0.01457939762622118, -0.051021210849285126, -0.02734803967177868, -0.00809257198125124, 0.05490130931138992, -0.02289065532386303, -0.029903175309300423, -0.006581990979611874, 0.033911705017089844, 0.0022814807016402483, 0.03440647944808006, 0.01753132790327072, 0.03354805335402489, -0.02420155704021454, -0.036281466484069824, -0.007173077203333378, 0.0020624021999537945, 0.045187581330537796, 0.0027461473364382982, -0.012675924226641655, 0.029980534687638283, 0.022801529616117477, 0.003983892500400543, -0.014516475610435009, 0.01641901023685932, -1.2892820450360887e-05, -0.0016925005475059152, -0.025974227115511894, -0.019958915188908577, -0.009788360446691513, -0.005276544019579887, -0.016369247809052467, 0.04316140338778496, -0.02788121998310089, -0.03916110098361969, -0.038846805691719055, -0.02184780314564705, 0.02371699921786785, -0.024793632328510284, -0.06867516785860062, -0.0017681967001408339, 0.03681030869483948, 0.05481299012899399, -0.004978352691978216, 0.009514437057077885, 0.009945914149284363, 0.011375009082257748, 0.05323649197816849, -0.0052546169608831406, -0.0017845688853412867, 0.013529526069760323, 0.007434090133756399, -0.038118038326501846, 0.03766004741191864, -0.03394804522395134, 0.030214756727218628, -0.010126680135726929, -0.010222465731203556, -0.02157791517674923, 0.035315074026584625, -0.010669639334082603, -0.042922623455524445, 0.02557794563472271, -0.022784002125263214, -0.05488380789756775, -0.04830208420753479, 0.014723383821547031, -0.019694043323397636, 0.02673303335905075, 0.04066987335681915, -0.048244595527648926, -0.049233581870794296, 0.027410270646214485, -0.02802710048854351, 0.009392348118126392, 0.004949131514877081, 0.07678505778312683, 0.03140868619084358, 0.046436477452516556, 0.018010638654232025, 0.0013477331958711147, -0.010276208631694317, 0.021734951063990593, 0.02258550003170967, -0.012265808880329132, 0.006894353311508894, 0.03106559067964554, 0.011665679514408112, -0.009202830493450165, -0.05846823379397392, 0.05988016352057457, 0.033969566226005554, -0.027348993346095085, -0.004677563905715942, -0.011651727370917797, 0.0025166745763272047, -0.022212807089090347, -0.03481747582554817, 0.04833846911787987, -0.007969716563820839, 0.023000996559858322, -0.02036755532026291, -0.008910201489925385, -0.04199553281068802, -0.01346516888588667, -0.023113960400223732, 0.023303475230932236, 0.03428415581583977, -0.014008346013724804, -0.014440263621509075, -0.048308540135622025, 0.0010786071652546525, -0.02548520639538765, -0.01493887696415186, -0.026454946026206017, 0.02570156939327717, 0.0003517258446663618, 0.047726117074489594, -0.0024258531630039215, -0.005328289233148098, -0.013256312347948551, 0.05596455559134483, 0.03948800638318062, 0.03296278044581413, 0.08756116032600403, 0.07373648136854172, 0.02980373054742813, -0.03385994955897331, 0.013182999566197395, 0.009841695427894592, -0.01657652109861374, 0.022109489887952805, 0.007377728819847107, 0.014292391948401928, 0.0077238488011062145, -0.04297471046447754, -0.021368185058236122, 0.05221710354089737, 0.013808864168822765, 0.0072023263201117516, -0.015307050198316574, -0.0013111429288983345, -0.06180979683995247, 0.010217363014817238, -0.056284286081790924, 0.027273083105683327, 0.022605543956160545, 0.021180370822548866, 0.0028154843021184206, -0.04808180034160614, 0.299955815076828, 0.033508457243442535, 0.0379316583275795, 0.04377209022641182, 0.004107399843633175, 0.010987085290253162, 0.012090117670595646, -0.038715146481990814, 0.04207572340965271, -0.04981252923607826, 0.03852429613471031, -0.03965389356017113, -0.000915409647859633, 0.011274090968072414, -0.003956240136176348, 0.0389707088470459, -0.025807054713368416, -0.012229224666953087, -0.007357411552220583, -0.04190361499786377, -0.0091925747692585, 0.00893227569758892, 0.006278542801737785, 0.05902113765478134, 0.00046044684131629765, 0.005499436054378748, 0.03786541894078255, 0.01594991236925125, 0.010374801233410835, -0.05010819062590599, 0.0015176663873717189, -0.03797340393066406, 0.025869369506835938, -0.028467683121562004, -0.009723743423819542, 0.012710990384221077, -0.020678294822573662, -0.0628599226474762, -0.03203631192445755, 0.04043249040842056, -0.012088408693671227, -0.028356321156024933, 0.01665247604250908, -0.0008890103781595826, 0.0018415804952383041, 0.05431489273905754, 0.012012416496872902, -0.009135236963629723, 0.04683899134397507, -0.04474634304642677, 0.012689621187746525, -0.03794116526842117, -0.011240901425480843, -0.030199216678738594, -0.04596317932009697, -0.02206737920641899, -0.014954832382500172, -0.013817147351801395, 0.025871776044368744, 0.06527233123779297, 0.0045433975756168365, 0.026305794715881348, 0.003339690389111638, 0.012175132520496845, -0.03351842239499092, 0.03264090418815613, 0.02504570782184601, 0.025438940152525902, -0.018340202048420906, 0.007462065201252699, 0.04703846201300621, 0.012334292754530907, -0.020480899140238762, 0.030254732817411423, 0.027170605957508087, 0.05093331262469292, 0.0012888633646070957, 0.004842773079872131, 0.030977271497249603, -0.037347212433815, -0.01041815523058176, 0.024637410417199135, -0.014802754856646061, 0.015590994618833065, 0.010573913343250751, 0.06144749000668526, -0.04938312619924545, -0.027040529996156693, -0.003837576601654291, 0.017719345167279243, -0.0021347710862755775, 0.03456955775618553, -0.016926975920796394, 0.034269772469997406, 0.064332015812397], metadata={}, excluded_embed_metadata_keys=[], excluded_llm_metadata_keys=[], relationships={}, metadata_template='{key}: {value}', metadata_separator='\\n', text_resource=MediaResource(embeddings=None, data=None, text='- · Understands detailed page layout, reading order, locates figures and recovers table structures\\n- · Extracts metadata from the document, such as title, authors, references and language\\n- · Optionally applies OCR, e.g. for scanned PDFs\\n- · Can be configured to be optimal for batch-mode (i.e high throughput, low time-to-solution) or interactive mode (compromise on efficiency, low time-to-solution)\\n- · Can leverage different accelerators (GPU, MPS, etc).\\n\\n## 2 Getting Started\\n\\nTo use Docling, you can simply install the docling package from PyPI. Documentation and examples are available in our GitHub repository at github.com/DS4SD/docling. All required model assets 1 are downloaded to a local huggingface datasets cache on first use, unless you choose to pre-install the model assets in advance.\\n\\nDocling provides an easy code interface to convert PDF documents from file system, URLs or binary streams, and retrieve the output in either JSON or Markdown format. For convenience, separate methods are offered to convert single documents or batches of documents. A basic usage example is illustrated below. Further examples are available in the Doclign code repository.\\n\\n```\\nfrom docling.document_converter import DocumentConverter Large\\n```\\n\\n```\\nsource = \"https://arxiv.org/pdf/2206.01062\" # PDF path or URL converter = DocumentConverter() result = converter.convert_single(source) print(result.render_as_markdown()) # output: \"## DocLayNet: A Human -Annotated Dataset for Document -Layout Analysis [...]\"\\n```\\n\\nOptionally, you can configure custom pipeline features and runtime options, such as turning on or off features (e.g. OCR, table structure recognition), enforcing limits on the input document size, and defining the budget of CPU threads. Advanced usage examples and options are documented in the README file. Docling also provides a Dockerfile to demonstrate how to install and run it inside a container.\\n\\n## 3 Processing pipeline\\n\\nDocling implements a linear pipeline of operations, which execute sequentially on each given document (see Fig. ', path=None, url=None, mimetype=None), image_resource=None, audio_resource=None, video_resource=None, text_template='{metadata_str}\\n\\n{content}'),\n",
       " Document(id_='d5413a56-1e3a-451a-be3d-8fb5768998f8', embedding=[0.03304922208189964, -0.03271831199526787, -0.0057618445716798306, 0.00557431997731328, 0.016338765621185303, 0.016834424808621407, -0.004626399837434292, -0.012262326665222645, -0.006139120552688837, 0.07478434592485428, -0.026605606079101562, 0.007868649438023567, 0.02259919047355652, 0.004594705533236265, -0.04903210327029228, 0.005575564224272966, -0.020533472299575806, -0.03500477597117424, -0.06468190252780914, 0.012712037190794945, -0.012730931863188744, -0.0035655866377055645, -0.05869809165596962, -0.004160370212048292, -0.030775384977459908, 0.0315217487514019, 0.006291151512414217, -0.013435517437756062, 0.09208349883556366, 0.046087976545095444, -0.0217237900942564, 0.004269628319889307, 0.016103047877550125, -0.018901197239756584, -0.0012882191222161055, -0.0008038025698624551, 0.00793201383203268, -0.03370997682213783, -0.01948077417910099, -0.0008970120688900352, 0.031799279153347015, -0.03922922536730766, 0.029413525015115738, -0.055754419416189194, -0.07692060619592667, 0.0162325631827116, -0.0016856536967679858, -0.012401854619383812, -0.03843626007437706, -0.04805842414498329, -0.017304349690675735, 0.007262107450515032, -0.00469736335799098, -0.015460971742868423, -0.01507877279073, -0.004903169348835945, 0.004080332815647125, 0.008034367114305496, -0.061111580580472946, 0.004267508629709482, 0.002511110156774521, 0.006783878430724144, -0.020261134952306747, -0.05106693506240845, -0.010036444291472435, 0.006318856496363878, -0.028863953426480293, -0.0019063198706135154, 0.007733022794127464, -0.004083254840224981, -0.05634225532412529, 0.03354406729340553, -0.050106555223464966, -0.016220709308981895, -0.016561731696128845, 0.001321211108006537, 0.017877649515867233, -0.02671385370194912, -0.030184045433998108, 0.06949026137590408, 0.004956176970154047, 0.03893877938389778, -0.01610584743320942, 0.016466563567519188, 0.0092921806499362, -0.050970036536455154, 0.05107426270842552, 0.0037835873663425446, 0.03606171905994415, 0.0036620162427425385, -0.0005580902798101306, 0.03919032961130142, -0.008220172487199306, 0.014560399577021599, 0.008614795282483101, 0.051540229469537735, -0.015485514886677265, 0.03102491982281208, -0.0074450247921049595, 0.04075397178530693, 0.03243694454431534, 0.0838654413819313, -0.01754029281437397, 0.04443492740392685, -0.05642339587211609, 0.03779052942991257, 0.05287971720099449, -0.04737165942788124, -0.04676731675863266, -0.03805188089609146, -0.009410897269845009, -0.03715457767248154, -0.021159455180168152, -0.023536384105682373, -0.002315189689397812, 0.0017616919940337539, 0.04626148194074631, -0.036246903240680695, -0.016940027475357056, 0.007687756326049566, 0.03140140697360039, -0.0006426525651477277, -0.024738864973187447, -0.010676713660359383, -0.00885931309312582, -0.0268475953489542, -0.00645578745752573, 0.03954097256064415, -0.027859728783369064, -0.025829873979091644, 0.0292402021586895, -0.021108882501721382, -0.03492829203605652, 0.0561404675245285, -0.0005346793332137167, -0.026212774217128754, 0.024709157645702362, -0.027553262189030647, 0.02277958020567894, -0.017372513189911842, -0.00041155473445542157, -0.05325750261545181, 0.002952702809125185, 0.07652807980775833, 0.030115855857729912, 0.021676672622561455, -0.01808650977909565, -0.019800810143351555, -0.06838726997375488, 0.04906030371785164, -0.05982258915901184, 0.0171662624925375, 0.044601697474718094, -0.001830707537010312, -0.05326495319604874, 0.01757971942424774, -0.006584562361240387, -0.00013914069859310985, 0.017245912924408913, 0.0009400855051353574, -0.013995627872645855, -0.01727208122611046, -0.04951150715351105, 0.04522748291492462, -0.026838624849915504, 0.03289034217596054, 0.002592112636193633, -0.02914404310286045, 0.029514392837882042, -0.015085257589817047, 0.008428936824202538, 0.02810320258140564, -0.02583126351237297, 0.01853162981569767, 0.06300317496061325, 0.023230886086821556, 0.028207821771502495, 0.008644062094390392, 0.027004268020391464, 0.009439256973564625, -0.012083173729479313, 0.0014529963955283165, 0.006865647155791521, 0.008488972671329975, -0.0016622912371531129, 0.015231949277222157, 0.003179742954671383, -0.037185460329055786, -0.010010799393057823, -0.011725146323442459, -0.002516860608011484, 0.0337156280875206, -0.015983687713742256, 0.03441815450787544, -0.0047294641844928265, -0.022930506616830826, -0.04393157735466957, -0.0035543390549719334, -0.0004858481988776475, -0.023060115054249763, -0.03192267566919327, 0.026138965040445328, 0.023736845701932907, 0.014092937111854553, -0.023313045501708984, 0.027345718815922737, 0.033887576311826706, 0.049592506140470505, -0.044975679367780685, 0.010817999951541424, 0.025417540222406387, 0.0031618226785212755, -0.046915631741285324, -0.03155732527375221, 0.02363482117652893, -0.044605277478694916, -0.016413820907473564, -0.02187698520720005, 0.016664540395140648, -0.012967266142368317, 0.014320041052997112, -0.004380979109555483, 0.02719418704509735, -0.009744568727910519, -0.004421177785843611, 0.011971233412623405, -0.018689841032028198, 0.03000761568546295, 0.037229347974061966, -0.017726078629493713, 0.002632447751238942, 0.028692010790109634, 0.025401275604963303, 0.025010930374264717, 0.049994535744190216, -0.013963866978883743, 0.052580151706933975, 0.02603483572602272, -0.025998631492257118, 0.006979451514780521, -0.006063555367290974, 0.023333840072155, 0.029137443751096725, 0.007219374645501375, 0.028533414006233215, 0.023012539371848106, -0.0026263981126248837, -0.04373866319656372, -0.01280535850673914, 0.011335739865899086, -0.05483715608716011, 0.03675639629364014, -0.014778119511902332, 0.0010594483464956284, -0.014019167050719261, -0.03380940109491348, 0.003992000129073858, 0.007744889240711927, -0.029684200882911682, -0.013803670182824135, 0.018614567816257477, 0.04613593593239784, -0.0008544091251678765, -0.01041839737445116, -0.019563714042305946, 0.02179226279258728, -0.00955444946885109, 0.015418946743011475, -0.0006915812264196575, -0.0422663651406765, -0.03868764266371727, -0.03807063028216362, -0.0737072005867958, 0.0039911395870149136, -0.025255003944039345, 0.006435520015656948, -0.011532457545399666, -0.035774316638708115, -0.008214994333684444, -0.006822842638939619, 0.02132921852171421, 0.017446592450141907, 0.009412641637027264, 0.022033486515283585, 0.0008237885194830596, 0.012525495141744614, -0.06338600814342499, 0.02916456013917923, -0.005476514808833599, 0.054516423493623734, 0.002993546659126878, 0.019992155954241753, -0.024293402209877968, -0.030946362763643265, 0.022403407841920853, -0.004602516070008278, 0.03573315218091011, -0.030173227190971375, -0.01836901344358921, -0.021476859226822853, -0.05173170566558838, -0.058802664279937744, 0.021781209856271744, -0.007350122090429068, -0.06867533177137375, 0.03774328529834747, 0.0026022219099104404, -0.012642637826502323, 0.03965349867939949, 0.019206088036298752, -0.02050110697746277, 0.028670644387602806, 0.02302122674882412, 0.03092225082218647, -0.030795888975262642, 0.0631604939699173, 0.01464926078915596, -0.01786082051694393, 0.013191661797463894, -0.05051984265446663, -0.029016345739364624, -0.002801174996420741, 0.015607078559696674, -0.07599063217639923, -0.017001936212182045, 0.03323506563901901, -0.00768255302682519, -0.07323881983757019, -0.028477925807237625, -0.0014662223402410746, -0.012037036940455437, -0.006758871488273144, -0.03500659763813019, 0.03252420201897621, 0.017423126846551895, -0.0005195089033804834, -0.06595306098461151, 0.014449970796704292, 0.028514491394162178, 0.05563671514391899, 0.016431516036391258, -0.013824477791786194, 0.011186381801962852, 0.030195452272892, -0.013913406059145927, 0.0028077932074666023, 0.028094472363591194, -0.03670072928071022, 0.012534019537270069, -0.029167935252189636, -0.002527746371924877, 0.024037301540374756, 0.03089207597076893, 0.03091544285416603, 0.020086877048015594, 0.02915114350616932, -0.03680167347192764, -0.015714984387159348, -0.05266319960355759, 0.0225528571754694, 0.01013485062867403, 0.04840778931975365, 0.004456880036741495, 0.007866879925131798, -0.01772831752896309, -0.04917760565876961, -0.01971936970949173, 0.0023964950814843178, 0.04745970293879509, -0.04385407269001007, 0.019533555954694748, 0.026706358417868614, -0.018713530153036118, 0.02277883142232895, -0.04335963353514671, -0.03721928223967552, 0.06551744043827057, 0.04273644834756851, 0.05981716141104698, -0.007897443138062954, 0.0074485852383077145, 0.0019420258468016982, 0.019843634217977524, 0.03375466912984848, -0.0041935560293495655, 0.035974156111478806, 0.0024166309740394354, 0.008150281384587288, -0.006160236895084381, -0.022920507937669754, 0.01151306927204132, 0.033576417714357376, -0.03369700163602829, 0.027792980894446373, -0.0665694996714592, -0.0166206955909729, 0.04895897954702377, 0.06578367948532104, 0.01429693028330803, 0.03225491940975189, 0.055842846632003784, 0.011181174777448177, -0.009377261623740196, 0.01957634650170803, 0.007436999585479498, -0.013479900546371937, -0.054677121341228485, 0.07223927229642868, -0.03921984136104584, 0.013324622064828873, -0.018236394971609116, -0.04880889505147934, -0.01040539238601923, 0.020445644855499268, -0.034160543233156204, 0.013972818851470947, -0.03321700170636177, -0.03665846213698387, -0.0025969482958316803, 0.03513287380337715, -0.045904457569122314, -0.02564971335232258, -0.031202111393213272, 0.011815188452601433, 0.05327489599585533, 0.001681129913777113, -0.029423289000988007, 0.010285193100571632, 0.01697002910077572, -0.026830198243260384, -0.07269111275672913, -0.01613805815577507, -0.05147193372249603, -0.04103424772620201, -0.06829705834388733, 0.05388127639889717, 0.020834067836403847, -0.014133406803011894, 0.021596243605017662, -0.04942500218749046, 0.01113553810864687, 0.0042115673422813416, -0.010622930712997913, -0.02098282426595688, 0.0250589270144701, -0.005165135022252798, 0.031341686844825745, 0.030565796419978142, 0.0050866873934865, 0.015746455639600754, -0.005521262530237436, -0.03255245462059975, 0.05901769548654556, -0.04461692273616791, 0.010635953396558762, -0.005896864924579859, -0.034768953919410706, -0.002078248420730233, -0.008350263349711895, -0.0019956002943217754, 0.005479488056153059, 0.027923638001084328, 0.04262479394674301, -0.006234027910977602, -0.05472135543823242, 0.03477425500750542, 0.010192138142883778, -0.008195661939680576, 0.009393041022121906, 0.02498205564916134, -0.01299436017870903, -0.0006122117629274726, -0.0037778255064040422, -0.02204785868525505, 0.01777707040309906, -0.035047248005867004, -0.012257060036063194, -0.022645751014351845, -0.04399189352989197, -0.009507310576736927, -0.005863368511199951, 0.04604002460837364, -0.02729600854218006, -0.03950422629714012, -0.011581101454794407, -0.07424964010715485, -0.016662968322634697, 0.025911124423146248, -0.04294507950544357, 0.02859857678413391, 0.015109073370695114, -0.027026213705539703, 0.023989589884877205, -0.04726474732160568, -0.018507743254303932, -0.04203037917613983, 0.011875050142407417, -0.024960411712527275, 0.00465201772749424, 0.014327525161206722, 0.050009820610284805, -0.014650119468569756, -0.045132897794246674, 0.02023432031273842, -0.019605914130806923, -0.020771490409970284, -0.04124477505683899, 0.009798974730074406, 0.01330367662012577, -0.006514457985758781, 0.01841047592461109, -0.0003655273176264018, 0.0204823836684227, -0.002591620199382305, -0.006086407694965601, -0.009383689612150192, 0.04150773212313652, -0.0104226628318429, -0.0065213702619075775, 0.0709119662642479, -0.005541312508285046, -0.06442252546548843, -0.054359324276447296, 0.04193440079689026, -0.0034302419517189264, 0.008448963984847069, 0.03714068979024887, -0.03782907873392105, -0.01723966747522354, -0.044921696186065674, -0.018943987786769867, -0.047044917941093445, -0.013970630243420601, 0.004688159096986055, -0.025076473131775856, -0.0018473786767572165, -0.0017556740203872323, 0.026343345642089844, -0.08270306885242462, -0.00021462595032062382, -0.044655777513980865, 0.04102962464094162, 0.003833819879218936, -0.004563554655760527, -0.019448764622211456, 0.0010268838377669454, -0.010379883460700512, 0.024559063836932182, 0.02119196020066738, -0.01667133904993534, -0.02162759378552437, -0.011809786781668663, 0.04015764594078064, 0.0330030657351017, -0.03467962518334389, -0.014145883731544018, 0.010891133919358253, -0.004954291507601738, 0.007099468726664782, 0.013412516564130783, -0.04876703768968582, 0.03111935779452324, -0.028444983065128326, 0.017786040902137756, 0.02473975531756878, -0.029145007953047752, -0.043154239654541016, 0.010670271702110767, 0.01771143637597561, -0.033182211220264435, -0.013701151125133038, -0.054879628121852875, 0.029906895011663437, 0.0008864745614118874, 0.008814636617898941, -0.04634693264961243, 0.011505459435284138, -0.04675104096531868, -0.01519016269594431, -0.004651336465030909, -0.031688034534454346, -0.0013715956592932343, 0.0016425895737484097, -0.005104690790176392, 0.048526979982852936, -0.02851339988410473, 0.04070470854640007, 0.06682069599628448, 0.019762642681598663, 0.014663055539131165, -0.03906703740358353, 0.018157606944441795, 0.020678864791989326, -0.024163078516721725, -0.025392843410372734, 0.015496928244829178, -0.008312917314469814, 0.012767200358211994, 0.017598610371351242, -0.021874969825148582, -0.02057936228811741, 0.03809942677617073, 0.0349748320877552, -0.008480928838253021, 0.01406980399042368, -0.009108328260481358, -0.021358240395784378, 0.0032738607842475176, 0.02367277257144451, -0.0003854756650980562, 0.004154922906309366, 0.022684942930936813, -0.029494715854525566, 0.022884422913193703, 0.029544612392783165, -0.032554756850004196, -0.003587868297472596, -0.04431234300136566, 0.08337345719337463, -0.010418814606964588, -0.05844515189528465, 0.06740541011095047, 0.028643516823649406, 0.00986412726342678, -0.05134541913866997, 0.038481809198856354, -0.004416299052536488, -0.015019763261079788, -0.028000537306070328, 0.03102394938468933, -0.008035149425268173, -0.009198540821671486, -0.011976255103945732, 0.039426177740097046, -0.02475978061556816, -0.0004538365174084902, 0.01271446980535984, -0.025485992431640625, 0.0029216459952294827, 0.031438834965229034, 0.03709198907017708, -0.01189251895993948, 0.005668420344591141, -0.03674278408288956, 0.0178749468177557, -0.019404536113142967, -0.023776385933160782, 0.05037766695022583, 0.027540983632206917, 0.028683241456747055, 0.02069011516869068, 0.008939514867961407, 0.03494507074356079, 0.013113412074744701, -0.013101805001497269, 0.025299256667494774, -0.023988382890820503, -0.04867789149284363, -0.0184464193880558, 0.017439542338252068, 0.0020599281415343285, 0.014464537613093853, -0.021792730316519737, 0.02907070890069008, 0.024491822347044945, 0.03731248527765274, 0.020585771650075912, -0.06126002222299576, 0.006513142492622137, -0.01528945378959179, 0.00995134748518467, -0.015657728537917137, -0.0413115993142128, -0.047273971140384674, 0.013278868049383163, -0.023983696475625038, -0.014439309947192669, -0.003995911683887243, -0.0016586242709308863, -0.03016500361263752, -0.02781900390982628, -0.017489276826381683, 0.013197986409068108, -0.001480251899920404, -0.03239946812391281, -0.013004953041672707, -0.036014728248119354, 0.007235811557620764, 0.027746224775910378, -0.005653597414493561, -0.024237584322690964, -0.017620187252759933, 0.020538853481411934, 0.01323294173926115, -0.005921395029872656, -0.021158529445528984, -0.018766287714242935, -0.037415746599435806, -0.006255893502384424, -0.03210162743926048, 0.0679108127951622, 0.02394770458340645, -0.04673188552260399, -0.009767808020114899, 0.019583212211728096, -0.02159040793776512, -0.013850483112037182, 0.009267574176192284, 0.043953824788331985, -0.04243363440036774, 0.013301808387041092, 0.003228959394618869, 0.009290417656302452, -0.020393576472997665, -0.04057484120130539, -0.02337447553873062, 0.03409688174724579, 0.04516112059354782, 0.009132364764809608, -0.019241057336330414, 0.04417963698506355, 0.032017383724451065, 0.04759732261300087, 0.004965462256222963, -0.006576853338629007, -0.00840703397989273, 0.012717161327600479, 0.04399006441235542, -0.03566711023449898, 0.017533496022224426, -0.05104896426200867, 0.025786252692341805, 0.040760986506938934, 0.027140852063894272, 0.018761591985821724, 0.0051667834632098675, -0.0124489925801754, -0.035653602331876755, 0.005240343511104584, -0.017587576061487198, 0.039014458656311035, 0.02574615553021431, -0.04232373461127281, -0.047887131571769714, 0.018523165956139565, -0.003992021549493074, -0.0032628437038511038, -0.03518481180071831, -0.05400504916906357, 0.014111801981925964, -0.027925748378038406, 0.028198642656207085, -0.01422855630517006, -0.0005506818415597081, -0.0005909904721193016, 0.02792476676404476, 0.03566475957632065, 0.01860259845852852, 0.02753230184316635, 0.019169626757502556, -0.011132747866213322, -0.023796873167157173, -0.019077418372035027, 0.003975292202085257, 0.026328209787607193, -0.026020154356956482, -0.031169945374131203, 0.0461714006960392, 0.042892731726169586, 0.009448080323636532, 0.0011206009658053517, -0.062085773795843124, -0.012351551093161106, 0.003489346941933036, 0.009232540614902973, -0.024913709610700607, 0.04265134409070015, -0.06898751109838486, 0.03302229568362236, -0.03170179948210716, -0.025372719392180443, -0.029711086302995682, 0.01453070342540741, 0.0502745658159256, -0.035884466022253036, -0.03775235265493393, 0.008201171644032001, 0.06816893815994263, -0.021594690158963203, 0.017563246190547943, 0.0014920160174369812, 0.00393090071156621, -0.0038317572325468063, -0.01055710855871439, -0.02423206903040409, 0.0179212037473917, 0.04437839239835739, -0.0009417872061021626, 0.00080452038673684, 0.07473421841859818, 0.05104880779981613, 0.0006061900639906526, -0.02335064858198166, 0.03086877055466175, 0.01643470861017704, -0.004159805830568075, -0.007739720866084099, -0.030827073380351067, -0.01593310758471489, -0.0018804884748533368, -0.02633928507566452, 0.02793712541460991, -0.012372424826025963, 0.0009540432365611196, -0.03991537168622017, -0.04763449355959892, 0.024203648790717125, -0.001939222915098071, -0.049996268004179, -0.03231181204319, -0.0019139568321406841, 0.04571868106722832, 0.006041612941771746, 0.01544408593326807, 0.020777825266122818, 0.023508960381150246, 0.04041139781475067, -0.001938611501827836, 0.028080608695745468, 0.0018754105549305677, 0.03291141614317894, -0.018176211044192314, 0.016125516965985298, -0.05256461352109909, 0.0231449156999588, 0.004217972978949547, -0.02799636498093605, -0.03435365483164787, 0.03755960986018181, -0.022766707465052605, -0.04886989668011665, 0.002758400747552514, -0.00494610657915473, -0.023610040545463562, -0.03120400384068489, 0.028194205835461617, -0.05311073735356331, 0.027318526059389114, 0.04313575103878975, -0.03950344771146774, -0.06887859106063843, 0.02969801239669323, -0.006891183089464903, 0.021251486614346504, 0.016529982909560204, 0.05067971721291542, 0.008093508891761303, 0.08093622326850891, 0.01895374432206154, 0.004470533225685358, 0.020797640085220337, 0.0065727513283491135, 0.015219862572848797, -0.02589038386940956, -0.0022792539093643427, 0.0045484816655516624, -0.0024974809493869543, -0.008261860348284245, -0.03744097054004669, 0.048225581645965576, 0.007268545217812061, -0.0108255036175251, -0.006542092654854059, 0.03145342320203781, 0.01903476193547249, 0.008717420510947704, -0.0010178597876802087, 0.043084967881441116, 0.0049745249561965466, -0.011756092309951782, -0.01163910236209631, -0.01814255118370056, -0.040899407118558884, -0.02640523388981819, -0.006567999720573425, 0.029429687187075615, -0.009216499514877796, -0.02587807923555374, 0.0003362098941579461, -0.020947132259607315, -0.021909823641180992, -0.035273227840662, -0.013935437425971031, -0.04562444984912872, 0.017091434448957443, 0.002266809344291687, 0.07220127433538437, 0.011912878602743149, -0.0101789440959692, 0.0034968876279890537, 0.047352902591228485, 0.014878842048346996, 0.021257566288113594, 0.05565890297293663, 0.07692862302064896, 0.005336705595254898, -0.020939843729138374, -0.002152869012206793, 0.030021969228982925, -0.01376426499336958, -0.0028916869778186083, -0.025035353377461433, 0.011780169792473316, -0.0005759749328717589, -0.06742265820503235, -0.002995978109538555, 0.06254012882709503, 0.007802499923855066, 0.002890694886445999, -0.02935446798801422, -0.02009233832359314, -0.05049237608909607, 0.018723109737038612, -0.048058174550533295, -0.0003022900491487235, 0.015456435270607471, 0.010849082842469215, -0.007772283162921667, -0.05126693472266197, 0.28001493215560913, 0.04549673944711685, 0.010083463042974472, 0.05351194739341736, 0.053027741611003876, 0.03941873461008072, 0.01512350793927908, -0.03732689097523689, 0.04788587614893913, -0.032086946070194244, 0.050517115741968155, -0.03717614710330963, -0.008716514334082603, 0.00798492506146431, -0.02374175190925598, 0.03861638158559799, -0.018479730933904648, -0.029393985867500305, -0.0195020642131567, -0.04979736730456352, -0.01615569181740284, 0.016128670424222946, 0.006893686950206757, 0.07614206522703171, 0.007625927217304707, -0.0016918278997763991, 0.006829101592302322, -0.008183557540178299, -0.0031006624922156334, -0.0383162684738636, 0.01069427840411663, -0.012620710767805576, 0.02996158041059971, -0.0038042571395635605, -0.02266242355108261, -0.01098397746682167, 0.0029743460472673178, -0.05107954517006874, -0.03926226869225502, 0.06433898210525513, -0.005002846475690603, -0.025854479521512985, 0.026682276278734207, -0.017939064651727676, -0.006457149051129818, 0.0631495863199234, 0.0041284202598035336, 0.011312605813145638, 0.035375144332647324, -0.05108102411031723, 0.025392381474375725, -0.04641559720039368, -0.014847392216324806, -0.029579972848296165, -0.04307451471686363, -0.019100328907370567, -0.00068374426336959, -0.0023873092141002417, 0.007557487580925226, 0.032825227826833725, 0.017125118523836136, 0.015261017717421055, 0.007313367910683155, 0.0197908915579319, -0.03592785447835922, 0.03925756365060806, 0.01123724039644003, 0.016270479187369347, -0.03108886629343033, 0.001842903671786189, 0.022219331935048103, -0.0019842907786369324, -0.019665859639644623, -0.0002816470223478973, 0.0005236893775872886, 0.059802934527397156, 0.0025740074925124645, 0.0233809445053339, 0.023974303156137466, -0.049354325979948044, -0.011637662537395954, 0.012885858304798603, -0.02848353236913681, -0.018468668684363365, 0.015213758684694767, 0.06343226134777069, -0.020913926884531975, -0.022379688918590546, -0.0014400386717170477, 0.006680857390165329, 0.0161232128739357, 0.05412760376930237, -0.018966514617204666, 0.023784231394529343, 0.0448627732694149], metadata={}, excluded_embed_metadata_keys=[], excluded_llm_metadata_keys=[], relationships={}, metadata_template='{key}: {value}', metadata_separator='\\n', text_resource=MediaResource(embeddings=None, data=None, text=\"1). Each document is first parsed by a PDF backend, which retrieves the programmatic text tokens, consisting of string content and its coordinates on the page, and also renders a bitmap image of each page to support downstream operations. Then, the standard model pipeline applies a sequence of AI models independently on every page in the document to extract features and content, such as layout and table structures. Finally, the results from all pages are aggregated and passed through a post-processing stage, which augments metadata, detects the document language, infers reading-order and eventually assembles a typed document object which can be serialized to JSON or Markdown.\\n\\n## 3.1 PDF backends\\n\\nTwo basic requirements to process PDF documents in our pipeline are a) to retrieve all text content and their geometric coordinates on each page and b) to render the visual representation of each page as it would appear in a PDF viewer. Both these requirements are encapsulated in Docling's PDF backend interface. While there are several open-source PDF parsing libraries available for python, we faced major obstacles with all of them for different reasons, among which were restrictive\\n\\n1 see huggingface.co/ds4sd/docling-models/\\n\\nFigure 1: Sketch of Docling's default processing pipeline. The inner part of the model pipeline is easily customizable and extensible.\\n\\n<!-- image -->\\n\\nlicensing (e.g. pymupdf [7]), poor speed or unrecoverable quality issues, such as merged text cells across far-apart text tokens or table columns (pypdfium, PyPDF) [15, 14].\\n\\nWe therefore decided to provide multiple backend choices, and additionally open-source a custombuilt PDF parser, which is based on the low-level qpdf [4] library. It is made available in a separate package named docling-parse and powers the default PDF backend in Docling. As an alternative, we provide a PDF backend relying on pypdfium , which may be a safe backup choice in certain cases, e.g. if issues are seen with particular font encodings.\\n\\n## 3.2 AI models\\n\", path=None, url=None, mimetype=None), image_resource=None, audio_resource=None, video_resource=None, text_template='{metadata_str}\\n\\n{content}'),\n",
       " Document(id_='cf7ff5df-d03e-4a27-bb10-da3a7df21e11', embedding=[0.03659595921635628, -0.040719859302043915, -0.004959502257406712, -0.009316511452198029, 0.010904936119914055, -0.0051091499626636505, 0.0027136970311403275, -0.0008559076231904328, 0.0023515967186540365, 0.052627164870500565, 0.009739024564623833, 0.009838114492595196, -0.010462665930390358, -0.003331907093524933, -0.01499672420322895, 0.020504308864474297, 0.005383057985454798, -0.013347608037292957, -0.05991164222359657, -0.0011776684550568461, 0.006821279879659414, 0.021206475794315338, -0.04986299201846123, -0.022265005856752396, -0.029066026210784912, 0.040893614292144775, 0.010176452808082104, -0.005126918200403452, 0.08547761291265488, 0.03764825686812401, -0.019230488687753677, -0.03610571473836899, 0.015624714083969593, -0.0025552953593432903, -0.02943308651447296, -0.022134635597467422, 0.01522761583328247, -0.02871355600655079, -0.013805549591779709, -0.059309374541044235, 0.015494735911488533, -0.0161469466984272, 0.009793968871235847, -0.07701339572668076, -0.058438509702682495, 0.010653321631252766, 0.024781987071037292, -0.032845865935087204, -0.03151554986834526, -0.03264831751585007, -0.011047516949474812, -0.0039049098268151283, -0.005067943595349789, -0.015941239893436432, -0.010027792304754257, -0.0109058553352952, 0.003959018271416426, 0.008852707222104073, -0.04179893061518669, -0.0033277373295277357, 0.037826210260391235, -0.007252510171383619, 0.007651318330317736, -0.05961171165108681, -0.0064901309087872505, 0.009779924526810646, 0.0023207752965390682, -0.0324380062520504, -0.0020343621727079153, 0.01048134546726942, -0.06368036568164825, 0.007334066554903984, -0.04117211326956749, -0.0016705048037692904, -0.04134533554315567, -0.03463499993085861, 0.007990334182977676, -0.0028940581250935793, -0.027863746508955956, 0.05988544225692749, 0.004872642457485199, 0.005947008728981018, -0.022930053994059563, 0.009707518853247166, -0.019880617037415504, -0.07249140739440918, 0.0635317862033844, 0.021536894142627716, 0.030560119077563286, -0.008751104585826397, -0.014613745734095573, 0.023506347090005875, -0.0028218564111739397, 0.03534883260726929, 0.013396584428846836, 0.04122008755803108, 0.005388231016695499, 0.04658929631114006, 0.014256644062697887, 0.023008786141872406, 0.02432052604854107, 0.07691565901041031, -0.030751777812838554, 0.04388901963829994, -0.0794990211725235, -0.0032019848003983498, 0.04532921686768532, -0.05614093318581581, -0.017824824899435043, -0.05899614095687866, -0.005782651714980602, -0.011390228755772114, -0.00358886132016778, 0.01957859843969345, 0.0021459050476551056, 0.014075993560254574, 0.005474073346704245, -0.007849623449146748, -0.0033420780673623085, 0.012293117120862007, 0.0011879745870828629, -0.004134887829422951, -0.026487981900572777, -0.02284391038119793, -0.003229070222005248, 0.01245825830847025, 0.025360841304063797, 0.04349958151578903, -0.03277551755309105, 0.0017136610113084316, 0.011474264785647392, -0.02758754789829254, -0.05335995927453041, 0.04208198934793472, -0.011942089535295963, -0.037708256393671036, 0.0026713002007454634, 0.006564322393387556, 0.01950754225254059, -0.022232579067349434, 0.003714067628607154, -0.034130338579416275, -0.018956853076815605, 0.07880804687738419, -0.01816607639193535, 0.004354169126600027, -0.026953697204589844, -0.005544517654925585, -0.0670732781291008, 0.01046823151409626, 0.00998331792652607, 0.01620909757912159, 0.024191631004214287, 0.024559438228607178, -0.05727163702249527, 0.0015188127290457487, -0.0072434647008776665, 0.002207790035754442, 0.010418140329420567, 0.007318187039345503, -0.015586567111313343, -0.014693289995193481, -0.04205392673611641, 0.00724880350753665, -0.03791441395878792, 0.05688478425145149, -0.02268996648490429, -0.027664322406053543, -0.0007678914116695523, -0.03323105350136757, -0.0030023360159248114, 0.03771097585558891, -0.02165968157351017, 0.03908221051096916, 0.045606132596731186, 0.03635299950838089, 0.0536983497440815, 0.002388041466474533, 0.012924190610647202, -0.0130761144682765, -0.012171190232038498, -0.02324414998292923, 0.0003533376439008862, 0.01656707562506199, 0.006578477565199137, 0.03744089603424072, 0.01881796307861805, -0.012222964316606522, -0.0006839841371402144, -0.004844100680202246, 0.033321212977170944, 0.06069707125425339, -0.027598068118095398, 0.023058798164129257, 0.011761950328946114, -0.019058573991060257, -0.06459171324968338, 0.029761631041765213, 0.01809842512011528, -0.04897964745759964, -0.041832007467746735, 0.0512901209294796, 0.030425457283854485, -0.015688102692365646, -0.06084439530968666, 0.018255513161420822, 0.025481775403022766, 0.04353433847427368, -0.07503174990415573, 0.00477986317127943, 0.020829200744628906, 0.01768631488084793, -0.008914797566831112, -0.00017352341092191637, 0.04500781372189522, -0.028071679174900055, 0.006554624065756798, -0.028338003903627396, 0.0040326532907783985, 0.0025654458440840244, 0.010991104878485203, 0.00777657562866807, 0.019844677299261093, -0.026979506015777588, -0.020548062399029732, 0.013307321816682816, 0.05117114633321762, 0.013525114394724369, 0.0322631299495697, 0.01891034096479416, -0.03453045338392258, 0.01685227081179619, -0.01673288643360138, 0.03745965659618378, 0.05349436402320862, -0.0169543270021677, 0.05101735144853592, 0.010471205227077007, 0.014501151628792286, 0.021081149578094482, 0.005900308955460787, 0.014528090134263039, 0.010908393189311028, 0.05872209742665291, 0.008480994030833244, 0.021594859659671783, -0.017447924241423607, 0.012525380589067936, -0.025102313607931137, -0.005551094189286232, -0.04958339408040047, 0.06584610790014267, 0.0035174728836864233, -0.0015536878490820527, -0.022959567606449127, -0.016235684975981712, 0.0037153002340346575, 0.052158527076244354, -0.020218711346387863, -0.04363388195633888, -0.018607376143336296, 0.031768716871738434, -0.005204970482736826, -0.022402575239539146, 0.014290586113929749, 0.04376190900802612, 0.013258651830255985, -0.005767442286014557, -0.003554590279236436, -0.0450061671435833, -0.031200846657156944, -0.04606873169541359, -0.10809659212827682, 0.020770972594618797, -0.04579745605587959, -0.02782302163541317, -0.017322704195976257, -0.014476025477051735, 0.017749594524502754, -0.015894921496510506, 0.014959979802370071, 0.023317312821745872, -0.03703147917985916, 0.030752822756767273, 0.002704307669773698, 0.024431316182017326, -0.061915550380945206, 0.04554470628499985, -0.024898160248994827, 0.01981063187122345, -0.0364191010594368, -0.0067316810600459576, -0.029979867860674858, 0.018599234521389008, 0.031258709728717804, -0.005648930557072163, -0.009436415508389473, 0.001483284286223352, -0.012878203764557838, -0.019693711772561073, -0.05323900654911995, -0.04337577149271965, 0.017670396715402603, -0.0009508028742857277, -0.06417275220155716, 0.04165137931704521, -0.0062988135032355785, -0.031048636883497238, 0.04262002930045128, 0.009201586246490479, -0.022968146950006485, 0.023159272968769073, 0.017309531569480896, 0.042806901037693024, -0.03685346990823746, 0.058083176612854004, 0.01696837693452835, -0.014941450208425522, 0.0074805766344070435, -0.031268585473299026, -0.04220964387059212, 0.0037465544883161783, -0.0044006467796862125, -0.053875163197517395, -0.014850585721433163, 0.036478087306022644, -0.009128479287028313, -0.09669522196054459, -0.016327515244483948, -0.012751379981637001, -0.07113730162382126, -0.01601395197212696, -0.014071100391447544, 0.03343924880027771, -0.008522646501660347, 0.01249075960367918, -0.02850683405995369, 0.009708676487207413, -0.004009320866316557, 0.016340550035238266, 0.003178891260176897, -0.05238027125597, 0.01783086359500885, 0.06734663248062134, -0.007729356177151203, -0.009955610148608685, 0.005017935298383236, -0.03489590436220169, 0.008916798047721386, -0.0014954630751162767, -0.004269625525921583, 0.03697303682565689, 0.010736566968262196, 0.003929935395717621, 0.013412867672741413, 0.006872779224067926, -0.05301510542631149, -0.01412369403988123, -0.0011440978851169348, -0.003885669866576791, 0.02374776266515255, 0.03526391461491585, -0.0022091588471084833, -0.00020895528723485768, -0.009625005535781384, -0.02132800780236721, 0.007337035145610571, 0.013098538853228092, 0.05205429717898369, -0.06514917314052582, 0.039180632680654526, 0.02700892463326454, -0.015073713846504688, 0.022745035588741302, -0.041251663118600845, -0.0631016418337822, 0.041588861495256424, 0.015800300985574722, 0.06753972917795181, -0.020971812307834625, 0.0053838398307561874, 0.004737589508295059, -0.008180477656424046, 0.04122191295027733, -0.006584268528968096, 0.04010875150561333, -0.012336373329162598, -0.02176995575428009, -0.01589735969901085, -0.009593049064278603, -0.014460460282862186, 0.03209603577852249, -0.0529952310025692, 0.00923294760286808, -0.05188491940498352, -0.035998083651065826, 0.026112360879778862, 0.07111714780330658, 0.021501848474144936, 0.002001487882807851, 0.07601241022348404, -0.02824564278125763, 0.037230752408504486, 0.035951219499111176, 0.024465449154376984, 0.012141400948166847, -0.06321187317371368, 0.014399764128029346, -0.03287220001220703, -0.008837459608912468, -0.010369292460381985, -0.010286439210176468, -0.0127260098233819, 0.025969209149479866, 0.003703516675159335, 0.011471258476376534, -0.035826776176691055, -0.03337233141064644, -0.018790123984217644, 0.02683509886264801, -0.030859312042593956, -0.021466681733727455, -0.03918459266424179, -0.006947170943021774, 0.03258199244737625, -0.03600635379552841, -0.019562378525733948, -0.012758001685142517, 0.032154493033885956, 0.011935267597436905, -0.05957761034369469, -0.02098914422094822, -0.008188007399439812, -0.02976268343627453, -0.0340811088681221, 0.06183034926652908, 0.042280033230781555, 0.0007008726242929697, 0.02072964422404766, -0.022899530827999115, 0.011282463558018208, 0.006381759885698557, -0.012486064806580544, -0.0067778052762150764, 0.016236159950494766, -0.006560586858540773, 0.0114634670317173, 0.06236615404486656, 0.00856044888496399, -0.0042759389616549015, 0.019643530249595642, -0.0324363149702549, 0.02296445704996586, -0.054686289280653, 0.0021894872188568115, 0.008114932104945183, 0.009644254110753536, -0.024486200883984566, -0.0012478198623284698, -0.00693014170974493, 0.001247165841050446, 0.008839738555252552, 0.018602151423692703, -0.00713268481194973, -0.04818090796470642, 0.04796642065048218, -0.014330128207802773, 0.0010308909695595503, 0.02513686753809452, -0.004583288915455341, -0.021240321919322014, -0.0136556476354599, 0.009683997370302677, -0.03969757631421089, 0.016540121287107468, -0.0045315236784517765, 0.020796531811356544, -0.042389266192913055, -0.0232393816113472, 0.010190089233219624, -0.028525998815894127, 0.0327552929520607, -0.03972763568162918, -0.021154090762138367, -0.022551845759153366, -0.028904486447572708, -0.020838789641857147, 0.006719806231558323, -0.03326040133833885, 0.020286044105887413, 0.0002248290547868237, -0.004873903468251228, 0.04401259869337082, -0.04491603747010231, -0.03439035639166832, -0.01397891715168953, -0.02299386076629162, 0.004709952976554632, 0.012279888615012169, 0.015195786021649837, 0.04532647132873535, -0.003898583585396409, -0.0260043703019619, -1.8232374543458718e-07, -0.009802921675145626, -0.020803308114409447, -0.027186783030629158, 0.010062271729111671, 0.0011264763306826353, 0.01419705431908369, -0.013576005585491657, -0.010092618875205517, 0.011081975884735584, -0.011227957904338837, 0.005005341954529285, -0.016643231734633446, 0.018108734861016273, -0.018103931099176407, -0.005445242393761873, 0.07089121639728546, 0.024555405601859093, -0.06026342883706093, -0.030696282163262367, 0.03490196913480759, -0.006647798698395491, 0.00543584069237113, 0.040748246014118195, -0.007255787029862404, -0.03386663272976875, -0.046925656497478485, -0.016539569944143295, -0.02312568761408329, -0.024306830018758774, -0.008990529924631119, -0.035192910581827164, 0.027050377801060677, 0.006643048021942377, 0.0363408625125885, -0.018728330731391907, -0.010659092105925083, -0.07724825292825699, 0.010432342998683453, -0.02126648835837841, -0.038919080048799515, -0.02338317595422268, -0.007194108795374632, -0.028134843334555626, 0.03310110792517662, 0.010734319686889648, -0.009630919434130192, -0.006842008326202631, 0.0020557306706905365, 0.03931267932057381, 0.02468012645840645, -0.027976060286164284, -0.03616839647293091, 0.016241872683167458, -0.020204870030283928, 0.005102166905999184, 0.00769638828933239, -0.03909752517938614, 0.03835152089595795, -0.031780436635017395, -0.00032804885995574296, -0.0032802647911012173, 0.00023231808154378086, -0.031242137774825096, -0.022671453654766083, 0.022271856665611267, -0.042805228382349014, -0.004481970798224211, -0.03239860385656357, 0.03772050142288208, -0.018863139674067497, -0.003168116556480527, -0.040805671364068985, -0.02509385161101818, -0.041100725531578064, -0.018785465508699417, -0.028067203238606453, -0.04864608496427536, 0.019701730459928513, -0.009234503842890263, 0.006088878493756056, 0.02598574385046959, -0.015843842178583145, 0.05001161992549896, 0.05609386786818504, 0.00956310797482729, -0.003003341145813465, -0.002596158068627119, -0.01509109977632761, -0.003978118300437927, -0.0101751908659935, -0.0029800771735608578, -0.0016839321469888091, -0.03699255362153053, 0.009218010120093822, -0.0104298684746027, -0.049460865557193756, -0.0005323616787791252, 0.02238629199564457, 0.05355182662606239, -0.023741770535707474, 0.03837550804018974, -0.014000281691551208, -0.03977646678686142, -0.026649978011846542, 0.03550044447183609, -0.010635681450366974, 0.016902143135666847, 0.013614149764180183, -0.031390201300382614, 0.006855051498860121, 0.02856473997235298, -0.01739269308745861, -0.016632771119475365, -0.009710919111967087, 0.06740257143974304, 0.00012754087219946086, -0.046628762036561966, 0.037766050547361374, 0.04841901734471321, -0.004862697795033455, -0.030408982187509537, -5.453304765978828e-05, 0.02682504430413246, 0.006751603446900845, -0.0061166915111243725, 0.008726930245757103, 0.003847371554002166, -0.036108989268541336, 0.02359381504356861, 0.040755487978458405, -0.01361441146582365, 0.013644592836499214, 0.036641839891672134, -0.016523253172636032, -0.013873652555048466, 0.004950316157191992, 0.044752493500709534, -0.0008865159470587969, -0.001286680344492197, 0.009860855527222157, 0.0028776514809578657, 0.009554249234497547, -0.01856444776058197, 0.07124771922826767, -0.0006224162061698735, 0.04610419273376465, 0.03720797225832939, 0.023793872445821762, 0.045397382229566574, -0.007710904814302921, -0.03626507520675659, 0.014539114199578762, -0.012218223884701729, -0.04158511757850647, -0.03093508630990982, 0.02437831461429596, -0.011684655211865902, 0.004660975653678179, -0.026047049090266228, 0.04537668079137802, 0.03689431771636009, 0.018201690167188644, 0.006500085815787315, -0.042241420596838, -0.0013872269773855805, -0.039118584245443344, 0.006740291137248278, -0.0017138444818556309, -0.05543439835309982, -0.021237516775727272, -0.0013991465093567967, 0.01370946690440178, -0.04123878479003906, -0.004064187873154879, 0.03426249697804451, -0.048788152635097504, -0.003569323569536209, 0.007530110888183117, 0.014368576928973198, 0.0013564456021413207, -0.036204610019922256, 0.015533311292529106, -0.008225788362324238, 0.012703502550721169, -0.00466541200876236, 0.0029137744568288326, -0.0016963270027190447, 0.0234499741345644, 0.013585114851593971, 0.0013910080306231976, -0.0005706967785954475, -0.029677817597985268, -0.00941239669919014, -0.060042571276426315, -0.01495889388024807, -0.027958707883954048, 0.04084332287311554, 0.024457765743136406, -0.05213761702179909, 0.0018681854708120227, 0.059325043112039566, 0.019694028422236443, -0.01555340550839901, -0.008897635154426098, 0.05946438014507294, -0.05483334884047508, 0.004777544643729925, 0.006488656159490347, 0.0088753467425704, -0.04891066253185272, -0.05777313932776451, 0.007510573603212833, -0.003750910982489586, 0.012125251814723015, -0.009258447214961052, 0.029633568599820137, 0.0732194110751152, 0.03192003816366196, 0.038968298584222794, -0.002592732897028327, -0.011056790128350258, -0.020986268296837807, -0.011808044277131557, 0.06038079410791397, -0.028979895636439323, -0.0191265307366848, -0.006361805833876133, 0.00076326634734869, 0.04346099868416786, -0.014284978620707989, 0.0025219705421477556, 0.020539000630378723, -0.005394664127379656, -0.02976427786052227, 0.03194204345345497, 0.0016489389818161726, 0.034893762320280075, 0.008461304008960724, -0.007375051733106375, -0.029527386650443077, 0.004920145496726036, -0.0358378142118454, 0.0014287057565525174, -0.03361758589744568, -0.027870459482073784, 0.013980062678456306, -0.023217489942908287, -8.228232218243647e-06, -0.0173217561095953, -0.02541944943368435, 0.007396046072244644, 0.032900966703891754, 0.03176164999604225, -0.0019032822456210852, 0.054452043026685715, -0.009782899171113968, 0.002526890253648162, -0.02455063909292221, -0.010998754762113094, -0.010872392915189266, 0.0012647422263398767, -0.019914660602808, -0.058822885155677795, 0.03835329785943031, 0.021983301267027855, 0.01571698486804962, -0.007201698608696461, -0.042316943407058716, -0.0196396317332983, 0.030868858098983765, 0.011480108834803104, -0.034620292484760284, 0.0397280789911747, -0.0540938563644886, 0.03561278432607651, -0.016285791993141174, -0.0017788914265111089, -0.03308544307947159, 0.05230477452278137, 0.03739158436655998, -0.02231757901608944, -0.0047051142901182175, -0.007130937650799751, 0.04598025977611542, -0.01277126930654049, 0.051579996943473816, 0.010327824391424656, 0.0313618928194046, -0.0019826379138976336, -0.027938371524214745, -0.010399390012025833, 0.025114119052886963, 0.029399679973721504, -0.010297982953488827, -0.007872143760323524, 0.06859876960515976, 0.04703425243496895, 0.0044432939030230045, 0.00658213859423995, 0.0678575187921524, -0.011891882866621017, 0.008021270856261253, -0.013296539895236492, -0.035542696714401245, 0.043519847095012665, 0.01206115260720253, -0.006668777205049992, 0.0006522060721181333, -0.012088238261640072, -0.032570865005254745, -0.023227307945489883, -0.05473388731479645, 0.0596340037882328, -0.005818299483507872, -0.06747155636548996, -0.038223639130592346, 0.03684459999203682, 0.03506392985582352, 0.003141086082905531, 0.022623253986239433, 0.0017162869917228818, -0.00035543617559596896, 0.024405866861343384, -0.01277893502265215, 0.024565361440181732, 0.013469868339598179, -0.005007716827094555, -0.031004508957266808, 0.012783678248524666, -0.024706311523914337, 0.02855536714196205, 0.006069005932658911, -0.01596543751657009, -0.012364807538688183, 0.041303601115942, 0.0117997657507658, -0.030623510479927063, 0.06852515041828156, -0.05476437509059906, -0.039475973695516586, -0.060484033077955246, 0.026598040014505386, -0.03363487496972084, 0.02993079647421837, 0.050330642610788345, -0.02836856245994568, -0.035856008529663086, 0.036514829844236374, -0.030621755868196487, -0.012158607132732868, 0.03158079832792282, 0.047558847814798355, 0.027388207614421844, 0.0741504356265068, 0.029126495122909546, 0.025378664955496788, 0.009740866720676422, -5.8660014474298805e-05, 0.008780018426477909, -0.023617390543222427, -0.017965173348784447, -0.008493752218782902, 0.017652925103902817, -0.0005075275548733771, -0.06072498857975006, 0.0500016063451767, 0.005686170421540737, -0.007244277745485306, 0.007157737854868174, 0.010150154121220112, 0.022161459550261497, -0.05740087479352951, -0.02273075468838215, 0.041557811200618744, -0.023781247437000275, 0.025857023894786835, -0.016086909919977188, -0.025158477947115898, -0.03614026680588722, -0.009624644182622433, -0.01992621272802353, 0.040366653352975845, 0.0029720221646130085, -0.029961591586470604, -0.013541818596422672, -0.024106403812766075, -0.04062867537140846, 0.0033843812998384237, -0.006244194693863392, -0.02628234587609768, 0.029105529189109802, 0.03268716111779213, 0.04245973005890846, 0.011009982787072659, -0.017955364659428596, -0.009454927407205105, 0.06106465682387352, 0.06734117865562439, 0.03548333793878555, 0.08071418106555939, 0.08212252706289291, 0.016332171857357025, -0.0071194772608578205, 0.015529831871390343, -0.015324246138334274, -0.003205358050763607, 0.01860734634101391, -0.000373758259229362, 0.03486444428563118, 0.015482626855373383, -0.05097323656082153, -0.002008056500926614, 0.03419001027941704, 0.021746432408690453, -0.020772894844412804, -0.037842776626348495, 0.00630715349689126, -0.062294647097587585, 0.006960931699723005, -0.05397244542837143, 0.04462096467614174, 0.018439479172229767, 0.01117101963609457, -0.03324931487441063, -0.05632997304201126, 0.2619708478450775, 0.056888241320848465, 0.0286569744348526, 0.046164099127054214, 0.049111317843198776, 0.009949609637260437, 0.03798860311508179, -0.04804087057709694, 0.01281188614666462, -0.008436464704573154, 0.026416707783937454, -0.023202188313007355, 0.00022981084475759417, -0.01006083469837904, -0.0042188409715890884, 0.025424899533391, -0.027470063418149948, 0.0028962146025151014, -0.005600049626082182, -0.05386108160018921, -0.0396086759865284, 0.009374122135341167, 0.010794367641210556, 0.03156525641679764, 0.031247597187757492, -0.015967957675457, 0.01038158405572176, -0.020025983452796936, -0.008098510093986988, -0.02764901891350746, 0.01837622933089733, -0.03310869634151459, 0.03988706320524216, -0.003626162651926279, -0.029794879257678986, 0.021837111562490463, -0.018653079867362976, -0.08756808936595917, -0.03800326585769653, 0.05286054685711861, -0.04788535088300705, -0.0442378968000412, 0.04639783501625061, -0.012463230639696121, -0.007550964597612619, 0.05120985954999924, 0.028137505054473877, 0.015871956944465637, 0.04624127596616745, -0.0680142417550087, 0.02293991670012474, -0.029249215498566628, 0.0022263801656663418, -0.012837067246437073, -0.04920940101146698, -0.006775049027055502, 0.0023281751200556755, -0.030219996348023415, 0.00038448875420726836, 0.04608737677335739, 0.016337281093001366, 0.01596563495695591, -0.022624872624874115, 0.010307395830750465, -0.0025485726073384285, 0.008537701331079006, 0.029897985979914665, 0.01542841736227274, -0.018183890730142593, 0.020483573898673058, 0.04523243382573128, 0.011646141298115253, -0.03510754555463791, 0.054996903985738754, 0.013721815310418606, 0.0431470163166523, -0.00946987047791481, 0.025246543809771538, 0.018923262134194374, -0.03691565617918968, -0.011912022717297077, 0.015165496617555618, -0.023135224357247353, 0.040549539029598236, 0.009056049399077892, 0.07320580631494522, -0.033331483602523804, -0.01528096478432417, -0.04217228665947914, 0.029711134731769562, -0.0013938499614596367, 0.035959456115961075, 0.024842364713549614, -0.014830493368208408, 0.011890152469277382], metadata={}, excluded_embed_metadata_keys=[], excluded_llm_metadata_keys=[], relationships={}, metadata_template='{key}: {value}', metadata_separator='\\n', text_resource=MediaResource(embeddings=None, data=None, text='\\nAs part of Docling, we initially release two highly capable AI models to the open-source community, which have been developed and published recently by our team. The first model is a layout analysis model, an accurate object-detector for page elements [13]. The second model is TableFormer [12, 9], a state-of-the-art table structure recognition model. We provide the pre-trained weights (hosted on huggingface) and a separate package for the inference code as docling-ibm-models . Both models are also powering the open-access deepsearch-experience, our cloud-native service for knowledge exploration tasks.\\n\\n## Layout Analysis Model\\n\\nOur layout analysis model is an object-detector which predicts the bounding-boxes and classes of various elements on the image of a given page. Its architecture is derived from RT-DETR [16] and re-trained on DocLayNet [13], our popular human-annotated dataset for document-layout analysis, among other proprietary datasets. For inference, our implementation relies on the onnxruntime [5].\\n\\nThe Docling pipeline feeds page images at 72 dpi resolution, which can be processed on a single CPU with sub-second latency. All predicted bounding-box proposals for document elements are post-processed to remove overlapping proposals based on confidence and size, and then intersected with the text tokens in the PDF to group them into meaningful and complete units such as paragraphs, section titles, list items, captions, figures or tables.\\n\\n## Table Structure Recognition\\n\\nThe TableFormer model [12], first published in 2022 and since refined with a custom structure token language [9], is a vision-transformer model for table structure recovery. It can predict the logical row and column structure of a given table based on an input image, and determine which table cells belong to column headers, row headers or the table body. Compared to earlier approaches, TableFormer handles many characteristics of tables, such as partial or no borderlines, empty cells, rows or columns, cell spans and hierarchy both on column-heading or row-heading level, tables with inconsistent indentation or alignment and other complexities. For inference, our implementation relies on PyTorch [2].\\n', path=None, url=None, mimetype=None), image_resource=None, audio_resource=None, video_resource=None, text_template='{metadata_str}\\n\\n{content}'),\n",
       " Document(id_='281e3917-f07f-4f02-8131-a5ad1cbfdda5', embedding=[0.023855648934841156, -0.03277719393372536, 0.0057394434697926044, -0.007147901225835085, 0.013371225446462631, -0.006357558071613312, 0.012332151643931866, -0.02439696528017521, 0.005156989675015211, 0.04730113595724106, -0.002664386061951518, -0.007777538150548935, -0.002918347017839551, -0.008174424059689045, 0.0017735284054651856, 0.008911864832043648, -0.0014029043959453702, -0.04412791132926941, -0.06656719744205475, 0.001771343406289816, -0.0023002212401479483, -0.029059283435344696, -0.05831074342131615, -0.011273992247879505, -0.02832067385315895, 0.0227959044277668, 0.014227697625756264, -0.026250116527080536, 0.09323715418577194, 0.04183043912053108, -0.03585415333509445, -0.02397247590124607, 0.01842215098440647, -0.0012129407841712236, -0.008296760730445385, -0.02719218283891678, 0.022042568773031235, -0.009753415361046791, -0.034122079610824585, -0.04213794693350792, 0.03439411148428917, -0.033057305961847305, 0.01987924799323082, -0.06415504217147827, -0.0688122808933258, 0.00033912909566424787, 0.029762158170342445, -0.0098768575116992, -0.04016399011015892, -0.03815220296382904, -0.011305159889161587, 0.00390133005566895, -0.025268152356147766, -0.014024198055267334, -0.023965848609805107, 0.009310771711170673, 0.00499579356983304, -0.017349690198898315, -0.04030069708824158, 0.024052729830145836, 0.02433420903980732, -0.0044570304453372955, 0.015084558166563511, -0.06031368300318718, -0.0018234774470329285, 0.037054579704999924, -0.01665724441409111, -0.025570444762706757, -0.000450409745099023, 0.0019397075520828366, -0.048627257347106934, 0.016184812411665916, -0.029963774606585503, -0.013850786723196507, -0.034781765192747116, -0.03830215707421303, 0.02006828971207142, -0.0009115231223404408, -0.041099030524492264, 0.06515829265117645, -0.00865321047604084, 0.005356955341994762, -0.00733509985730052, 0.011300310492515564, -0.007969400845468044, -0.06834082305431366, 0.07305018603801727, -0.010880899615585804, 0.036507364362478256, 0.000775905151385814, -0.013344276696443558, 0.04389357566833496, 0.00482417456805706, 0.01118365116417408, -0.006923834793269634, 0.06535321474075317, -0.01106612291187048, 0.034630514681339264, 0.009677991271018982, 0.021084614098072052, 0.0200410895049572, 0.07352026551961899, -0.011419287882745266, 0.040002547204494476, -0.06236005574464798, 0.004271244164556265, 0.03904767334461212, -0.05431394279003143, -0.03570878878235817, -0.04998078942298889, 0.002920236438512802, -0.023725735023617744, -0.015978211537003517, -0.008568175137043, -0.006138320080935955, 0.007293224800378084, 0.0161328986287117, -0.013844727538526058, 0.003323016921058297, 0.00397699186578393, -0.004859390202909708, -0.013042430393397808, -0.0006489661755040288, -0.021107155829668045, -0.005429564043879509, -0.013350245542824268, 0.016318151727318764, 0.046006981283426285, -0.04631947726011276, -0.014723376370966434, 0.020310867577791214, -0.029268544167280197, -0.06503555923700333, 0.047263722866773605, -0.011167354881763458, -0.007320969831198454, 0.048342905938625336, -0.002560277935117483, 0.022254018113017082, -0.01846417412161827, 0.0023210898507386446, -0.04797688126564026, -0.018918003886938095, 0.07897888869047165, -0.020435597747564316, 0.020065365359187126, -0.011456944979727268, -0.010858895257115364, -0.07501253485679626, 0.03327682986855507, -0.012001910246908665, -0.0008177162962965667, 0.0302998386323452, 0.03928045928478241, -0.045613791793584824, 0.015071566216647625, -0.007583211176097393, -0.008315440267324448, -0.01550663448870182, 0.008682009764015675, 0.00051051628543064, -0.014393921010196209, -0.03871714323759079, 0.004770680796355009, -0.03243495151400566, 0.06279698014259338, -0.029879050329327583, -0.025374438613653183, -0.010993145406246185, -0.024463895708322525, 0.01718161441385746, 0.02648722194135189, -0.01144198328256607, 0.018593208864331245, 0.04687408357858658, 0.024408213794231415, 0.04914466291666031, 0.012812326662242413, 0.016987673938274384, 0.016209272667765617, -0.008273402228951454, 0.002176696667447686, -9.186647366732359e-05, 0.019036361947655678, 0.012945501133799553, 0.04820659011602402, 0.01336340419948101, -0.0071610212326049805, 0.0024399098474532366, -0.0037496781442314386, 0.018531866371631622, 0.05627612769603729, -0.009372265078127384, 0.024575619027018547, -0.007600628305226564, -0.024895310401916504, -0.054195601493120193, -0.0004333554534241557, 0.00496265571564436, -0.02681008167564869, -0.02525182254612446, 0.026912949979305267, 0.04243917763233185, 0.010733888484537601, -0.06139908358454704, 0.012500450946390629, 0.02913053333759308, 0.038122858852148056, -0.055813051760196686, 0.018315287306904793, 0.03589008003473282, 0.0035477986093610525, -0.048903439193964005, 0.0006795540684834123, 0.04658057540655136, -0.03783194348216057, 0.0020832070149481297, -0.019599217921495438, -0.012163742445409298, -0.000371962902136147, 0.0059835962019860744, -0.02124003693461418, 0.0357392318546772, -0.03937262296676636, -0.015127195045351982, 0.03298285976052284, 0.030267473310232162, 0.02340109646320343, 0.031240055337548256, 0.01869087666273117, -0.04495728760957718, 0.03190332651138306, -0.014896979555487633, 0.038146164268255234, 0.04646923765540123, -0.006148116197437048, 0.05525331571698189, 0.020917145535349846, 0.0012050713412463665, 0.03292076662182808, -0.022139109671115875, 0.03228624537587166, 0.015464019030332565, 0.03407849743962288, 0.014320448972284794, 0.03227442502975464, -0.012436393648386002, -0.01768822968006134, -0.015663456171751022, -0.011677724309265614, -0.04909452423453331, 0.05326106399297714, 0.0018327927682548761, 0.0035177224781364202, -0.018223633989691734, -0.02030298486351967, 0.004539061337709427, 0.03101411834359169, -0.02012186497449875, -0.02442753314971924, -0.0023904319386929274, 0.035601504147052765, 0.0021241737995296717, -0.01372096873819828, 0.0024591938126832247, 0.034343842417001724, -0.018261145800352097, 0.008922774344682693, 0.01205295231193304, -0.03396817669272423, -0.04140803590416908, -0.04820510372519493, -0.07863273471593857, 0.021757453680038452, -0.041739556938409805, -0.0055915056727826595, -0.0006194449379108846, -0.03939966857433319, 0.014607708901166916, 0.005348403472453356, -0.0006705574342049658, 0.028503209352493286, -0.025739358738064766, 0.03789587318897247, 0.0065443553030490875, 0.008422854356467724, -0.05788061395287514, 0.03351942449808121, -0.011323151178658009, 0.04341055080294609, -0.019858967512845993, -0.0015797974774613976, -0.04899252951145172, -0.00031043431954458356, 0.008943591266870499, -0.009264802560210228, -0.01262175478041172, -0.008852909319102764, -0.011652713641524315, -0.018464885652065277, -0.0444202721118927, -0.06391604989767075, 0.01833084225654602, -0.03425930067896843, -0.046691183000802994, 0.03089415654540062, -0.01573488861322403, -0.031974975019693375, 0.05009504780173302, 0.0016922802897170186, -0.028990931808948517, 0.023107880726456642, -0.007814248092472553, 0.03803446888923645, -0.03139202669262886, 0.05490432307124138, 0.006010540295392275, -0.02366187982261181, -0.006715413182973862, -0.028208065778017044, -0.027288490906357765, 0.001508510671555996, -0.018004631623625755, -0.039216816425323486, -0.026631753891706467, 0.03289829567074776, 0.0009967789519578218, -0.08239541947841644, -0.015078558586537838, 0.00742537435144186, -0.052120234817266464, -0.009628497064113617, -0.02396131493151188, 0.036971382796764374, 0.018828395754098892, 0.0009619079646654427, -0.05082624405622482, 0.027213171124458313, 0.0011840980732813478, 0.012723474763333797, 0.013090797699987888, -0.03441972658038139, 0.006521678064018488, 0.06331547349691391, 0.0029581172857433558, 0.0035568741150200367, 0.01055791787803173, -0.0369737483561039, -0.00631994754076004, -0.017500486224889755, -0.013223053887486458, 0.029523901641368866, 0.019843686372041702, 0.0173311997205019, 0.01966891810297966, 0.014996210113167763, -0.044841866940259933, -0.023595549166202545, 0.003905040677636862, 0.01390821859240532, 0.03440472111105919, 0.03536415472626686, -0.004049311857670546, 0.0045617795549333096, -0.01830216869711876, -0.012241264805197716, 0.009731339290738106, 0.015709171071648598, 0.06644850969314575, -0.07106903940439224, 0.042260363698005676, 0.03034479171037674, -0.025129932910203934, 0.02475597895681858, -0.03350353613495827, -0.05002583563327789, 0.038293350487947464, 0.0066413977183401585, 0.07519146054983139, -0.02970605529844761, 0.00853012502193451, 0.005535034462809563, -0.01295945793390274, 0.0334019809961319, 0.020210163667798042, 0.027305401861667633, -0.008633678779006004, -0.018657496199011803, -0.012389251962304115, -0.024459179490804672, -0.012094718404114246, 0.03082582913339138, -0.036055270582437515, 0.007117095403373241, -0.06174483150243759, -0.022113490849733353, 0.02655206248164177, 0.06272067874670029, 0.02454887144267559, 0.01388551015406847, 0.06680824607610703, -0.03485064208507538, 0.03359580412507057, 0.036021117120981216, 0.021807245910167694, 0.024068035185337067, -0.053276319056749344, 0.03963860869407654, -0.035693615674972534, 0.003236129181459546, -0.0033347427379339933, -0.026508808135986328, -0.009429339319467545, 0.020368851721286774, -0.003846515202894807, 0.017391346395015717, -0.016751810908317566, -0.03173143416643143, 0.009039151482284069, 0.026386428624391556, -0.029895896092057228, -0.029791951179504395, -0.03900258243083954, 0.010813143104314804, 0.03552570939064026, -0.036361031234264374, -0.011124808341264725, -0.0024435995146632195, 0.023182515054941177, 0.0040324293076992035, -0.03565552458167076, -0.0040462021715939045, -0.005088372156023979, -0.02455838769674301, -0.03169111907482147, 0.06978452205657959, 0.036119863390922546, 0.021022042259573936, 0.01396285742521286, -0.0172483641654253, -0.007587369065731764, -0.003491676179692149, -0.01609846204519272, -0.025901716202497482, 0.024631455540657043, 0.017085902392864227, 0.04435168579220772, 0.0456339456140995, 0.011493196710944176, 0.012601586990058422, 0.026150858029723167, -0.03926981985569, 0.05032426118850708, -0.05020690709352493, 0.011944693513214588, 0.002441974589601159, -0.0002306052774656564, -0.021491069346666336, -0.029378654435276985, -0.006608243566006422, -0.0064670913852751255, -0.01592661254107952, 0.015248162671923637, -0.009934711270034313, -0.06913931667804718, 0.056209735572338104, -0.0147778345271945, -0.004995775409042835, 0.020639531314373016, 0.0033377702347934246, -0.03526287525892258, -0.018218612298369408, 0.011896971613168716, -0.0031947235111147165, 0.024135712534189224, -0.011778423562645912, 0.02760201133787632, -0.02977694384753704, -0.03853204473853111, 0.0014014823827892542, -0.00472894124686718, 0.03345822915434837, -0.04169086366891861, -0.03659836947917938, -0.01669371873140335, -0.03440932556986809, -0.0010670428164303303, 0.01880425401031971, -0.04482771083712578, 0.01167344395071268, -0.01156906969845295, -0.0253303200006485, 0.03448958322405815, -0.045322395861148834, -0.02805737406015396, -0.028434759005904198, -0.00800318457186222, -0.004260432906448841, 0.004678713157773018, 0.03364258632063866, 0.023954123258590698, -0.010899612680077553, -0.04806322604417801, 0.013678256422281265, -0.009757286868989468, -0.033628009259700775, -0.04448927193880081, 0.022914523258805275, -0.0009360539261251688, 0.001114576356485486, -0.02175680547952652, -0.015578217804431915, -0.0060798488557338715, -0.03561006858944893, -0.0013013518182560802, -0.014347064308822155, 0.03193482756614685, -0.00654249545186758, 0.015728788450360298, 0.04572134092450142, 0.009045399725437164, -0.07260427623987198, -0.051299091428518295, 0.0229144636541605, 0.00570001732558012, -0.005308515392243862, 0.022836588323116302, -0.016410883516073227, -0.029676897451281548, -0.02691984735429287, -0.01982348971068859, -0.027273869141936302, -0.01879728212952614, -0.004767715465277433, -0.04197780415415764, 0.0009444195893593132, -0.01730792410671711, 0.05476090684533119, -0.02436796948313713, -0.017589030787348747, -0.07361961156129837, 0.009020313620567322, -0.013526731170713902, -0.017247578129172325, -0.029792018234729767, 0.006482810713350773, -0.030966566875576973, 0.019038235768675804, 0.007694062776863575, -0.0033547866623848677, -0.025927269831299782, 0.016932768747210503, 0.04889217019081116, 0.039265815168619156, -0.03584491088986397, -0.02305501140654087, 0.008036753162741661, -0.0025561386719346046, -0.006587970536202192, -0.018702981993556023, -0.04231710359454155, 0.038651518523693085, -0.05383298173546791, 0.004259155131876469, 0.007546508219093084, -0.0005632407264783978, -0.023799993097782135, -0.018917296081781387, 0.021085252985358238, -0.041124965995550156, 0.010875090025365353, -0.048399895429611206, 0.045363906770944595, -0.029593978077173233, -0.006799423601478338, -0.05065115541219711, -0.02170414663851261, -0.05650458484888077, -0.0025975406169891357, -0.01751476339995861, -0.0380544550716877, 0.0023865988478064537, 0.0073992968536913395, 0.010949975810945034, 0.028633970767259598, -0.016880448907613754, 0.043972618877887726, 0.048309575766325, 0.016752155497670174, 0.006988657638430595, -0.006266722455620766, 0.017347978428006172, 0.0037153507582843304, -0.032236821949481964, 0.008886963129043579, 0.007859164848923683, -0.029355067759752274, 0.009969556704163551, 0.006916786078363657, -0.03893252834677696, -0.00712693901732564, 0.03853154927492142, 0.041970137506723404, -0.006726650986820459, 0.026422517374157906, 0.00506037101149559, -0.039244383573532104, -0.019925815984606743, 0.03894276171922684, -0.01742599718272686, -0.006306210067123175, 0.03303951025009155, -0.031313277781009674, 0.02834422141313553, 0.04387348145246506, -0.038474127650260925, -0.007636381778866053, -0.012439992278814316, 0.07543568313121796, -0.007553430739790201, -0.05871233716607094, 0.05192917585372925, 0.03189823403954506, -0.005808578338474035, -0.031603533774614334, 0.009759150445461273, -0.006861070636659861, -0.009030620567500591, -0.007937830872833729, 0.007411556784063578, 0.005701473448425531, -0.008824458345770836, -0.0014834015164524317, 0.03665410727262497, -0.027976885437965393, 0.004121497273445129, 0.01267177239060402, -0.008242396637797356, -0.024065308272838593, 0.020037876442074776, 0.05427660793066025, -0.016128187999129295, 0.012267965823411942, 0.01115366443991661, 0.00322278612293303, 0.007210229989141226, 0.005177129991352558, 0.0321950763463974, 0.015890780836343765, 0.03758835047483444, 0.04342014715075493, 0.01789061538875103, 0.04238061606884003, -0.007149589713662863, -0.02426866628229618, 0.01525077223777771, -0.0015700614312663674, -0.0465395450592041, -0.029741963371634483, 0.0027312589809298515, 0.005803324747830629, 0.00996733084321022, -0.018329232931137085, 0.03730950504541397, 0.027295177802443504, 0.008862512186169624, 0.03956510126590729, -0.05886562168598175, 0.0174202062189579, -0.040165480226278305, 0.008681909181177616, -0.006238508969545364, -0.07303481549024582, -0.04519099369645119, 0.0023074387572705746, -0.0023620091378688812, -0.05366801097989082, -0.011606484651565552, 0.029200010001659393, -0.03181052580475807, -0.010541039519011974, 0.019631395116448402, 0.010121849365532398, -0.011416281573474407, -0.03415457531809807, 0.020219050347805023, -0.00760693522170186, 0.009959260933101177, 0.01748320832848549, -0.010098055005073547, -0.014621319249272346, 0.008526448160409927, 0.0033824327401816845, 0.03040316328406334, 0.012606133706867695, -0.025442618876695633, -0.011868700385093689, -0.05944042652845383, -0.0015534590929746628, -0.039533644914627075, 0.04778183251619339, 0.03658100962638855, -0.05513034388422966, -0.006295803003013134, 0.061784107238054276, 0.010302392765879631, -0.0382106639444828, 0.0014621050795540214, 0.06013612449169159, -0.04500207677483559, 0.013027905486524105, -0.003588702529668808, 0.02617214061319828, -0.053445082157850266, -0.03798524662852287, -0.003038547234609723, -0.003669844241812825, -0.004065986257046461, 0.00030753310420550406, 0.012002934701740742, 0.07354813814163208, 0.05906546860933304, 0.036738935858011246, -0.00396993150934577, -0.024528656154870987, -0.00021686057152692229, -0.014259076677262783, 0.04393778741359711, -0.0031876673456281424, -0.03313462436199188, -0.04750975966453552, 0.021113017573952675, 0.05298478528857231, -0.0009263645624741912, 0.016625501215457916, -0.004987072665244341, 0.004012129735201597, -0.045617155730724335, 0.0024127033539116383, 0.011584199965000153, 0.03248056024312973, 0.017796281725168228, -0.020358530804514885, -0.04176903888583183, 0.0167999304831028, -0.016880329698324203, -0.013643477112054825, -0.041736796498298645, -0.03182651847600937, 0.009003819897770882, -0.0455741249024868, 0.0226020235568285, -0.012071234174072742, -0.030750924721360207, -0.003005627542734146, 0.04277116432785988, 0.03675931692123413, 0.0010023398790508509, 0.04798265919089317, -0.009394120424985886, 0.028815973550081253, -0.016915131360292435, -0.015020187944173813, 0.004713185131549835, 0.011234031990170479, -0.03514094650745392, -0.053245097398757935, 0.029169637709856033, 0.03846900910139084, -0.00016035165754146874, -0.0012059053406119347, -0.04961640015244484, 0.010901517234742641, 0.026249997317790985, 0.019928550347685814, -0.04136379435658455, 0.03860466554760933, -0.0323256254196167, 0.04729413613677025, -0.03260760009288788, 0.006250895094126463, -0.020163225010037422, 0.03314841911196709, 0.040639884769916534, -0.018650081008672714, -0.009028805419802666, 0.0038154905196279287, 0.07535333186388016, -0.016681792214512825, 0.051482465118169785, -0.002786837285384536, 0.018438071012496948, -0.010500112548470497, -0.03404412791132927, -0.030688682571053505, 0.009163963608443737, 0.02322155237197876, 0.019388433545827866, -0.016259493306279182, 0.0671888217329979, 0.04009857773780823, -0.006947503890842199, 0.0017129800980910659, 0.04896984621882439, 0.013238645158708096, -0.0018802937120199203, -0.007447052281349897, -0.03616343438625336, 0.017809392884373665, -0.02045804262161255, -0.01361748669296503, 0.0033203461207449436, 0.008062276989221573, -0.014395572245121002, -0.018694167956709862, -0.042156368494033813, 0.02672598510980606, -0.009919700212776661, -0.06690984219312668, -0.029557110741734505, 0.02209220454096794, 0.040452729910612106, 0.012644396163523197, 0.02527501992881298, 0.000656135322060436, 0.013148831203579903, 0.013559257611632347, 0.0061693135648965836, 0.011268356814980507, 0.02136462926864624, 0.008935551159083843, -0.043881066143512726, 0.019379286095499992, -0.051115039736032486, 0.03907793387770653, -0.005730487871915102, 0.001596766640432179, -0.020453529432415962, 0.028045224025845528, -0.008015573024749756, -0.0276812557131052, 0.04338281601667404, -0.03234842047095299, -0.03166430816054344, -0.04906315729022026, 0.039615433663129807, -0.044541314244270325, 0.04024500399827957, 0.06168213114142418, -0.03378777951002121, -0.04415826499462128, 0.0320354700088501, -0.030265260487794876, 0.008618657477200031, 0.03244088217616081, 0.07036252319812775, 0.015177068300545216, 0.053667955100536346, 0.017613394185900688, 0.015726106241345406, 0.016565939411520958, 0.010149895213544369, -0.006610524840652943, -0.02613249234855175, -0.01404003519564867, -0.013181804679334164, 0.02167465165257454, -0.0068556820042431355, -0.059576619416475296, 0.05080445110797882, 0.016547799110412598, -0.01772640272974968, -0.010333641432225704, 0.002014649799093604, 0.011558161117136478, -0.03931601345539093, -0.02191077545285225, 0.04893214628100395, -0.0010967289563268423, 0.04296189546585083, -0.025185922160744667, -0.02703407034277916, -0.031619373708963394, -0.005170157179236412, -0.00956358015537262, 0.0319790355861187, -0.0017218941356986761, -0.014216119423508644, 0.0058050029911100864, -0.04349600896239281, -0.046887002885341644, -0.01400057040154934, 0.0035831453278660774, -0.021136023104190826, 0.05359968915581703, 0.024338606745004654, 0.04999849572777748, 0.0064578549936413765, 6.014751852490008e-05, -0.005005353596061468, 0.0394117645919323, 0.05790401250123978, 0.027247373014688492, 0.08811664581298828, 0.0742800161242485, 0.021602516993880272, -0.014581202529370785, 0.005448103882372379, 0.0004763512988574803, -0.00014681939501315355, 0.02461945451796055, -0.0037792122457176447, 0.025026550516486168, 0.002062617801129818, -0.0587833896279335, -0.012860294431447983, 0.0600271075963974, 0.008104496635496616, -0.02189679443836212, -0.03169713169336319, 0.024161947891116142, -0.06299572438001633, 0.003253405215218663, -0.04699333384633064, 0.03820362314581871, 0.034026604145765305, 0.01428004540503025, -0.034189727157354355, -0.029051456600427628, 0.2652721405029297, 0.05774076655507088, 0.03788761794567108, 0.046683747321367264, 0.032777801156044006, 0.03403707593679428, 0.03670849651098251, -0.04895243048667908, 0.02044239267706871, -0.008240698836743832, 0.03198155760765076, -0.03977537155151367, 0.016649048775434494, -0.004614104982465506, -0.0032036779448390007, 0.0274214968085289, -0.02535914070904255, -0.024292131885886192, -0.005461511667817831, -0.06841567903757095, -0.022519651800394058, 0.005151009187102318, 0.003200859297066927, 0.0432283952832222, 0.026771269738674164, -0.03057357296347618, 0.021223384886980057, -0.002467034151777625, -0.005774430464953184, -0.03361174091696739, 0.012327314354479313, -0.03594217449426651, 0.03912018612027168, -0.01895879954099655, -0.03621671348810196, 0.015982937067747116, -0.0025716593954712152, -0.07947780936956406, -0.013510863296687603, 0.040203534066677094, -0.023521816357970238, -0.04175349324941635, 0.027798039838671684, -0.01807127147912979, -0.016288597136735916, 0.05032879114151001, 0.019020311534404755, 0.014385760761797428, 0.054801952093839645, -0.0674763172864914, 0.0022923159413039684, -0.03518666699528694, 0.008254057727754116, -0.017510319128632545, -0.05543674901127815, -0.013848105445504189, -0.009268810041248798, -0.04446909576654434, 0.007763188797980547, 0.06939738988876343, 0.025754859670996666, 0.02293889969587326, -0.0005234221462160349, 0.01601540856063366, -0.026799358427524567, 0.03684934601187706, 0.019392095506191254, 0.0049965763464570045, -0.04080817103385925, 0.006120110396295786, 0.039401136338710785, 0.004792990628629923, -0.007451287936419249, 0.00886035431176424, 0.026660675182938576, 0.060476165264844894, -0.018369007855653763, 0.04050569608807564, 0.009175810031592846, -0.047974370419979095, -0.002412242814898491, 0.010091066360473633, -0.006180113181471825, 0.004693806637078524, 0.009974081069231033, 0.06211851164698601, -0.04696447029709816, -0.013324660249054432, -0.02812608703970909, 0.029987605288624763, 0.009064317680895329, 0.047828298062086105, -0.01325549092143774, 0.003932897001504898, 0.02269788458943367], metadata={}, excluded_embed_metadata_keys=[], excluded_llm_metadata_keys=[], relationships={}, metadata_template='{key}: {value}', metadata_separator='\\n', text_resource=MediaResource(embeddings=None, data=None, text='\\nThe Docling pipeline feeds all table objects detected in the layout analysis to the TableFormer model, by providing an image-crop of the table and the included text cells. TableFormer structure predictions are matched back to the PDF cells in post-processing to avoid expensive re-transcription text in the table image. Typical tables require between 2 and 6 seconds to be processed on a standard CPU, strongly depending on the amount of included table cells.\\n\\n## OCR\\n\\nDocling provides optional support for OCR, for example to cover scanned PDFs or content in bitmaps images embedded on a page. In our initial release, we rely on EasyOCR [1], a popular thirdparty OCR library with support for many languages. Docling, by default, feeds a high-resolution page image (216 dpi) to the OCR engine, to allow capturing small print detail in decent quality. While EasyOCR delivers reasonable transcription quality, we observe that it runs fairly slow on CPU (upwards of 30 seconds per page).\\n\\nWe are actively seeking collaboration from the open-source community to extend Docling with additional OCR backends and speed improvements.\\n\\n## 3.3 Assembly\\n\\nIn the final pipeline stage, Docling assembles all prediction results produced on each page into a well-defined datatype that encapsulates a converted document, as defined in the auxiliary package docling-core . The generated document object is passed through a post-processing model which leverages several algorithms to augment features, such as detection of the document language, correcting the reading order, matching figures with captions and labelling metadata such as title, authors and references. The final output can then be serialized to JSON or transformed into a Markdown representation at the users request.\\n\\n## 3.4 Extensibility\\n\\nDocling provides a straight-forward interface to extend its capabilities, namely the model pipeline. A model pipeline constitutes the central part in the processing, following initial document parsing and preceding output assembly, and can be fully customized by sub-classing from an abstract baseclass ( BaseModelPipeline ) or cloning the default model pipeline. This effectively allows to fully customize the chain of models, add or replace models, and introduce additional pipeline configuration parameters. To use a custom model pipeline, the custom pipeline class to instantiate can be provided as an argument to the main document conversion methods. ', path=None, url=None, mimetype=None), image_resource=None, audio_resource=None, video_resource=None, text_template='{metadata_str}\\n\\n{content}'),\n",
       " Document(id_='6e65612a-32a0-4ca6-93de-2dc580aedbb2', embedding=[0.03391862288117409, -0.01815868355333805, -0.010174451395869255, 0.005700233858078718, 0.002757126232609153, -0.0019707430619746447, 0.016257625073194504, -0.03645189106464386, -0.00823079701513052, 0.06656815111637115, -0.015342666767537594, 0.003578155068680644, 0.010497831739485264, -0.02002999559044838, -0.003631732426583767, 0.020212016999721527, -0.010899888351559639, -0.056347448378801346, -0.05796566233038902, 0.0024444793816655874, 0.008685166947543621, -0.0018698025960475206, -0.06670118868350983, -0.002884423593059182, -0.05080798268318176, 0.026768764480948448, 0.016840141266584396, 0.018380455672740936, 0.0886135995388031, 0.03190796077251434, -0.004230543505400419, -0.01469686720520258, 0.026081934571266174, -0.011959589086472988, -0.02677401900291443, -0.025228653103113174, 0.008639766834676266, 0.004335487261414528, -0.02374141849577427, -0.03446618840098381, 0.04095327481627464, -0.051404744386672974, 0.044636957347393036, -0.050874847918748856, -0.053307510912418365, -0.003776528174057603, 0.015057534910738468, -0.014776820316910744, -0.0458887554705143, -0.054539214819669724, -0.007504270412027836, 0.028276514261960983, -0.013521586544811726, -0.031088564544916153, -0.011401914991438389, -0.005245096981525421, -0.010395089164376259, -0.0028563139494508505, -0.06356748193502426, -0.007761289365589619, 0.0408761240541935, -0.009521719999611378, 0.0001925435644807294, -0.06539258360862732, 0.014136401005089283, 0.034414760768413544, 0.0020714723505079746, -0.019461801275610924, -0.01921255700290203, 0.017646489664912224, -0.06760301440954208, 0.021129682660102844, -0.03182113915681839, -0.011549376882612705, -0.02223636396229267, -0.01484503410756588, 0.016474470496177673, 0.0008491550688631833, -0.04528956860303879, 0.05097850412130356, 0.02069508284330368, 0.018479935824871063, -0.01320509985089302, 0.013790614902973175, -0.019522247835993767, -0.08222229033708572, 0.04710811749100685, -0.03979036957025528, 0.011697698384523392, -0.01205360796302557, -0.0022921895142644644, 0.017257461324334145, 0.009192314930260181, 0.012183710932731628, 0.008924795314669609, 0.04963349923491478, -0.02288021706044674, 0.047449640929698944, 0.017063679173588753, 0.023788684979081154, 0.01888193003833294, 0.07573925703763962, -0.009834147989749908, 0.02136697806417942, -0.06837586313486099, 0.0300963893532753, 0.03532344475388527, -0.06665875017642975, -0.012648515403270721, -0.04406251013278961, 0.007330382242798805, 0.018712623044848442, -0.02133399061858654, 0.0032107976730912924, 0.012731370516121387, 0.00045866784057579935, 0.017337238416075706, -0.004043044988065958, 0.0017853528261184692, -0.008358721621334553, 0.02243848517537117, -0.030280493199825287, -0.03858549892902374, -0.01967916451394558, -0.008650061674416065, -0.020614618435502052, 0.016956113278865814, 0.057241786271333694, -0.03996977582573891, -0.04086034744977951, -0.0042577930726110935, -0.010726160369813442, -0.057854726910591125, 0.06024235859513283, 0.02415604516863823, -0.02732941322028637, 0.011659902520477772, 0.0036900534760206938, 0.030016174539923668, -0.019468309357762337, -0.010729009285569191, -0.06182592362165451, 0.001485448912717402, 0.09818627685308456, -0.0052028680220246315, 0.0011807660339400172, -0.008416038937866688, -0.009943448007106781, -0.05474116653203964, 0.04098900780081749, -0.02756880596280098, 0.021180395036935806, 0.023890072479844093, 0.024822810664772987, -0.02816051058471203, -0.0032641044817864895, -0.01253990363329649, 0.009809842333197594, 0.012648815289139748, 0.02165663056075573, 0.014224516227841377, -0.010223851539194584, -0.05129029601812363, 0.008847927674651146, -0.02677265554666519, 0.050516024231910706, -0.017381800338625908, -0.02253752574324608, 0.0268823504447937, -0.051001228392124176, 0.030777886509895325, 0.020208418369293213, -0.01625763438642025, 0.02178102359175682, 0.056445468217134476, 0.027166198939085007, 0.05236596614122391, 0.014887191355228424, 0.03697696328163147, 0.004923274274915457, -0.01913573406636715, -0.00437251478433609, 0.01722230389714241, 0.0026063688565045595, 0.03552106022834778, 0.04479807987809181, -0.0074926018714904785, -0.04048910737037659, 0.0036735646426677704, -0.02033167891204357, -0.0030880444683134556, 0.044019266963005066, -0.032998740673065186, 0.02497776970267296, 0.01586177386343479, -0.018997110426425934, -0.0421719066798687, 0.02363048493862152, -0.01064977329224348, -0.0619659349322319, -0.023505333811044693, 0.03707347437739372, 0.007706342730671167, 0.01255019847303629, -0.03562144190073013, 0.010574360378086567, -0.003725787391886115, 0.02680562622845173, -0.041199155151844025, 0.019695688039064407, 0.03313182666897774, -0.004217952489852905, -0.04071502387523651, -0.01839095912873745, 0.04429282620549202, -0.012908026576042175, -0.02076074853539467, -0.016837984323501587, 0.013978901319205761, 0.0012153288116678596, 0.040118392556905746, -0.010976344347000122, 0.029378440231084824, -0.04680579900741577, -0.021661382168531418, -0.0126750273630023, -0.005252232775092125, -0.0038438565097749233, 0.04219970852136612, 0.010308992117643356, -0.01913589984178543, 0.053930897265672684, -0.025509126484394073, 0.03537771478295326, 0.03834269940853119, -0.015055296942591667, 0.053763821721076965, 0.016414504498243332, -0.01827646978199482, 0.013674517162144184, -0.04362763091921806, 0.03287795931100845, 0.012563896365463734, 0.026019200682640076, 0.0114812720566988, 0.030213613063097, -0.022253427654504776, -0.025971928611397743, -0.012016013264656067, 0.017977043986320496, -0.03407701104879379, 0.045915089547634125, -0.0215450469404459, -0.02467011660337448, -0.02667192555963993, -0.023199189454317093, -0.0021659433841705322, 0.014495202340185642, -0.028726741671562195, -0.026312490925192833, -0.018650775775313377, 0.04588809981942177, -0.007016304414719343, -0.011900513432919979, 0.0008031714241951704, 0.05687211453914642, -0.008880765177309513, 0.006720241159200668, 0.04306608811020851, -0.013059546239674091, -0.031100047752261162, -0.04119333252310753, -0.09106551855802536, 0.012102940119802952, -0.04114939272403717, 0.004264190327376127, 0.017487160861492157, -0.035682011395692825, -0.006978999823331833, -0.002840348519384861, 0.015001388266682625, 0.027089759707450867, 0.006885960698127747, 0.058710310608148575, 0.021183393895626068, 0.022856656461954117, -0.0564582422375679, 0.02294836938381195, -0.012378945015370846, 0.025095494464039803, -0.014609862118959427, 0.0033730766735970974, -0.019604608416557312, -0.005774515215307474, 0.0513162799179554, -0.005654319655150175, 0.01686226576566696, 0.002302217995747924, -0.012630419805645943, -0.021505990996956825, -0.049574870616197586, -0.049157027155160904, 0.01827685534954071, -0.019457297399640083, -0.060547612607479095, 0.03098568134009838, 0.0028154850006103516, -0.016972532495856285, 0.06126369535923004, 0.002849302953109145, -0.04072469100356102, 0.02997100166976452, 0.015481622889637947, 0.03284722939133644, -0.03719914332032204, 0.07596088200807571, 0.0118497209623456, -0.03253033384680748, -0.005995180457830429, -0.05999305844306946, -0.013279405422508717, -0.007149416022002697, 0.000632579205557704, -0.06572167575359344, -0.03801964223384857, 0.02928263694047928, -0.003955743741244078, -0.07253673672676086, 0.006672476883977652, -0.010737416334450245, -0.02832057513296604, -0.004809842444956303, -0.029176807031035423, 0.03273303061723709, 0.01232215017080307, -0.0031751268543303013, -0.06385137140750885, 0.016068147495388985, -0.030330954119563103, 0.02551328018307686, 0.015288863331079483, -0.03261163458228111, 0.01526717934757471, 0.04366140067577362, -0.002208232181146741, -0.015405723825097084, 0.021644793450832367, -0.03343923017382622, -0.025450924411416054, -0.005383907817304134, -0.025355961173772812, -0.0026387260295450687, 0.028983280062675476, 0.02685619704425335, 0.015282840467989445, 0.01975272037088871, -0.04746057465672493, -0.019803687930107117, -0.02043599635362625, 0.008845084346830845, 0.01567963883280754, 0.04152282699942589, 0.020375056192278862, 0.018276572227478027, -0.026844527572393417, -0.007552601397037506, 0.015449807047843933, 0.005407213699072599, 0.06975577026605606, -0.0362774096429348, 0.034315675497055054, -0.006896532140672207, -0.03607882931828499, -0.001991667551919818, -0.003885584184899926, -0.04074355587363243, 0.06523699313402176, 0.02133006416261196, 0.058753617107868195, 0.017600005492568016, 0.012109873816370964, 0.00872966181486845, -0.010005067102611065, 0.04564325883984566, -0.00030647218227386475, 0.05996648594737053, -0.013056738302111626, 0.0019210928585380316, -0.005704354029148817, -0.023266950622200966, 0.010133469477295876, 0.03561592474579811, -0.058208562433719635, 0.004049798473715782, -0.047634582966566086, -0.03655863180756569, 0.05296854302287102, 0.05240839719772339, 0.019771305844187737, 0.002255544764921069, 0.06435295939445496, -0.04920171573758125, 0.012121744453907013, 0.02214929834008217, 0.03386572375893593, 0.0028501548804342747, -0.0849793404340744, 0.05304925516247749, -0.01407643686980009, 0.021008647978305817, -0.028611138463020325, -0.027968550100922585, -0.04648371785879135, 0.006656368263065815, -0.0129315797239542, 0.010800844989717007, -0.04433867707848549, -0.02333485521376133, -0.012158529832959175, 0.028768669813871384, -0.02621704898774624, -0.0204874649643898, -0.012414640747010708, 0.024560008198022842, 0.03449013829231262, -0.006715148687362671, -0.011161771602928638, -0.03440365567803383, 0.007327967323362827, 0.005448289215564728, -0.03573743999004364, -0.034138862043619156, -0.03636772185564041, -0.021250056102871895, -0.061056461185216904, 0.05147562548518181, 0.022778823971748352, -0.0015694745816290379, -0.0017634282121434808, -0.051363661885261536, 0.007364639546722174, 0.022111183032393456, -0.006029604934155941, 0.0008919950923882425, 0.02759464643895626, -0.01416595745831728, 0.016015347093343735, 0.027040135115385056, 0.0119010079652071, -0.0024875153321772814, 0.0034921448677778244, -0.054374195635318756, 0.0308519396930933, -0.06280632317066193, -0.004989381413906813, -0.002547567244619131, -0.03825840353965759, 0.009229438379406929, 0.0025086889509111643, 0.016767820343375206, -0.012057171203196049, -0.007401803974062204, 0.03782869875431061, -0.013762975111603737, -0.025152841582894325, 0.03728041425347328, 0.01843801699578762, 0.0038227506447583437, 0.048378147184848785, 0.012949639931321144, -0.02199084497988224, 0.004990455694496632, 0.023684462532401085, -0.009775024838745594, 0.01943976618349552, 0.00756892841309309, 0.028228331357240677, -0.02900162898004055, -0.004994015675038099, -0.009203792549669743, -0.023166486993432045, 0.045317769050598145, -0.003346045268699527, -0.021667009219527245, -0.00914283748716116, -0.02833390049636364, -0.019411075860261917, 0.009200246073305607, -0.04094940051436424, 0.04710729047656059, -0.0016210005851462483, -0.03895532339811325, 0.026808757334947586, -0.048090849071741104, -0.0572456493973732, -0.006631478667259216, -0.000925334170460701, -0.017735350877046585, 0.01927979104220867, 0.030366210266947746, 0.009557412937283516, 0.0020000769291073084, -0.06139111891388893, 0.04388090595602989, -0.032658111304044724, -0.04649002477526665, -0.033647049218416214, 0.010046927258372307, -0.014810770750045776, 0.0012843658914789557, -0.0010298333363607526, -0.008668256923556328, 0.014713449403643608, -0.008315610699355602, 0.009313158690929413, -0.0065596201457083225, 0.052714329212903976, 0.003290259512141347, -0.014696725644171238, 0.05519937723875046, -0.004022170323878527, -0.0582294799387455, -0.04324566572904587, 0.03154326602816582, 0.019394682720303535, 0.0022257473319768906, 0.026730826124548912, -0.032284948974847794, -0.031703222543001175, -0.059158213436603546, -0.0030192064587026834, -0.003613367909565568, -0.010131659917533398, -0.000572135322727263, -0.05379805341362953, 0.008887146599590778, -0.006225710269063711, 0.046565692871809006, -0.05063802748918533, -0.005476535763591528, -0.05660715699195862, 0.0486091673374176, -0.03829791396856308, -0.010238616727292538, -0.021751346066594124, 0.001613958040252328, -0.003930611070245504, 0.03902523219585419, 0.01269499957561493, 0.008266272954642773, -0.013793356716632843, 0.01898019015789032, 0.006995842792093754, 0.05675872787833214, -0.04043646529316902, -0.030013764277100563, 0.0047275652177631855, -0.004871570039540529, 0.003812065115198493, 0.0049777342937886715, -0.04829926788806915, 0.039153967052698135, -0.05422333627939224, 0.00828267727047205, 0.00700200442224741, -0.00012585944205056876, -0.026731666177511215, -0.015214836224913597, 0.013324188068509102, -0.026586303487420082, 0.01129339262843132, -0.021629899740219116, 0.026972496882081032, 0.014919014647603035, -0.007208677940070629, -0.03359227627515793, 0.0032633000519126654, -0.046110011637210846, -0.02468579076230526, -0.011768651194870472, -0.05009417608380318, -0.007320294622331858, -0.04069216176867485, 0.012065300717949867, 0.029913436621427536, -0.027781959623098373, 0.04647417366504669, 0.05521249398589134, 0.01660221442580223, 0.020397473126649857, -0.0004891061689704657, 0.0016758809797465801, 0.0229935459792614, -0.002935713389888406, -0.013249501585960388, -0.004524680320173502, -0.010372987948358059, 0.0064347474835813046, 0.02470860816538334, -0.03820176050066948, -0.019480686634778976, 0.045491017401218414, 0.04663420096039772, -0.003931394312530756, 0.01808224432170391, -0.025455744937062263, -0.03347587212920189, -0.023970285430550575, 0.05001553148031235, -0.0064005376771092415, 0.000546037801541388, 0.03609323501586914, -0.022511154413223267, -0.012762474827468395, 0.03793345391750336, -0.017221352085471153, -0.022131571546196938, -0.015818752348423004, 0.06199096888303757, -0.013308953493833542, -0.05099055543541908, 0.07484615594148636, 0.029942438006401062, 0.0019645290449261665, -0.07206305861473083, 0.0029347832314670086, 0.013368135318160057, -0.020008055493235588, -0.020847201347351074, 0.015054835937917233, -4.026569513371214e-05, -0.03318482264876366, 0.016111642122268677, 0.03077089786529541, -0.02186684124171734, 0.015449834056198597, 0.017937220633029938, -0.015438401140272617, -0.030316494405269623, 0.022262709215283394, 0.035507652908563614, -0.023599503561854362, 0.01339109055697918, -0.013404806144535542, 0.00956734735518694, -0.0037435463164001703, -0.006137089803814888, 0.061618536710739136, -0.0246005617082119, 0.014509845525026321, 0.04655458405613899, 0.0024938611313700676, 0.03666839003562927, -0.017548097297549248, -0.014569542370736599, 0.016397750005126, -0.02838776633143425, -0.04749686270952225, -0.03740449249744415, 0.030479133129119873, 0.0019100697245448828, 0.025343136861920357, -0.009303459897637367, 0.042871810495853424, 0.026376422494649887, -0.002586937276646495, -0.007373747881501913, -0.03877050802111626, -0.003174197394400835, -0.04507330432534218, -0.00909251905977726, 0.009302069433033466, -0.030095228925347328, -0.03783995658159256, -0.006974209565669298, -0.013606410473585129, -0.043409302830696106, -0.01772581785917282, 0.019148260354995728, -0.02573450841009617, -0.025256812572479248, 0.003400270128622651, 0.027225947007536888, -0.03720349818468094, -0.032597895711660385, -0.004450630396604538, -0.027302315458655357, 0.019462551921606064, 0.012475120835006237, -0.00657357694581151, -0.008326292037963867, 0.002250324236229062, 0.011513814330101013, 0.0006228888523764908, -0.008103828877210617, -0.03953871503472328, 0.00733115104958415, -0.05350207909941673, -0.02964555099606514, -0.012352997437119484, 0.043198518455028534, 0.02552127093076706, -0.04597519338130951, -0.0037471731193363667, 0.028404807671904564, 0.0030231860000640154, -0.008308717049658298, 0.008318361826241016, 0.061654333025217056, -0.04161403700709343, 0.0015875528333708644, -0.004645364359021187, 0.03962548077106476, -0.032877810299396515, -0.025613442063331604, -0.019727205857634544, -0.023986879736185074, 0.020358074456453323, -0.005011045839637518, 0.03362441435456276, 0.05442076176404953, 0.046240996569395065, 0.04459458217024803, 0.007982725277543068, -0.014716461300849915, 0.002342555206269026, 0.009994135238230228, 0.02396298386156559, -0.0019359892467036843, 0.027733245864510536, -0.042530808597803116, 0.053218305110931396, 0.04840227961540222, 0.002224970841780305, 0.0011077194940298796, 0.0005240299506112933, -0.017830369994044304, -0.027541643008589745, 0.008624293841421604, 0.00762200728058815, 0.04232710599899292, 0.023633252829313278, -0.02407851256430149, -0.0568704828619957, 0.004510681610554457, -0.033437926322221756, 0.0023134576622396708, -0.04863694682717323, -0.031568676233291626, 0.007963467389345169, -0.042747627943754196, 0.026453932747244835, -0.018390534445643425, -0.01386663131415844, 0.014878060668706894, 0.0403495617210865, -0.003430664539337158, 0.0014117551036179066, 0.020549247041344643, 0.02432812750339508, 0.0113255325704813, -0.04853518679738045, -0.03414291515946388, 0.0019814965780824423, 0.012296033091843128, -0.04255560040473938, -0.03599728271365166, 0.0036634462885558605, 0.05895562842488289, 0.028603585436940193, 0.0008844234398566186, -0.0762738510966301, -0.0017514516366645694, 0.016592558473348618, 0.005888011306524277, -0.05738897994160652, 0.03623199090361595, -0.032902125269174576, 0.04457772150635719, -0.0230505783110857, -0.016160758212208748, -0.004824440460652113, 0.02999410405755043, 0.038352616131305695, 0.00017660597222857177, -0.010508568957448006, 0.0031902086921036243, 0.04451492428779602, -0.03034917451441288, 0.052089642733335495, 0.013947773724794388, 0.021793602034449577, -0.004459907300770283, -0.0007224189466796815, -0.02526271902024746, 0.04587530344724655, 0.03488826006650925, 0.009342048317193985, 0.01853177323937416, 0.056532591581344604, 0.04812602698802948, -0.03489607200026512, 0.01585385948419571, 0.04406724497675896, 0.004784525837749243, -0.0036633205600082874, -0.025326933711767197, -0.04339601844549179, 0.023589298129081726, -0.0248277485370636, -0.02178478240966797, -0.0016884031938388944, -0.020846420899033546, -0.00822730828076601, -0.03123786672949791, -0.03871801495552063, 0.034048307687044144, -0.017663970589637756, -0.05804275721311569, -0.02346763014793396, 0.011297456920146942, 0.04534328356385231, 0.007398310583084822, 0.03553539514541626, 0.0013193663908168674, 0.04667842760682106, 0.03689751774072647, 0.020442551001906395, 0.00926455482840538, 0.004426733590662479, 0.03681963309645653, -0.03879735618829727, 0.01239302009344101, -0.01508218701928854, 0.0077891722321510315, 0.021021490916609764, -0.015442097559571266, -0.0052572558633983135, 0.04734678566455841, 0.006075071636587381, -0.045133914798498154, 0.008460836485028267, -0.0315684974193573, -0.006100855302065611, -0.04137921333312988, 0.02807766944169998, -0.035339854657649994, 0.019240401685237885, 0.03634032607078552, -0.02448028139770031, -0.03130987286567688, 0.016819342970848083, -0.016222238540649414, 0.0016172882169485092, 0.0396377332508564, 0.03976257145404816, -0.00381217198446393, 0.06696520000696182, 0.04379412531852722, 0.02502945065498352, 0.002153696957975626, 0.020416082814335823, -0.011249858886003494, -0.019023830071091652, 0.030556993559002876, 0.019311854615807533, 0.03932420164346695, 0.010259040631353855, -0.03842904046177864, 0.043953947722911835, 0.027475617825984955, -0.005606621038168669, -0.014222182333469391, -0.0017597274854779243, 0.0075559718534350395, -0.04506521672010422, -0.010369732975959778, 0.06529021263122559, -0.003217518562451005, 0.023459741845726967, -0.026417002081871033, -0.015941420570015907, -0.03509294614195824, -0.008368252776563168, 0.02599460631608963, 0.017187893390655518, -0.016340147703886032, -0.028177667409181595, -0.01229778490960598, -0.04857286065816879, -0.021872399374842644, -0.03440426290035248, -0.0025316765531897545, -0.038521211594343185, 0.03304896131157875, 0.0030540800653398037, 0.054330531507730484, 0.03401634097099304, -0.005600399803370237, -0.0051809679716825485, 0.06439969688653946, 0.06836098432540894, 0.00885140709578991, 0.05384518578648567, 0.0404776930809021, 0.02842104248702526, -0.024049455299973488, 0.009244122542440891, -0.00021110515808686614, -0.01135240402072668, 0.013103201985359192, -0.009407640434801579, 0.01854540780186653, 0.005060444585978985, -0.05800926312804222, -3.8918809877941385e-05, 0.04080004617571831, 0.017990078777074814, -0.007093684282153845, -0.029331205412745476, -0.030865702778100967, -0.07334692031145096, 0.027169372886419296, -0.045729152858257294, 0.015574313700199127, 0.015823857858777046, 0.022122327238321304, -0.033459343016147614, -0.04724651947617531, 0.2626228332519531, 0.05277923867106438, 0.04015888646245003, 0.0350978709757328, 0.03483027219772339, 0.025966042652726173, 0.049298547208309174, -0.023836005479097366, 0.03456742316484451, -0.04452340304851532, 0.031727567315101624, -0.007534203119575977, 0.01060341577976942, 0.03119100071489811, -0.005304711405187845, 0.027239562943577766, -0.039338644593954086, -0.02026192843914032, -0.0023667537607252598, -0.04251512512564659, 0.00017693523841444403, 0.014265359379351139, -0.0021933012176305056, 0.055381499230861664, 0.020931437611579895, -0.009761697612702847, 0.023224178701639175, -0.008208560757339, -0.0089110704138875, -0.02229228988289833, 0.021724682301282883, -0.030045578256249428, 0.02921447530388832, -0.03522120788693428, -0.030237914994359016, 0.012127320282161236, -0.0028700893744826317, -0.05840742960572243, -0.02650684304535389, 0.03367868438363075, -0.011015396565198898, -0.0316259041428566, 0.007466083858162165, 0.0014101678971201181, -0.0048993113450706005, 0.044738661497831345, 0.024048179388046265, 0.02621551789343357, 0.05526164919137955, -0.07247721403837204, 0.023544467985630035, -0.03801937401294708, -0.016815252602100372, -0.011753210797905922, -0.05941188335418701, -0.019579168409109116, -0.018635282292962074, -0.021943064406514168, -0.014931024983525276, 0.041052207350730896, 0.008150250650942326, -0.011067152954638004, -0.0074214087799191475, 0.017286166548728943, -0.03416479006409645, 0.0393989272415638, -0.006427234970033169, -0.0004586417635437101, -0.047385480254888535, -0.0022232444025576115, 0.02254735492169857, 0.017367981374263763, -0.017903167754411697, 0.006815038155764341, 0.03894773870706558, 0.04637732729315758, -0.017097413539886475, 0.03237413987517357, 0.000119255535537377, -0.03064127266407013, -0.02058403007686138, 0.00892686564475298, -0.02169344760477543, 0.005176521372050047, -0.003115862375125289, 0.061825837939977646, -0.05166242644190788, -0.0013110656291246414, -0.0051302541978657246, 0.020254211500287056, -0.005953630432486534, 0.05231062322854996, 0.0026711185928434134, -0.018276430666446686, 0.03588379546999931], metadata={}, excluded_embed_metadata_keys=[], excluded_llm_metadata_keys=[], relationships={}, metadata_template='{key}: {value}', metadata_separator='\\n', text_resource=MediaResource(embeddings=None, data=None, text='We invite everyone in the community to propose additional or alternative models and improvements.\\n\\nImplementations of model classes must satisfy the python Callable interface. The \\\\_\\\\_call\\\\_\\\\_ method must accept an iterator over page objects, and produce another iterator over the page objects which were augmented with the additional features predicted by the model, by extending the provided PagePredictions data model accordingly.\\n\\n## 4 Performance\\n\\nIn this section, we establish some reference numbers for the processing speed of Docling and the resource budget it requires. All tests in this section are run with default options on our standard test set distributed with Docling, which consists of three papers from arXiv and two IBM Redbooks, with a total of 225 pages. Measurements were taken using both available PDF backends on two different hardware systems: one MacBook Pro M3 Max, and one bare-metal server running Ubuntu 20.04 LTS on an Intel Xeon E5-2690 CPU. For reproducibility, we fixed the thread budget (through setting OMP NUM THREADS environment variable ) once to 4 (Docling default) and once to 16 (equal to full core count on the test hardware). All results are shown in Table 1.\\n\\nIf you need to run Docling in very low-resource environments, please consider configuring the pypdfium backend. While it is faster and more memory efficient than the default docling-parse backend, it will come at the expense of worse quality results, especially in table structure recovery.\\n\\nEstablishing GPU acceleration support for the AI models is currently work-in-progress and largely untested, but may work implicitly when CUDA is available and discovered by the onnxruntime and\\n\\ntorch runtimes backing the Docling pipeline. We will deliver updates on this topic at in a future version of this report.\\n\\nTable 1: Runtime characteristics of Docling with the standard model pipeline and settings, on our test dataset of 225 pages, on two different systems. OCR is disabled. We show the time-to-solution (TTS), computed throughput in pages per second, and the peak memory used (resident set size) for both the Docling-native PDF backend and for the pypdfium backend, using 4 and 16 threads.\\n', path=None, url=None, mimetype=None), image_resource=None, audio_resource=None, video_resource=None, text_template='{metadata_str}\\n\\n{content}'),\n",
       " Document(id_='64828f28-a305-4e93-934e-cb1f6ff9ecdf', embedding=[0.03356887772679329, -0.01869959570467472, 0.008944334462285042, 0.016075655817985535, -0.023810334503650665, 0.022608021274209023, 0.004973323084414005, -0.030268102884292603, -0.010539267212152481, 0.041352298110723495, -0.010921274311840534, -0.005618666298687458, 0.005313820671290159, -0.011046692728996277, -0.010090733878314495, 0.017087623476982117, -0.010702209547162056, -0.06149047613143921, -0.04086916893720627, 0.005488282069563866, -0.012472426518797874, 0.0018676409963518381, -0.04597780108451843, -0.005312926601618528, -0.02602517046034336, 0.03440634533762932, 0.01362866535782814, 0.03328397125005722, 0.0870116576552391, 0.03140997886657715, -0.02354004792869091, -0.011737258173525333, 0.03076995722949505, -0.0342651903629303, -0.0062181646935641766, -0.025838786736130714, -0.010838000103831291, 0.007762625813484192, -0.012788144871592522, -0.06945125013589859, 0.03746051713824272, -0.0498788021504879, 0.03419503942131996, -0.052902791649103165, -0.03473081812262535, -0.007040129508823156, -0.005282272584736347, -0.01145640853792429, -0.023254519328475, -0.0780196338891983, -0.004083988256752491, 0.025652451440691948, -0.00879388116300106, -0.011428452096879482, 0.006624109111726284, -0.0018540483433753252, -0.014974669553339481, -0.011212257668375969, -0.055696453899145126, 0.010900423862040043, 0.023973913863301277, 0.004653554875403643, 0.00036488944897428155, -0.04179672524333, -0.005841224920004606, 0.01164809800684452, -0.014054170809686184, 0.009699135087430477, -0.006233362015336752, 0.007679150905460119, -0.04570675268769264, 0.03358391299843788, -0.03892897814512253, -0.026242680847644806, -0.0013333476381376386, -0.013849470764398575, 0.03456210345029831, -0.0015576332807540894, -0.03824950382113457, 0.04330885037779808, 0.03367224335670471, 0.05168946087360382, -0.01551589835435152, 0.02925928868353367, -0.014192712493240833, -0.06972886621952057, 0.023026688024401665, -0.024942751973867416, 0.0031933151185512543, -0.021343493834137917, 0.029129108414053917, 0.0009274493786506355, -0.021240033209323883, 0.01682928204536438, 0.02282528765499592, 0.0443301796913147, -0.0068831113167107105, 0.04082362726330757, -0.0012682151282206178, 0.006075785495340824, 0.0026349960826337337, 0.04045567661523819, -0.004672281444072723, 0.023127077147364616, -0.054330892860889435, 0.009006829932332039, 0.02790636569261551, -0.05272412672638893, 0.0021627279929816723, -0.015274224802851677, 0.010584523901343346, 0.008934395387768745, -0.056611258536577225, -0.006178445182740688, 0.02171691693365574, -0.008907828480005264, 0.005297817289829254, 0.0004284369933884591, -0.012376743368804455, 2.0015968402731232e-05, 0.012926258146762848, -0.024470629170536995, -0.024153435602784157, -0.013185965828597546, -0.019364038482308388, -0.041485197842121124, -0.011369721032679081, 0.0381237156689167, -0.029914872720837593, -0.034921370446681976, 0.0154603635892272, -1.7500085959909484e-05, -0.049225982278585434, 0.024402352049946785, 0.0303365346044302, -0.027789318934082985, 0.022604860365390778, 0.0046127974055707455, 0.0345752015709877, -0.012715131044387817, 0.01893734373152256, -0.06341613084077835, -0.0038183280266821384, 0.09465194493532181, -0.013226968236267567, 0.01914651319384575, -0.002326623536646366, -0.01055439468473196, -0.03120778687298298, 0.026083195582032204, -0.012702720239758492, 0.004932694137096405, 0.037956688553094864, 0.022664304822683334, 0.0006849121418781579, -0.009972693398594856, -0.0008601463050581515, 0.012922353111207485, 0.02407648228108883, 0.024641577154397964, 0.004995191004127264, -0.0036669347900897264, -0.07094218581914902, 0.03118702955543995, -0.06507623940706253, 0.02191336080431938, 0.023988278582692146, -0.046760763972997665, 0.02262674644589424, -0.044295117259025574, 0.0024090546648949385, 0.027049250900745392, -0.053940240293741226, 0.027096372097730637, 0.04650391638278961, 0.03531597554683685, 0.041935909539461136, 0.01283328142017126, 0.016475306823849678, 0.013050454668700695, -0.003744659246876836, 0.016717422753572464, 0.0320337675511837, 0.010757753625512123, 0.02121243253350258, 0.045885030180215836, 0.008577900938689709, -0.04287862405180931, -0.005770410876721144, -0.02315153181552887, -0.021630635485053062, 0.03234610706567764, -0.00605181697756052, 0.035077303647994995, -0.003922628704458475, -0.019442621618509293, -0.03351140767335892, -0.026756230741739273, -0.04588436335325241, -0.04686699062585831, -0.012568417005240917, 0.052057843655347824, 0.01882290095090866, 0.02855403535068035, -0.029941782355308533, -0.0008974389056675136, 0.013865755870938301, 0.03111952915787697, -0.03088502585887909, 0.011976777575910091, 0.04021955281496048, -0.013971041887998581, -0.039666593074798584, 0.013936962932348251, 0.06286714226007462, -0.0014541253913193941, -0.024910761043429375, -0.004199804738163948, 0.037041980773210526, -0.00595862278714776, 0.0025170508306473494, 0.004013336729258299, 0.05234498158097267, -0.02446962147951126, -0.024025388062000275, 0.0038561816327273846, -0.02312404103577137, 0.014475994743406773, 0.026006150990724564, 0.01816415973007679, -0.03387903794646263, 0.03887050226330757, -0.036048196256160736, 0.03562821447849274, 0.05124874785542488, -0.036269351840019226, 0.0524509958922863, 0.05454232916235924, -0.033639390021562576, 0.020227709785103798, -0.027720805257558823, 0.03022805228829384, 0.0030497286934405565, -0.002533059334382415, 0.04015418142080307, 0.031015392392873764, -0.008014899678528309, -0.037868764251470566, -0.022965265437960625, 0.0214772317558527, -0.0677262619137764, 0.027086520567536354, -0.0014870663871988654, -0.02950844168663025, -0.028345366939902306, -0.027360398322343826, 0.028372429311275482, 0.05020349845290184, -0.019739905372262, 0.00929716695100069, -0.014979778788983822, 0.031170371919870377, 0.004563880153000355, 0.008211849257349968, -0.005925050936639309, 0.02766689658164978, 0.00940700899809599, 0.02336278185248375, 0.038043852895498276, -0.017266826704144478, -0.03914916142821312, -0.034271880984306335, -0.04328044876456261, 0.020101141184568405, -0.01645500957965851, 0.007188755553215742, 0.007206234615296125, -0.04025343060493469, 0.0041297003626823425, -0.03636116161942482, 0.023689577355980873, 0.030073853209614754, 0.002019608160480857, -0.014786872081458569, 0.014634005725383759, 0.026528717949986458, -0.05560755729675293, 0.008631870150566101, -0.005499742925167084, 0.05003908649086952, -0.010722358711063862, 0.02342870645225048, -0.016929930076003075, -0.017062285915017128, 0.04871783405542374, -0.0013469781260937452, -0.005671917460858822, 0.007736662868410349, 0.011255468241870403, -0.016979597508907318, -0.023848216980695724, -0.043520551174879074, 0.01554194651544094, -0.012031204998493195, -0.05718836933374405, 0.03625662624835968, 0.015080139972269535, -0.0024165497161448, 0.04323754087090492, -0.00530313141644001, -0.037651315331459045, 0.00869940035045147, 0.01870223507285118, 0.03901558369398117, -0.038030754774808884, 0.06344719976186752, 0.030475670471787453, -0.029227254912257195, -0.02710193395614624, -0.04997475817799568, -0.033947307616472244, 0.002961059333756566, 0.0036916586104780436, -0.046984296292066574, -0.01914147660136223, 0.04503384605050087, -0.005203860811889172, -0.05138980969786644, 0.007938898168504238, -0.01326003484427929, -0.05441313609480858, 0.0065213474445044994, -0.02555048279464245, 0.03247946500778198, -0.007172591984272003, 0.01763642020523548, -0.05448933318257332, 0.036041826009750366, -0.02271445468068123, 0.03125273808836937, 0.015871744602918625, 0.009331696666777134, 0.00964609906077385, 0.03306877240538597, -0.01799481175839901, -0.011256620287895203, 0.010320212692022324, -0.02594008296728134, 0.004884245805442333, -0.009131080470979214, -0.028704587370157242, 0.03393632173538208, 0.036453306674957275, 0.06102071702480316, 0.03947003185749054, 0.005440417677164078, -0.04620208963751793, -0.014154319651424885, -0.008206235244870186, -0.012864993885159492, -0.022441480308771133, 0.0358014851808548, 0.015615707263350487, -0.00528511218726635, 0.008161790668964386, -0.023830801248550415, -0.013981209136545658, 0.028828473761677742, 0.04531135410070419, -0.061685752123594284, 0.02252810448408127, 0.03526431694626808, -0.021388275548815727, 0.008295777253806591, 0.0025329936761409044, -0.038669947534799576, 0.06181119754910469, -0.0023853364400565624, 0.044697847217321396, 0.024181708693504333, 0.031011728569865227, 0.020041024312376976, -0.018075013533234596, 0.0448952354490757, -0.009857001714408398, 0.032300204038619995, -0.0199944619089365, 0.00553607614710927, -0.033380940556526184, -0.031363118439912796, 0.0022873992566019297, 0.033899612724781036, -0.04626796022057533, -0.001414421247318387, -0.03519530966877937, -0.008136417716741562, 0.044849757105112076, 0.029760215431451797, 0.00963129848241806, 0.016419539228081703, 0.04796659201383591, -0.007706002797931433, 0.03125271573662758, 0.004130109678953886, 0.03177405148744583, -0.002718504751101136, -0.02488897554576397, 0.051495444029569626, -0.020809056237339973, 0.03061673417687416, -0.015547925606369972, -0.015981681644916534, -0.03167767450213432, 0.0117354616522789, -0.0296495258808136, -0.0007561661768704653, -0.03471938520669937, -0.02741219289600849, 0.01638144627213478, 0.03413553535938263, 0.02310461923480034, 0.007424038369208574, -0.054583873599767685, 0.021284891292452812, 0.030002987012267113, 0.001943117706105113, -0.006404939107596874, -0.004454138223081827, 0.003167132381349802, -0.015427159145474434, -0.04379937797784805, -0.027560237795114517, -0.03971455991268158, -0.029644597321748734, -0.05535692349076271, 0.05538824200630188, 0.03735410049557686, 0.013482816517353058, -0.005857528652995825, -0.0514686219394207, 0.005326482001692057, 0.008824403397738934, 0.0011020642705261707, -0.02136029116809368, 0.02971704676747322, -0.00979089830070734, 0.016628334298729897, 0.03710357844829559, 0.032466281205415726, -0.009437298402190208, 0.004119974095374346, -0.05290629342198372, 0.0408782996237278, -0.06408632546663284, -0.03764255344867706, -0.005448604468256235, -0.02454366162419319, -0.012566023506224155, 0.0004335363919381052, 0.013740058057010174, -0.009247740730643272, 0.008154066279530525, 0.03586738184094429, -0.00745645584538579, -0.03888624906539917, 0.026566406711935997, 0.007746931631118059, -0.011336799710988998, 0.03569270297884941, 0.0011998593108728528, 0.00755822379142046, 0.025543032214045525, 0.013747939839959145, -0.03701547533273697, 0.03206201642751694, 0.003620891133323312, 0.04004891216754913, -0.02891603484749794, -0.01494346559047699, -0.022327139973640442, -0.014575849287211895, 0.050291676074266434, -0.0047026509419083595, -0.00325629860162735, -0.0005067457095719874, -0.03948575258255005, -0.02884296514093876, 0.028612865135073662, -0.04152125120162964, 0.03153437376022339, 0.007279025856405497, -0.03700613975524902, 0.03854265436530113, -0.04273447021842003, -0.03494974970817566, -0.04070044308900833, -0.020174119621515274, -0.019041918218135834, 0.03324342146515846, 0.04072757810354233, 0.026674829423427582, -0.03272382915019989, -0.028175126761198044, 0.0303319301456213, -0.01944613642990589, -0.029769735410809517, -0.0218658410012722, 0.018809013068675995, -0.006597654428333044, 0.006496703252196312, -0.004556724335998297, -0.020651914179325104, -0.011606814339756966, 0.0017239204607903957, -0.017530659213662148, 0.011733517050743103, 0.02076495997607708, -0.011074528098106384, -0.04175882786512375, 0.08129726350307465, 0.028407327830791473, -0.03772525489330292, -0.02723047323524952, 0.040704142302274704, -0.006186231505125761, 0.03441637009382248, 0.03344311565160751, -0.00028446331270970404, -0.02296755462884903, -0.04161461070179939, -0.004328766372054815, -0.01301251258701086, -0.02285510115325451, -0.008342282846570015, -0.05674118176102638, -0.004274982493370771, 0.024482764303684235, 0.034037936478853226, -0.07454115897417068, -0.002604228211566806, -0.041734594851732254, 0.007953130640089512, -0.029400978237390518, 0.0005569606437347829, -0.026948094367980957, 0.008378568105399609, 0.0017113432986661792, 0.01906394585967064, 0.002708912594243884, 0.0005482344422489405, -0.03292949125170708, 0.0031632082536816597, -0.002951956819742918, 0.0742887556552887, -0.057416606694459915, -0.020775102078914642, 0.019886525347828865, 0.0009906282648444176, -0.0020331870764493942, -0.00013801395834889263, -0.07052489370107651, 0.06055290997028351, -0.0599839985370636, 0.00021204030781518668, -0.011855020187795162, 0.005899679847061634, -0.03159143775701523, -0.015390986576676369, 0.023073656484484673, -0.005513106472790241, -0.007218245882540941, -0.021646391600370407, 0.01634322479367256, 0.02234679087996483, 0.029380422085523605, -0.04368072375655174, 0.007721817120909691, -0.048347461968660355, -0.01288624107837677, -0.015048319473862648, -0.0418420284986496, -0.012312998063862324, -0.014105282723903656, 0.006203852593898773, -0.007200463209301233, -0.038960620760917664, 0.043529655784368515, 0.0665038675069809, 0.02167845331132412, 0.031912911683321, -0.03594033047556877, 0.001925371470861137, 0.043445926159620285, -0.019488703459501266, -0.03091336041688919, -0.02379360795021057, -0.0006278070504777133, 0.011560289189219475, 0.009190062992274761, -0.03028630092740059, -0.0065277209505438805, 0.026511186733841896, 0.052704423666000366, -0.01916317269206047, 0.0037882651668041945, -0.004756910260766745, -0.053732842206954956, -0.0025826897472143173, 0.0541495606303215, 0.02040252275764942, 0.01335057057440281, 0.045225873589515686, -0.028947679325938225, 0.0012549813836812973, 0.02351621724665165, -0.02740008756518364, -0.011426903307437897, -0.01166476309299469, 0.035487428307533264, -0.038821395486593246, -0.04577505961060524, 0.07651294767856598, 0.00648367777466774, -0.004276560619473457, -0.07432034611701965, -0.011015131138265133, -0.03511928394436836, -0.03529956564307213, -0.0035599886905401945, 0.025531146675348282, -0.02099091373383999, -0.017104800790548325, -0.01157369650900364, -0.0008620685548521578, -0.00954615231603384, -0.007213840261101723, 0.0018294377950951457, -0.01110156811773777, -0.021671507507562637, 0.03962387144565582, 0.043186627328395844, -0.010049616917967796, -0.011182727292180061, -0.023501558229327202, 0.02707134373486042, -0.018689217045903206, 0.016572104766964912, 0.03817684203386307, -0.01477749738842249, 0.04633369669318199, 0.017616871744394302, 0.011477933265268803, 0.05432112514972687, -0.014279470779001713, -0.04365331307053566, 0.006853331346064806, -0.02516143210232258, -0.05572007969021797, -0.010623357258737087, 0.03939460963010788, 0.002847854048013687, 0.0656585544347763, -0.015372135676443577, 0.021579522639513016, 0.03797401115298271, 0.023335978388786316, 0.0024564217310398817, -0.025664420798420906, -0.006642703898251057, -0.035132765769958496, 0.00791057851165533, -0.02381877973675728, -0.0336303748190403, -0.06646835803985596, 0.015588009729981422, -0.026980239897966385, -0.03240414708852768, -0.0013541689841076732, 0.0218326635658741, -0.045918796211481094, -0.025914840400218964, -0.010945421643555164, 0.0167496707290411, -0.026205575093626976, -0.039554402232170105, 0.009719030000269413, -0.03336942940950394, 0.016220537945628166, 0.005459851119667292, -0.006231559906154871, -0.02928876131772995, 0.014982546679675579, 0.018236780539155006, -0.029525678604841232, -0.01977084018290043, -0.037310000509023666, -0.02311098948121071, -0.0492822639644146, 0.004317727871239185, -0.017289290204644203, 0.012567398138344288, 0.026818621903657913, -0.040229685604572296, -0.02007739059627056, 0.02379860356450081, -0.0021821591071784496, -0.012577797286212444, -0.005091534927487373, 0.03289361670613289, -0.03150809183716774, -0.010611296631395817, -0.0076568047516047955, 0.02952643483877182, -0.019780045375227928, -0.01606808975338936, -0.01699008420109749, 0.023847978562116623, 0.031811535358428955, 0.017013119533658028, -0.006989404559135437, 0.06240472570061684, 0.037186283618211746, 0.03126790001988411, 0.027642063796520233, -0.009271855466067791, -0.0024934604298323393, 0.02154170349240303, 0.01611294597387314, -0.008580751717090607, 0.022545434534549713, -0.01656326837837696, 0.04775777831673622, 0.05770108476281166, -0.008900145068764687, 0.020047137513756752, -0.004935466218739748, 0.02731177769601345, -0.03455599024891853, -0.009757575578987598, 0.010791070759296417, 0.03275440260767937, 0.042225394397974014, -0.05629570037126541, -0.032623354345560074, 0.020816311240196228, -0.013313126750290394, -0.013292692601680756, -0.04960569739341736, -0.04436413198709488, 0.01684846170246601, -0.05210772529244423, 0.03348572924733162, -0.015723221004009247, 0.006771115120500326, 0.02661447785794735, 0.033507976680994034, 0.0041005429811775684, 0.013669021427631378, 0.03529377281665802, 0.012757654301822186, -0.008537786081433296, -0.06683818995952606, -0.03395438566803932, -0.0022501677740365267, 0.028330184519290924, -0.024146363139152527, -0.05117538198828697, 0.026815708726644516, 0.07604625076055527, 0.015122065320611, 0.01415174175053835, -0.06411682069301605, -0.006291911005973816, 0.023733729496598244, 0.0041609168983995914, -0.04674402251839638, 0.037230659276247025, -0.03359972685575485, 0.020107543095946312, -0.0020736202131956816, -0.041443776339292526, -0.028433144092559814, -0.0028654364868998528, 0.057208988815546036, 0.0011535158846527338, -0.04175954312086105, 0.021261412650346756, 0.04676437005400658, -0.00410139374434948, 0.03490333631634712, 0.023915410041809082, -0.016300667077302933, 0.002221801085397601, -0.0044855112209916115, -0.000995886162854731, 0.037659816443920135, 0.012733125127851963, 0.0008305287337861955, -0.023722121492028236, 0.06912225484848022, 0.015298358164727688, -0.009743813425302505, 0.013129066675901413, 0.0176073107868433, -0.0070105064660310745, -0.021533720195293427, -0.0004516057961154729, -0.04047311097383499, 0.0023687267675995827, -0.002177910879254341, -0.015941346064209938, 0.013225642964243889, -0.01778334751725197, -0.029328603297472, -0.03317220136523247, -0.024451322853565216, 0.05454283580183983, -0.018657367676496506, -0.06050754338502884, -0.025908445939421654, -0.0026036265771836042, 0.04493965581059456, 0.01003200002014637, 0.014823834411799908, -0.0011708448873832822, 0.03914231061935425, 0.0194301288574934, 0.0378260612487793, 0.01338188722729683, 0.001808595727197826, 0.010575388558208942, -0.032047443091869354, 0.008507835678756237, -0.028069520369172096, -0.0019502559443935752, 0.01130757573992014, -0.035900503396987915, -0.046297140419483185, 0.03237617760896683, 0.012634375132620335, -0.04036913439631462, -0.01750010997056961, -0.00754547817632556, -0.007496652193367481, -0.009850661270320415, 0.022691790014505386, -0.033569321036338806, 0.026452913880348206, 0.0395934097468853, -0.03391873463988304, -0.056461602449417114, 0.009641200304031372, -0.015524131245911121, -0.009215263649821281, 0.04236498475074768, 0.04190291464328766, -0.006665812339633703, 0.08189084380865097, 0.022101879119873047, -0.0058607980608940125, 0.013568386435508728, -0.04000096395611763, 0.00997205451130867, -0.008779056370258331, 0.017559165135025978, -0.0076773324981331825, 0.0021677848417311907, 0.012337487190961838, -0.03916536644101143, 0.04061249643564224, 0.02556113712489605, 0.005787169560790062, -0.01885668747127056, 0.01277252845466137, 0.020311083644628525, -0.0433809719979763, -0.016248995438218117, 0.0641879290342331, 0.01573421247303486, 0.010309761390089989, -0.005751425866037607, -0.01640450395643711, -0.024289995431900024, -0.01667616330087185, 0.012932687066495419, 0.044831834733486176, -0.03368504345417023, -0.03834673762321472, -0.006712680216878653, -0.0468270368874073, -0.0074121360667049885, -0.02203940786421299, -0.009703648276627064, -0.012754596769809723, 0.03442329913377762, -0.005452032200992107, 0.04807959869503975, 0.019934281706809998, -0.00941278226673603, -0.02339031733572483, 0.05424214154481888, 0.05615721270442009, -0.004494074732065201, 0.036938056349754333, 0.0416521281003952, 0.034656986594200134, -0.023595495149493217, 0.026738988235592842, 0.01028552744537592, -0.035460297018289566, 0.0006474017282016575, -0.04608236998319626, 0.030602991580963135, -0.003558078780770302, -0.041003819555044174, 0.027336833998560905, 0.061767272651195526, 0.03513889014720917, -0.016358230262994766, -0.011633623391389847, -0.0033625748474150896, -0.0832989364862442, -0.006466086953878403, -0.04484410211443901, 0.005767159629613161, -0.009199138730764389, 0.004734979942440987, -0.0157227274030447, -0.07251864671707153, 0.31327828764915466, 0.036955781280994415, 0.044929128140211105, 0.013193259947001934, -0.00760882580652833, 0.008112208917737007, 0.01730361208319664, -0.030182339251041412, 0.057084064930677414, -0.05185522511601448, 0.032296210527420044, -0.003949443809688091, 0.010878642089664936, 0.033192358911037445, -0.02925066463649273, 0.04022352397441864, -0.02432393841445446, -0.010701604187488556, 0.0033993993420153856, -0.033879831433296204, -0.01705288700759411, 0.007413771003484726, 0.010836071334779263, 0.043186005204916, 0.011278768070042133, 0.0009806209709495306, 0.01235993206501007, 0.008467995561659336, 0.015152573585510254, -0.012601178139448166, 0.034132711589336395, -0.008427158929407597, 0.03586059436202049, -0.035044871270656586, -0.02721318230032921, 0.021730467677116394, 0.00027466571191325784, -0.07491374015808105, -0.01812731847167015, 0.05637631192803383, -0.0023740753531455994, -0.03381515294313431, 0.008754357695579529, -0.023033851757645607, -0.034701380878686905, 0.06926058977842331, 0.0005457460647448897, 0.024966081604361534, 0.048012204468250275, -0.061611026525497437, 0.011175704188644886, -0.005591505207121372, -0.017631515860557556, -0.023839373141527176, -0.07569924741983414, -0.006499273702502251, -0.039953529834747314, -0.020400479435920715, 0.0068731908686459064, 0.024118078872561455, -0.003544324543327093, -0.017607953399419785, 0.008690651506185532, 0.013137147761881351, -0.0452282540500164, 0.049583520740270615, -0.017813721671700478, 0.03957224637269974, -0.061618391424417496, -0.022393696010112762, 0.039768677204847336, -0.015296696685254574, -0.01288633979856968, 0.010584171861410141, 0.018951836973428726, 0.03710673004388809, -0.005043771117925644, 0.014993329532444477, -0.0017053846968337893, -0.031010063365101814, -0.032168611884117126, 0.025399306789040565, -0.016165876761078835, -0.002005113987252116, -0.01977257989346981, 0.042928460985422134, -0.04014020785689354, -0.008192108012735844, -0.014267249964177608, 0.0247122161090374, -0.005623620934784412, 0.05263626202940941, -0.0019898719619959593, -0.006774367298930883, 0.0437609925866127], metadata={}, excluded_embed_metadata_keys=[], excluded_llm_metadata_keys=[], relationships={}, metadata_template='{key}: {value}', metadata_separator='\\n', text_resource=MediaResource(embeddings=None, data=None, text=\"\\n| CPU                         | Thread budget   | native backend   | native backend   | native backend   | pypdfium backend   | pypdfium backend   | pypdfium backend   |\\n|-----------------------------|-----------------|------------------|------------------|------------------|--------------------|--------------------|--------------------|\\n|                             |                 | TTS              | Pages/s          | Mem              | TTS                | Pages/s            | Mem                |\\n| Apple M3 Max                | 4               | 177 s 167 s      | 1.27 1.34        | 6.20 GB          | 103 s 92 s         | 2.18 2.45          | 2.56 GB            |\\n| (16 cores) Intel(R) E5-2690 | 16 4 16         | 375 s 244 s      | 0.60 0.92        | 6.16 GB          | 239 s 143 s        | 0.94 1.57          | 2.42 GB            |\\n\\n## 5 Applications\\n\\nThanks to the high-quality, richly structured document conversion achieved by Docling, its output qualifies for numerous downstream applications. For example, Docling can provide a base for detailed enterprise document search, passage retrieval or classification use-cases, or support knowledge extraction pipelines, allowing specific treatment of different structures in the document, such as tables, figures, section structure or references. For popular generative AI application patterns, such as retrieval-augmented generation (RAG), we provide quackling , an open-source package which capitalizes on Docling's feature-rich document output to enable document-native optimized vector embedding and chunking. It plugs in seamlessly with LLM frameworks such as LlamaIndex [8]. \", path=None, url=None, mimetype=None), image_resource=None, audio_resource=None, video_resource=None, text_template='{metadata_str}\\n\\n{content}'),\n",
       " Document(id_='2aef069e-7de8-46f1-9102-d3ce84758427', embedding=[0.020938752219080925, -0.037919990718364716, 0.006407110020518303, -0.0020286040380597115, 0.007731385994702578, 0.012078146450221539, 0.03105098009109497, -0.005286520812660456, 0.0026826877146959305, 0.045374546200037, 0.014174307696521282, -0.01603424921631813, 0.015833130106329918, -0.024334533140063286, 0.012546864338219166, 0.03234352171421051, 0.016554713249206543, -0.019983651116490364, -0.07224210351705551, -0.004334776662290096, 0.009194925427436829, 0.003931037615984678, -0.08592154830694199, -0.031139856204390526, -0.025309301912784576, 0.031716518104076385, 0.023415038362145424, -0.017873292788863182, 0.08470722287893295, 0.04376404732465744, -0.008392361924052238, -0.04924042895436287, 0.028518598526716232, 0.015230934135615826, -0.048676133155822754, -0.01261703111231327, 0.017157787457108498, -0.003301989519968629, -0.009490578435361385, -0.023947855457663536, -0.00696569075807929, -0.03276906907558441, 0.014453751966357231, -0.06929340213537216, -0.05577350780367851, 0.012675276957452297, 0.04064589738845825, -0.015387832187116146, -0.030139924958348274, -0.03058016113936901, 0.010630786418914795, 0.010436282493174076, -0.002911532064899802, -0.025317277759313583, -0.0011821315856650472, -0.0003466036869212985, 0.007060439791530371, -0.0012142874766141176, -0.06121263653039932, 0.0008628193172626197, 0.01651643216609955, -0.009009245783090591, 0.00040328456088900566, -0.05772126466035843, 0.0033015133813023567, 0.012606434524059296, -0.02100682258605957, -0.020039202645421028, -0.0037637241184711456, 0.020028790459036827, -0.031020179390907288, -0.005581959616392851, -0.027079829946160316, -0.016832606866955757, -0.05484245717525482, -0.043809741735458374, 0.017605284228920937, 0.022693395614624023, -0.047062862664461136, 0.04807019978761673, -0.01730199158191681, 0.020682862028479576, 0.013316182419657707, -0.007687345612794161, -0.026083683595061302, -0.06351437419652939, 0.07779368758201599, -0.017425458878278732, 0.034485045820474625, -0.003910480998456478, -0.016603227704763412, 0.02655637264251709, -0.004999036900699139, 0.02095072716474533, -0.004201641771942377, 0.04277030751109123, -0.017488081008195877, 0.05031212419271469, 0.02492053061723709, 0.04392906278371811, 0.001391956233419478, 0.07936250418424606, -0.007767819799482822, 0.022604530677199364, -0.040355782955884933, 0.011905044317245483, 0.019723374396562576, -0.05515226349234581, -0.015481770969927311, -0.04063018783926964, -0.009431419894099236, -0.0029356915038079023, -0.0036263337824493647, -0.02906828373670578, -0.0055596958845853806, 0.015233417972922325, -0.004986210726201534, 0.008656610734760761, -0.011454476974904537, 0.0004923584638163447, -0.010159062221646309, -0.026307567954063416, -0.017158757895231247, -0.03230337053537369, 0.003575956216081977, -0.005424116272479296, 0.0001529492874396965, 0.035358723253011703, -0.039352718740701675, -0.028599528595805168, -0.004698196891695261, -0.039782118052244186, -0.05466878041625023, 0.06636208295822144, -0.020807653665542603, -0.028432678431272507, 0.03169402480125427, 0.011077633127570152, 0.03163861110806465, -0.012512444518506527, 0.015495238825678825, -0.01159536000341177, -0.04290295019745827, 0.05611518770456314, -0.017440516501665115, 0.0033593352418392897, -0.007871991954743862, -0.053835976868867874, -0.0719870775938034, 0.04327726736664772, -0.021705487743020058, 0.0024801832623779774, 0.024346817284822464, 0.02821015939116478, -0.06300755590200424, -0.0013883160427212715, -0.02378258667886257, 0.021817337721586227, 0.004085563123226166, 0.02437259815633297, 0.02060203067958355, 0.010279970243573189, -0.0390680655837059, 0.001937778084538877, -0.015224034897983074, 0.036076709628105164, -0.0034907551016658545, -0.00603775167837739, 0.018647940829396248, -0.04186324030160904, 0.02194460667669773, 0.025769557803869247, 0.0020099037792533636, 0.023101843893527985, 0.07152275741100311, 0.04369058087468147, 0.057920850813388824, 0.0034226884599775076, 0.0298762284219265, 0.00735403411090374, -0.013346085324883461, -0.01036303211003542, 0.0010768657084554434, 0.015588032081723213, -0.010571281425654888, 0.038843050599098206, -0.009507395327091217, -0.0021625685039907694, 0.013949264772236347, -0.0060924747958779335, -0.004009666386991739, 0.055512022227048874, -0.01998656429350376, 0.03440362960100174, -0.025665882974863052, -0.013353833928704262, -0.06298498064279556, 0.010594467632472515, 0.007033310830593109, -0.04283524304628372, -0.03719959408044815, 0.03470870852470398, 0.02384304255247116, 0.020418943837285042, -0.06442144513130188, 0.00023874876205809414, 0.011021558195352554, 0.014537754468619823, -0.04405629262328148, 0.030410699546337128, 0.028322942554950714, 0.005774554330855608, -0.020373810082674026, -0.029884159564971924, 0.044894877821207047, -0.035011615604162216, -0.029612218961119652, 0.0056511955335736275, -0.015530873090028763, -0.019481873139739037, 0.010306843556463718, -0.0214850977063179, 0.030664615333080292, -0.0056022279895842075, -0.029179470613598824, 0.01674557663500309, 0.006898438557982445, 0.03911422938108444, 0.02199419215321541, 0.02429266832768917, -0.04254961386322975, 0.04613258317112923, -0.017042631283402443, 0.06386695057153702, 0.03589847311377525, 0.0005372778978198767, 0.05161469429731369, 0.03241283446550369, 0.01625116541981697, 0.04697743058204651, -0.013976839371025562, 0.0343489870429039, 0.02681046910583973, 0.03910703957080841, -0.013982041738927364, 0.004284527152776718, -0.007266428787261248, -0.01387533638626337, 0.02156374789774418, 0.005431845784187317, -0.06539802998304367, 0.04950221627950668, -0.003984121140092611, 0.0008856512140482664, -0.03476802259683609, 0.005463647656142712, 0.002816153457388282, 0.03925258293747902, -0.019079100340604782, -0.029856368899345398, -0.014192607253789902, 0.05039149522781372, 0.017951371148228645, -0.010834559798240662, 0.016016565263271332, 0.029418397694826126, -0.010107917711138725, 0.01325909048318863, 0.015482204034924507, -0.04730752855539322, -0.037692807614803314, -0.055098045617341995, -0.07587534189224243, 0.007387406192719936, -0.024197569116950035, -0.01917446404695511, -0.03397627919912338, -0.0532224215567112, -0.011599314399063587, -0.0010744741884991527, 0.011671791784465313, 0.015678079798817635, -0.03537607938051224, 0.040268875658512115, 0.0174394641071558, 0.003841896541416645, -0.05367111787199974, 0.025415940210223198, -0.007039799354970455, 0.029213862493634224, -0.009573141112923622, -0.01372101716697216, -0.026239784434437752, -0.0006180466734804213, 0.04272248223423958, -9.625193342799321e-05, 0.014610678888857365, 0.01001084316521883, -0.009239191189408302, -0.01271727867424488, -0.05316421017050743, -0.046189721673727036, 0.027835341170430183, -0.028184406459331512, -0.0460548996925354, 0.003086853539571166, -0.015675317496061325, -0.010234110988676548, 0.04976322874426842, 0.016174154356122017, -0.03225133568048477, 0.035120196640491486, 0.012102681212127209, 0.026305465027689934, -0.03056321293115616, 0.031453050673007965, 0.02473095804452896, -0.04141966998577118, -0.022309577092528343, -0.053998347371816635, -0.025471488013863564, -0.0017552690114825964, -0.007187134586274624, -0.03426216542720795, -0.041419271379709244, 0.03882940486073494, -0.018678264692425728, -0.1061297208070755, -0.005651541985571384, 0.007626502308994532, -0.04074375331401825, -0.009770913980901241, -0.01242230273783207, 0.03219491243362427, -0.01028972864151001, 0.000564622925594449, -0.06843932718038559, -0.0005365813267417252, 0.009410124272108078, 0.00913227628916502, -0.0030607343651354313, -0.043631412088871, 0.01773071102797985, 0.03233226388692856, -0.004220699425786734, -0.016250429674983025, -0.0052087921649217606, -0.039532098919153214, -0.012993507087230682, -0.004020606633275747, 0.011441406793892384, 0.03112005442380905, 0.02301507070660591, 0.04518038406968117, 0.02180328778922558, 0.029527753591537476, -0.0574052631855011, -0.0072206975892186165, 0.008952489122748375, -0.008748139254748821, 0.018971065059304237, 0.059662315994501114, -0.016818203032016754, 0.0085910614579916, 0.0016887141391634941, -0.018471620976924896, -0.0025004723574966192, 0.010603847913444042, 0.05507010966539383, -0.07184196263551712, 0.03256210684776306, 0.0043346635065972805, -0.026517098769545555, 0.03373906761407852, -0.0060042343102395535, -0.058057092130184174, 0.061605364084243774, 0.01725601777434349, 0.04468770697712898, -0.016929052770137787, 0.018525412306189537, 0.014137542806565762, 0.018857190385460854, 0.04620584473013878, 0.00673622265458107, 0.03985051065683365, -0.015708768740296364, -0.03946313261985779, -0.02643979899585247, -0.0385175459086895, -0.0157895777374506, 0.03610382229089737, -0.05326232314109802, -0.0004310770018491894, -0.02940063364803791, -0.049124881625175476, 0.05464847385883331, 0.027180053293704987, 0.020946236327290535, 0.013942169025540352, 0.05479750037193298, -0.017847562208771706, 0.012784644030034542, 0.022228820249438286, 0.028840024024248123, 0.016024339944124222, -0.03325151652097702, 0.04532263055443764, -0.025791676715016365, -0.017474563792347908, -0.012687807902693748, -0.018527351319789886, -0.03496710583567619, -0.004553001374006271, -0.007174924481660128, 0.0099886953830719, -0.01908493973314762, -0.02228407748043537, 0.00827351026237011, 0.05031800642609596, -0.039577025920152664, 0.001453221426345408, -0.05048999562859535, 0.025114476680755615, 0.031539496034383774, -0.035315435379743576, -0.012820495292544365, -0.00450438167899847, 0.015602221712470055, -0.002840770175680518, -0.0192731861025095, 0.0058833882212638855, -0.007455568760633469, -0.002280140994116664, -0.04588428884744644, 0.047035690397024155, 0.02485288493335247, 0.00929037295281887, 0.0017622990999370813, -0.05118652805685997, -0.015514308586716652, -0.03275969997048378, 0.019773229956626892, 0.009145992808043957, 0.026780787855386734, -0.004320812411606312, 0.04155799001455307, 0.05236038938164711, 0.016531992703676224, -0.014355317689478397, 0.043268926441669464, -0.02597450464963913, 0.038977816700935364, -0.037589769810438156, 0.0019268550677224994, -0.010230260901153088, 0.0029455048497766256, -0.03188062086701393, -0.02311684563755989, 0.005540768150240183, -0.006360671017318964, -0.007668334525078535, 0.03565793111920357, 0.003423697082325816, -0.025755025446414948, 0.05626489967107773, -0.025344060733914375, -0.014977158047258854, 0.040722332894802094, 0.013939411379396915, -0.013507749885320663, -0.01604555733501911, 0.006831228267401457, -0.0017863877583295107, 0.01922791264951229, -0.021931912750005722, 0.013392799533903599, -0.02466941997408867, -0.020866507664322853, 0.0068977526389062405, -0.028650736436247826, 0.032196324318647385, -0.029089299961924553, -0.008935686200857162, -0.027624759823083878, -0.010288647376000881, -0.020187782123684883, 0.011959508061408997, -0.04043472930788994, 0.029078397899866104, -0.013835585676133633, -0.031082360073924065, 0.024043625220656395, -0.04605382680892944, -0.06635739654302597, -0.019395986571907997, 0.01299606915563345, -0.0060955011285841465, -0.003143973182886839, 0.016881057992577553, 0.023243902251124382, -0.01892503723502159, -0.012071728706359863, 0.010445312596857548, -0.025636397302150726, 0.003470440162345767, -0.04638117551803589, 0.021097132936120033, 0.0026526134461164474, -0.002719639567658305, -0.03542766347527504, -0.008203680627048016, -0.0285166148096323, 0.006433096248656511, -0.00039584748446941376, -0.03396058455109596, 0.009354169480502605, -0.013451744802296162, -0.027040932327508926, 0.06379202753305435, -0.006628156639635563, -0.07604620605707169, -0.050350211560726166, 0.009689724072813988, 0.012919957749545574, -0.014521748758852482, 0.034797292202711105, -0.01325311791151762, -0.03586534038186073, -0.05159059911966324, -0.00936269760131836, -0.010347959585487843, -0.011500408872961998, -0.013652019202709198, -0.03976782411336899, 0.03424986079335213, 0.013037222437560558, 0.051711004227399826, -0.008016242645680904, -0.03110015206038952, -0.05993025004863739, 0.012608645483851433, -0.056545257568359375, -0.022183295339345932, -0.02954668179154396, 0.01755485311150551, -0.025800097733736038, 0.017473885789513588, 0.012971515767276287, -0.010757139883935452, -0.020068993791937828, -0.003396392799913883, 0.026875602081418037, 0.03496144711971283, -0.048789191991090775, -0.021784186363220215, 0.0089826425537467, -0.012673739343881607, 0.010692386887967587, -0.0045012217015028, -0.06674657016992569, 0.05888458341360092, -0.03601430729031563, 0.022120453417301178, -0.0005915956571698189, -0.005562968552112579, -0.003457657527178526, -0.024607140570878983, 0.005588139872997999, -0.0233048927038908, -0.0069798799231648445, -0.03589266166090965, 0.02861950173974037, -0.014917880296707153, -0.009034860879182816, -0.05528247356414795, -0.015301205217838287, -0.010419159196317196, -0.03622626140713692, -0.02619723230600357, -0.042022306472063065, -0.002875453559681773, -0.006539000663906336, 0.014173060655593872, 0.02375444583594799, 0.00044255077955313027, 0.013373015448451042, 0.044168852269649506, -0.0019776257686316967, -0.0029410363640636206, -0.002581175649538636, 0.007195618469268084, 0.010883777402341366, -0.006734540686011314, -0.02456563152372837, 0.0010339304571971297, -0.03437177091836929, 0.01183098554611206, -0.011477523483335972, -0.05784311145544052, 0.0024264450185000896, 0.03676672279834747, 0.05320192500948906, -0.015729790553450584, 0.017924247309565544, -0.001968679018318653, -0.019778484478592873, -0.02204170450568199, 0.036309801042079926, -0.030046775937080383, -0.026790333911776543, 0.038084566593170166, -0.010261809453368187, -0.005911188665777445, 0.03349088504910469, -0.01676049456000328, -0.02928789146244526, -0.0020900359377264977, 0.06356051564216614, 0.012199469842016697, -0.058732423931360245, 0.04707751050591469, 0.021056022495031357, -0.012244299054145813, -0.045092806220054626, 0.020686708390712738, -0.005235833581537008, -0.004030031617730856, 0.004547541029751301, 0.01359058078378439, 0.0030545282643288374, 0.005790610332041979, -0.008992038667201996, 0.017206797376275063, -0.033735696226358414, -0.013548080809414387, 0.009746590629220009, 0.011115297675132751, -0.026242617517709732, 0.01703777350485325, 0.031881168484687805, 0.005897414870560169, -0.004981850273907185, 0.01496446505188942, 0.038221895694732666, -0.005435182712972164, -0.007826878689229488, 0.039979174733161926, 0.02143973670899868, 0.020469093695282936, 0.02903815545141697, 0.011611494235694408, 0.026907792314887047, -0.012286524288356304, -0.03941831737756729, 0.04282645136117935, 0.004279308952391148, -0.05178968608379364, -0.0078008645214140415, -0.014777598902583122, 0.013063664548099041, 0.02067219652235508, -0.0036492568906396627, 0.055225174874067307, 0.032639309763908386, 0.014883454889059067, 0.027606893330812454, -0.04038025066256523, 0.009609568864107132, -0.05683044344186783, -0.0016494503943249583, 0.008424947038292885, -0.027720078825950623, -0.0500839501619339, 0.00264332490041852, 0.020178141072392464, -0.06566191464662552, -0.009271991439163685, 0.0366288460791111, -0.031552162021398544, -0.0006983819184824824, 0.011872212402522564, 0.018415164202451706, -0.031741779297590256, -0.021638670936226845, 0.0365956649184227, -0.0039922273717820644, 0.01658557914197445, 0.014844032935798168, -0.013800542801618576, -0.008826067671179771, 0.03276822715997696, 0.0051127090118825436, 0.012827225029468536, 0.022939985617995262, -0.05439306050539017, -0.019621748477220535, -0.04717938229441643, 0.004703940358012915, -0.020918190479278564, 0.054030705243349075, 0.012698698788881302, -0.05043458193540573, 0.008876241743564606, 0.028674855828285217, 0.02166980691254139, -0.02120412141084671, -0.004910976160317659, 0.06465905159711838, -0.04532349482178688, -0.009851757436990738, 0.007418686058372259, -0.005322422366589308, -0.06257906556129456, -0.045073751360177994, -0.01398329809308052, -0.015516450628638268, 0.0015877820551395416, 0.006499098148196936, 0.021012790501117706, 0.0641828179359436, 0.03480363264679909, 0.03984934464097023, 0.007709167432039976, -0.01584574393928051, -0.007006349973380566, -0.004696542397141457, 0.06017966568470001, -0.007690670900046825, -0.013704312033951283, -0.021900054067373276, -0.010468821041285992, 0.04066603258252144, -0.019388506188988686, 0.017810683697462082, 0.004130884539335966, 0.0024894527159631252, -0.033668916672468185, 0.013989241793751717, 0.010731575079262257, 0.008259459398686886, 0.03818834200501442, -0.017035100609064102, -0.011668250896036625, -0.014127461239695549, -0.016545452177524567, -0.032658178359270096, -0.07009497284889221, -0.024660905823111534, 0.0012806293088942766, -0.038266927003860474, 0.025882737711071968, -0.02725774608552456, -0.03304854780435562, 0.004305047914385796, 0.012174970470368862, 0.017322853207588196, -0.007078477181494236, 0.03632204607129097, 0.00397998932749033, -0.0019766665063798428, -0.025039106607437134, -0.02449255995452404, 3.131417543045245e-05, 0.013365518301725388, -0.03776955604553223, -0.043883416801691055, 0.02565484121441841, 0.0577571876347065, -0.0014304107753559947, -0.0066160219721496105, -0.03515510633587837, 0.007493299897760153, 0.02518613450229168, 0.022446906194090843, -0.0717756375670433, 0.051349565386772156, -0.05724845826625824, 0.04931562393903732, -0.02861880138516426, -0.005518321879208088, -0.015673672780394554, 0.02162840962409973, 0.025927595794200897, 0.0031538736075162888, -0.03216082975268364, -0.015210794284939766, 0.04742990434169769, 0.0008114143856801093, 0.050389546900987625, 0.027247171849012375, 0.03196943178772926, -0.0040411981754004955, -0.011165905743837357, 0.006932498887181282, 0.0018543744226917624, 0.026496104896068573, 0.021298564970493317, -0.009064544923603535, 0.04515428468585014, 0.03496887907385826, -0.010760672390460968, -0.003645174205303192, 0.035230930894613266, -0.0015461986185982823, 0.0024477089755237103, -0.013301748782396317, -0.009470085613429546, 0.04950564727187157, -0.018474897369742393, -0.014572144486010075, 0.01983446627855301, 0.009114004671573639, -0.006112203933298588, -0.026912640780210495, -0.02594359591603279, 0.01753099635243416, -0.022621195763349533, -0.06640028953552246, -0.019422873854637146, 0.016830315813422203, 0.03833151236176491, 0.01097236480563879, 0.028940102085471153, -0.012993020936846733, 0.024624546989798546, 0.03561926260590553, -0.005443775560706854, -0.002874344354495406, 0.013402280397713184, -0.0032172463834285736, -0.04164481908082962, 0.04434671998023987, -0.03247133642435074, 0.0364811047911644, -0.00303934165276587, -0.016381477937102318, -0.02002560719847679, 0.0533703975379467, 0.020199483260512352, -0.021333880722522736, 0.025801517069339752, -0.0152783477678895, -0.0450352281332016, -0.05777892470359802, 0.0032572096679359674, -0.058213770389556885, 0.034535542130470276, 0.05095047876238823, -0.02919113077223301, -0.0448787659406662, 0.02051098830997944, -0.018501324579119682, -0.014234257861971855, 0.03454480320215225, 0.04400161653757095, 0.0068834088742733, 0.048869501799345016, 0.03580687195062637, 0.03202830255031586, 0.019380971789360046, 0.015345431864261627, -0.009431526996195316, -0.01053521130234003, 0.0282882172614336, -0.0060369353741407394, 0.008731486275792122, 0.0020435634069144726, -0.058188799768686295, 0.056627314537763596, 0.0328914113342762, 0.0051316507160663605, -0.006821438204497099, 0.015309636481106281, 0.009278569370508194, -0.04750623553991318, -0.00234406185336411, 0.06543571501970291, -0.006216047797352076, 0.04122680425643921, -0.010644100606441498, -0.01369546540081501, -0.061057284474372864, -0.012691180221736431, -0.021158624440431595, 0.046265989542007446, 0.013932464644312859, -0.049725160002708435, -0.0021416351664811373, -0.053550831973552704, -0.021207327023148537, -0.005066852550953627, -0.012448322959244251, -0.03945042937994003, 0.05309607833623886, 0.00951268058270216, 0.03494395315647125, 0.0047113532200455666, 0.011125246062874794, -0.011065119877457619, 0.04399964585900307, 0.04738776013255119, 0.021319597959518433, 0.08469291776418686, 0.0642816498875618, 0.03629974275827408, 0.004294257145375013, 0.006382106337696314, 0.023673268035054207, 0.008419480174779892, 0.01728195510804653, -0.01638086885213852, 0.014567668549716473, -0.014821916818618774, -0.04502834752202034, -0.0013516065664589405, 0.06367358565330505, 0.010646789334714413, -0.003191924886777997, -0.020344842225313187, 0.007862706668674946, -0.06793197244405746, 0.011107012629508972, -0.03982190415263176, 0.04818229004740715, 0.02372230403125286, 0.01030319556593895, -0.000270345713943243, -0.03908907622098923, 0.2926609218120575, 0.03259357810020447, 0.040097758173942566, 0.04541618749499321, 0.02350856177508831, 0.010837258771061897, 0.058135081082582474, -0.029896829277276993, 0.05486135929822922, -0.04987984523177147, 0.006480803247541189, -0.02821941487491131, -0.008496827445924282, -0.0018053111853078008, 0.002568021649494767, 0.03227237984538078, -0.004728284664452076, -0.01957128569483757, -0.0188851747661829, -0.0452440083026886, -0.04699420928955078, 0.013188270851969719, 0.010475428774952888, 0.05618945136666298, 0.0035985016729682684, -0.03208756446838379, 0.02619609422981739, -0.026581022888422012, 0.011237665079534054, -0.05072920769453049, 0.02303309738636017, -0.0265315193682909, 0.03950926288962364, -0.030940569937229156, -0.013931593857705593, 0.01985999569296837, -0.010273460298776627, -0.07589588314294815, -0.0009535442222841084, 0.04769030958414078, -0.0015583172207698226, -0.04542522504925728, 0.016020357608795166, -0.014653152786195278, -0.005844742991030216, 0.059972699731588364, 0.008806698024272919, -0.0050788517110049725, 0.05297137424349785, -0.06477701663970947, 0.011841870844364166, -0.03786823898553848, 0.026557689532637596, -0.019469313323497772, -0.05313149094581604, -0.01590125262737274, -0.0353769026696682, -0.044478271156549454, 0.007330100983381271, 0.05679703876376152, 0.03211015835404396, 0.014573142863810062, 0.002535063773393631, 0.02601819857954979, -0.00988149456679821, 0.04037804156541824, 0.020438028499484062, 0.016630137339234352, -0.008996986784040928, 0.0012333060149103403, 0.03338571637868881, 0.01595291867852211, -0.01006021536886692, 0.014450342394411564, 0.009093313477933407, 0.04963986575603485, -0.010615477338433266, 0.021421151235699654, 0.014547119848430157, -0.023095227777957916, -0.0035965621937066317, 0.002332332544028759, -0.023324444890022278, 0.01339947059750557, -0.007487731520086527, 0.05919778719544411, -0.03979964554309845, -0.002696058014407754, -0.01583331637084484, 0.03920312225818634, 0.0012945797061547637, 0.04303324595093727, -0.006247451063245535, -0.008534396067261696, 0.03442184999585152], metadata={}, excluded_embed_metadata_keys=[], excluded_llm_metadata_keys=[], relationships={}, metadata_template='{key}: {value}', metadata_separator='\\n', text_resource=MediaResource(embeddings=None, data=None, text='Since Docling is fast, stable and cheap to run, it also makes for an excellent choice to build document-derived datasets. With its powerful table structure recognition, it provides significant benefit to automated knowledge-base construction [11, 10]. Docling is also integrated within the open IBM data prep kit [6], which implements scalable data transforms to build large-scale multi-modal training datasets.\\n\\n## 6 Future work and contributions\\n\\nDocling is designed to allow easy extension of the model library and pipelines. In the future, we plan to extend Docling with several more models, such as a figure-classifier model, an equationrecognition model, a code-recognition model and more. This will help improve the quality of conversion for specific types of content, as well as augment extracted document metadata with additional information. Further investment into testing and optimizing GPU acceleration as well as improving the Docling-native PDF backend are on our roadmap, too.\\n\\nWe encourage everyone to propose or implement additional features and models, and will gladly take your inputs and contributions under review . The codebase of Docling is open for use and contribution, under the MIT license agreement and in alignment with our contributing guidelines included in the Docling repository. If you use Docling in your projects, please consider citing this technical report.\\n\\n## References\\n\\n- [1] J. AI. Easyocr: Ready-to-use ocr with 80+ supported languages. https://github.com/ JaidedAI/EasyOCR , 2024. Version: 1.7.0.\\n- [2] J. Ansel, E. Yang, H. He, N. Gimelshein, A. Jain, M. Voznesensky, B. Bao, P. Bell, D. Berard, E. Burovski, G. Chauhan, A. Chourdia, W. Constable, A. Desmaison, Z. DeVito, E. Ellison, W. Feng, J. Gong, M. Gschwind, B. Hirsh, S. Huang, K. Kalambarkar, L. Kirsch, M. Lazos, M. Lezcano, Y. Liang, J. Liang, Y. ', path=None, url=None, mimetype=None), image_resource=None, audio_resource=None, video_resource=None, text_template='{metadata_str}\\n\\n{content}'),\n",
       " Document(id_='78edc927-0c25-4bdd-b0dc-76410539bf71', embedding=[0.03442339226603508, -0.036818504333496094, -0.020709138363599777, -0.009949801489710808, -0.007486492395401001, -0.010242587886750698, -0.00032615259988233447, -0.014656252227723598, 0.027408922091126442, 0.04709548503160477, 0.00882574450224638, -0.016595134511590004, 0.04830843582749367, -0.03338979184627533, -0.02037792280316353, -0.0008132609073072672, -0.006136880721896887, -0.010724776424467564, -0.022025655955076218, 0.01058528944849968, -0.007347774691879749, -0.011282502673566341, -0.08620967715978622, -0.023971272632479668, -0.03141389787197113, 0.041368518024683, 0.03289281949400902, 0.021764550358057022, 0.0827038586139679, 0.046342819929122925, -0.02297973446547985, -0.027742154896259308, 0.019808288663625717, -0.0333263985812664, 0.007973859086632729, -0.033410076051950455, 0.030533695593476295, 0.006943122949451208, -0.005635889247059822, -0.025961875915527344, 0.025673959404230118, -0.0319112129509449, 0.05296618118882179, -0.07344630360603333, -0.07233306020498276, -0.008772832341492176, 0.040145449340343475, -0.00541254598647356, -0.0024192268028855324, -0.03949051722884178, -0.018870975822210312, 0.025131268426775932, -0.003068121150135994, -0.013533576391637325, 0.014144237153232098, 0.027417389675974846, 0.011787917464971542, -0.008073106408119202, -0.042815886437892914, 0.011032963171601295, 0.012012842111289501, 0.022775253280997276, 0.003956351894885302, -0.04556042701005936, -0.005736406892538071, 0.015428951010107994, -0.017408940941095352, 0.0017043175175786018, -0.012426575645804405, -0.0017701401375234127, -0.04692962393164635, -0.005623998120427132, -0.06488742679357529, -0.025977784767746925, -0.02481391839683056, -0.0068420241586863995, 0.007853512652218342, -0.0019382754107937217, -0.053958453238010406, 0.057336002588272095, 0.0011128982296213508, 0.01844724453985691, 0.003800563281401992, 0.0040589082054793835, -0.026140121743083, -0.06838009506464005, 0.06631705164909363, -0.01626049540936947, 0.0012375485384836793, -0.00434501888230443, 0.0007120907539501786, 0.014557180926203728, -0.006238570902496576, -0.0005465899012051523, 0.02367609180510044, 0.031703926622867584, -0.019661152735352516, 0.03735179081559181, -0.005574462004005909, 0.007364145014435053, 0.028954921290278435, 0.05271551385521889, -0.012906488962471485, 0.04965795949101448, -0.05067569017410278, -0.0006556268781423569, 0.02845797687768936, -0.04648517444729805, -0.021196287125349045, -0.030377807095646858, 0.008554384112358093, -0.002732072724029422, -0.002688434673473239, 0.003679105779156089, 0.003712497651576996, 0.014834215864539146, -0.00621006777510047, 0.017667429521679878, 0.025460127741098404, -0.01871432550251484, 0.00878711137920618, -0.02433965727686882, -0.005121289752423763, -0.021250005811452866, 0.012213158421218395, -0.047035157680511475, 0.005333878565579653, 0.057327043265104294, -0.03793412074446678, -0.0043793716467916965, 0.03859914094209671, 1.4242814359022304e-05, -0.0394732803106308, 0.0270075686275959, 0.005173174664378166, -0.029894717037677765, 0.002350138733163476, 0.008668337017297745, 0.02584589645266533, -0.008885866962373257, -0.00636243773624301, -0.0059824720956385136, 0.004431203473359346, 0.10652733594179153, -0.011092008091509342, 0.00615270622074604, -0.003324291668832302, -0.027091097086668015, -0.06270517408847809, 0.04207416996359825, -0.0013094289461150765, 0.009519419632852077, 0.01964454911649227, -0.00016970111755654216, -0.02286355569958687, 0.024546489119529724, -0.02291959337890148, 0.011384683661162853, 0.005978726781904697, 0.020639032125473022, 0.013774779625236988, -0.011076629161834717, -0.05262363702058792, 0.01892898418009281, 0.00818667933344841, 0.05944635719060898, -0.028799476101994514, -0.0494915246963501, -0.00964827835559845, -0.020559431985020638, 0.01583719253540039, 0.0033361578825861216, -0.014974064193665981, 0.02533385530114174, 0.07649189978837967, 0.03359554335474968, 0.036226026713848114, -0.007647550664842129, 0.030640151351690292, 0.012294200249016285, -0.035903532058000565, -0.02078332006931305, 0.018693245947360992, 0.02105467952787876, -0.0004247664473950863, 0.04765928536653519, -0.020766671746969223, -0.019931156188249588, 0.02207936719059944, -0.01734345778822899, -0.015478812158107758, 0.050610337406396866, -0.0457736998796463, 0.03884200379252434, -0.011664221994578838, 0.0008942278800532222, -0.07306276261806488, 0.0010532857850193977, -0.0008036451181396842, -0.07145646959543228, -0.03221852704882622, 0.03341872617602348, 0.002797280205413699, -0.006728318519890308, -0.060208290815353394, -0.019499434158205986, 0.021779770031571388, 0.03361361473798752, -0.01354413665831089, 0.03262779861688614, 0.05723552778363228, -0.006126728840172291, -0.03584601730108261, -0.03281658887863159, 0.0460176020860672, -0.016148967668414116, -0.007105695549398661, 0.01738034188747406, -0.024529816582798958, -0.00102426263038069, 0.015813767910003662, 0.004865560214966536, 0.0307514276355505, -0.04116290435194969, 0.007508760783821344, 0.01370253972709179, 0.0012315636267885566, 0.042459551244974136, 0.035260967910289764, -0.00175085652153939, 0.006791132502257824, 0.0519598051905632, -0.01568637602031231, 0.0739046037197113, 0.048674583435058594, 0.025388048961758614, 0.06298251450061798, 0.035232625901699066, 0.012109494768083096, 0.02913942001760006, -0.008543879725039005, 0.027456270530819893, 0.00278808013536036, 0.01830076426267624, 0.023559164255857468, 0.03229343891143799, -0.009475440718233585, -0.01427663303911686, -0.016525570303201675, 0.014547588303685188, -0.050152137875556946, 0.05072999373078346, 0.014693889766931534, 0.009207486175000668, -0.019329318776726723, -0.015596789307892323, 0.007305763196200132, 0.04959544911980629, -0.031207354739308357, -0.027858184650540352, 0.0034597492776811123, 0.05684239789843559, -0.00881421472877264, -0.018875140696763992, 0.026436718180775642, 0.05839378386735916, 0.0007704972522333264, 0.004092107526957989, 0.020126113668084145, -0.005498506594449282, -0.06176864355802536, -0.018658867105841637, -0.09654802083969116, -0.027587538585066795, -0.06051819398999214, 0.005951281171292067, 0.009972170926630497, -0.03281937167048454, 0.02395237609744072, 0.00015531395911239088, -0.00547026889398694, 0.024550745263695717, 0.009279553778469563, 0.03748255968093872, 0.035739485174417496, -0.005644516088068485, -0.026080405339598656, 0.05865003168582916, -0.022093983367085457, 0.04290080815553665, -0.028708742931485176, -0.0006663891253992915, -0.025646833702921867, -0.00022104317031335086, 0.0035305481869727373, 0.004881969187408686, -0.0026414089370518923, -0.025664852932095528, -0.057938046753406525, -0.004839628003537655, -0.014825657941401005, 0.0010154887568205595, 0.015569069422781467, -0.024570759385824203, -0.041167404502630234, 0.06643752753734589, -0.011446244083344936, -0.023917170241475105, 0.049771811813116074, 0.04199620336294174, -0.03602449968457222, 0.04539179801940918, 0.009687120094895363, 0.05008521303534508, -0.039487145841121674, 0.08330650627613068, 0.020025478675961494, 0.0023708411026746035, -0.015752948820590973, -0.0283220075070858, -0.0024132239632308483, 0.0024257919285446405, -0.005938247311860323, -0.049422577023506165, -0.004957412835210562, 0.02927488647401333, -0.03805144876241684, -0.06439030170440674, 0.004938991274684668, -0.008945230394601822, -0.05459592118859291, -0.015430408529937267, -0.014950616285204887, 0.041348304599523544, 0.035204414278268814, 0.0215514674782753, -0.018466483801603317, 0.015356012620031834, -0.009042100049555302, 0.017987065017223358, -0.005603071302175522, -0.05115853250026703, 0.021389052271842957, 0.04642906412482262, 0.0067255753092467785, 0.028315182775259018, 0.0012245156103745103, -0.0413258858025074, -0.010406150482594967, -0.012649169191718102, -0.007186570204794407, 0.011379203759133816, 0.011848628520965576, 0.016748618334531784, -0.005526610650122166, 0.03490437567234039, -0.04257352650165558, -0.011272815056145191, -0.008865409530699253, 0.016026921570301056, 0.024616235867142677, 0.06298785656690598, 0.01080288365483284, 0.0072728791274130344, -0.020435629412531853, -0.018452094867825508, -0.018681257963180542, 0.009163729846477509, 0.05235525965690613, -0.057942695915699005, 0.048659902065992355, -0.011347977444529533, -0.031121879816055298, 0.018311847001314163, -0.014245929196476936, -0.04596523568034172, 0.03101974166929722, 0.02458626590669155, 0.043131738901138306, -0.015601753257215023, -0.026592446491122246, -0.022940853610634804, 0.007917139679193497, 0.021174071356654167, 0.009891963563859463, 0.0418170690536499, -0.020268624648451805, 0.009980174712836742, -0.03914424777030945, -0.028072847053408623, 0.0015176208689808846, 0.03787888586521149, -0.013563976623117924, -0.005537765566259623, -0.038136471062898636, -0.045529674738645554, 0.0334179513156414, 0.02501298487186432, 0.01381522323936224, 0.014622088521718979, 0.04199015349149704, 0.005981353111565113, 0.01696203462779522, 0.031188705936074257, 0.02185678482055664, -0.007520975079387426, -0.06804009526968002, 0.0492265522480011, 0.0005751182325184345, 0.012137559242546558, -0.018862741068005562, -0.0006949286907911301, 0.00047317074495367706, 0.016942784190177917, -0.01739669218659401, 0.008604703471064568, -0.02444828487932682, -0.0035437061451375484, 0.015042774379253387, 0.04999414086341858, -0.02379044145345688, -0.039691876620054245, -0.040914781391620636, 0.0193050354719162, 0.04571499302983284, -0.031978487968444824, -0.0023716180585324764, -0.015711823478341103, -0.006313526537269354, 0.021302400156855583, -0.05755292624235153, -0.037747595459222794, -0.012952491641044617, 0.002436690963804722, -0.07896190136671066, 0.07003773748874664, 0.04011451452970505, -0.02538374625146389, 0.01111557986587286, -0.07950359582901001, 0.013573695905506611, 0.016731256619095802, -0.012555376626551151, 0.004361069295555353, 0.008755994029343128, -0.0004757636343128979, 0.013417706824839115, 0.02720051072537899, 0.004695674404501915, 0.007678282912820578, 0.040432270616292953, -0.06049313768744469, 0.059645410627126694, -0.04324410855770111, 0.010000625625252724, -0.011446211487054825, -0.015764275565743446, -0.016778070479631424, 0.015243705362081528, 0.019619368016719818, -0.022582190111279488, -0.004662237595766783, 0.04145720973610878, -0.023295605555176735, -0.025869179517030716, 0.05046023055911064, -0.0006540894974023104, -0.008465629070997238, 0.03692984953522682, 0.014147658832371235, -0.003630381077528, -0.006543835159391165, -0.009592984803020954, -0.03470068797469139, 0.010761570185422897, -0.02153271622955799, 0.013695601373910904, -0.03084956668317318, 0.006132348440587521, -0.003198799444362521, -0.019040735438466072, 0.05973562225699425, -0.017212191596627235, -0.01938968524336815, -0.01558329164981842, -0.05283532291650772, -0.021732332184910774, 0.030941728502511978, -0.022309109568595886, 0.023906284943223, 0.003752295859158039, -0.009319818578660488, 0.008456137962639332, -0.018726788461208344, -0.05052883177995682, -0.015958519652485847, -0.002049735514447093, -0.017276132479310036, 0.022516487166285515, 0.028343914076685905, 0.04530801251530647, -0.02911290153861046, -0.06999599188566208, 0.029760442674160004, -0.0006142841884866357, 0.011461377143859863, -0.04432861506938934, -0.0028900832403451204, -0.01358573418110609, -0.01398325152695179, -0.019222673028707504, -0.013508947566151619, -0.030317697674036026, -0.012602894566953182, -0.015577700920403004, 0.0013035866431891918, 0.04465646296739578, -0.028648875653743744, -0.04185139015316963, 0.08047396689653397, -0.010157886892557144, -0.07509464770555496, -0.04644400253891945, 0.011663643643260002, 0.0011346485698595643, 0.021811243146657944, 0.024344706907868385, -0.020092042163014412, -0.046875692903995514, -0.04576881229877472, 0.004345333203673363, -0.06548469513654709, 0.024841386824846268, -0.011437450535595417, -0.03116574138402939, -0.008029007352888584, 0.021881412714719772, 0.059386976063251495, -0.04038228839635849, -0.0057386127300560474, -0.04040037840604782, -0.001786605454981327, -0.06395690143108368, -0.0071185859851539135, -0.0255497507750988, -0.00966615229845047, -0.013062353245913982, 0.07193135470151901, -0.00478409556671977, -0.011602834798395634, -0.023919889703392982, -0.0026886051055043936, -0.003259529359638691, 0.0447118766605854, -0.0567803755402565, -0.023509569466114044, 0.030400458723306656, 0.00457078218460083, -0.0015701603842899203, -0.01050370279699564, -0.042471371591091156, 0.019845109432935715, -0.015485906973481178, 0.000242718102526851, -0.022846275940537453, -0.01945796050131321, -0.054694194346666336, -0.03743209317326546, 0.0343901664018631, -0.0581764318048954, 0.03408687561750412, -0.039951931685209274, 0.03502291068434715, 0.003541886107996106, -0.004710061941295862, -0.022242311388254166, -0.008664087392389774, -0.05226445943117142, -0.05187368020415306, 0.01673320308327675, 0.0011345816310495138, -0.0026218874845653772, -0.011130239814519882, -0.0030607259832322598, 0.025550417602062225, -0.04528556391596794, 0.05147940292954445, 0.047479286789894104, 0.010977192781865597, -0.0012821557465940714, -0.007638158742338419, 0.016593335196375847, 0.013039780780673027, -0.014038189314305782, -0.0076669324189424515, 0.0025957294274121523, -0.04155280441045761, -0.006489354185760021, -0.0325421579182148, -0.04143330454826355, -0.005963216070085764, -0.007620308082550764, 0.0348706990480423, -0.026356248185038567, 0.03287738189101219, 0.0037014465779066086, -0.04174164682626724, -0.008956816978752613, 0.062172211706638336, -0.015239992178976536, -0.007519072853028774, 0.02833367884159088, -0.00011669690866256133, 0.0039199902676045895, 0.025508398190140724, -0.01665249839425087, -0.026685263961553574, 0.01116889901459217, 0.07805576920509338, -0.02262493595480919, -0.05703548714518547, 0.037358466535806656, 0.04134925827383995, 0.00553749967366457, -0.036067478358745575, -0.0018398626707494259, 0.012382054701447487, -0.00021194985311012715, -0.029552806168794632, 0.014644309878349304, -0.00806371495127678, -0.021649394184350967, 0.0027393074706196785, 0.033022038638591766, 0.019536448642611504, -0.009699304588139057, 0.026136258617043495, 0.006860943976789713, -0.023529089987277985, 0.017190279439091682, 0.038925863802433014, -0.04708320274949074, -0.022115768864750862, -0.010429044254124165, 0.0012777994852513075, -0.0029371341224759817, -0.020872971042990685, 0.05736576020717621, 0.006731903180480003, 0.017039012163877487, 0.04814665764570236, 0.02287382073700428, 0.04626628756523132, -0.0033138920553028584, -0.024031735956668854, 0.006600491236895323, -0.00795971043407917, -0.023481573909521103, -0.03237508609890938, 0.03845386952161789, -0.0033767269924283028, 0.041875213384628296, -0.031774431467056274, 0.030364541336894035, 0.03343164548277855, 0.009159116074442863, 0.0007203646819107234, -0.05493512377142906, -0.0004065880202688277, -0.05946248024702072, 0.0005321495700627565, -0.00014428913709707558, -0.04219789803028107, -0.048608191311359406, 0.007432345766574144, 0.0015244991518557072, -0.039659179747104645, -0.017452958971261978, 0.04172471910715103, -0.037271853536367416, 0.0011362145887687802, -0.02261512540280819, 0.008695489726960659, -0.05676717311143875, -0.017944784834980965, -0.009424800053238869, 0.0032176452223211527, -0.008355583995580673, 0.01431799866259098, -0.004402532242238522, 0.008441245183348656, 0.010788275860249996, 0.03312600776553154, -0.004682397935539484, -0.0021556962747126818, -0.03557681292295456, -0.023284191265702248, -0.03679712116718292, 0.014763690531253815, -0.020319856703281403, 0.010854671709239483, 0.02285093255341053, -0.01817764714360237, 0.01840161345899105, -0.004819377325475216, 0.04107228294014931, -0.06225764378905296, 0.005238067824393511, 0.03605503961443901, -0.02524663880467415, -0.009121374227106571, -0.015332306735217571, 0.005796618759632111, -0.04946338012814522, -0.02152046374976635, 0.031406551599502563, -0.005215372424572706, 0.004878607578575611, 0.011086208745837212, 0.025779707357287407, 0.06554996222257614, 0.0027673952281475067, 0.04152964800596237, -0.010368755087256432, 0.024488750845193863, 0.007896685041487217, 0.007250586990267038, 0.04420323297381401, 0.009958721697330475, -0.04122219979763031, -0.030729899182915688, 0.035593364387750626, 0.022567642852663994, -0.0142809534445405, 0.02901512011885643, -0.0196644589304924, -0.033978573977947235, -0.03521662950515747, 0.022235596552491188, 0.028068413957953453, 0.05428113043308258, 0.024427607655525208, -0.016442561522126198, -0.024997027590870857, -0.01702459342777729, -0.03249041363596916, -0.02050059847533703, -0.05098634958267212, -0.03862660750746727, 0.016910945996642113, -0.04054849594831467, -0.0075790490955114365, 0.01710149273276329, -0.014090031385421753, 0.007793806958943605, 0.022189298644661903, 0.0029203027952462435, 0.0028494216967374086, -0.0048631103709340096, 0.0074825710617005825, 0.035612884908914566, -0.06747113913297653, -0.014719297178089619, -0.013711316511034966, 0.04230346530675888, -0.047503117471933365, -0.038051217794418335, 0.0054884180426597595, 0.04176531359553337, -0.013964620418846607, -0.005777400452643633, -0.06425143778324127, 0.015201601199805737, 0.01443079486489296, 0.02801607921719551, -0.04425666108727455, 0.01239996962249279, -0.02604762464761734, 0.022330205887556076, -0.007914921268820763, 0.00987826008349657, -0.03655219450592995, 0.00817601103335619, 0.020254135131835938, -0.012359674088656902, -0.029850320890545845, 0.0362146720290184, 0.025756798684597015, -0.04101237282156944, 0.022088559344410896, 0.012443373911082745, 0.02069653384387493, 0.013851989060640335, -0.005240647122263908, -0.0027563078328967094, 0.008826063945889473, 0.03302161023020744, 0.030036870390176773, -0.0025284220464527607, 0.05235109478235245, 0.047796253114938736, -0.0067546553909778595, 0.005761025007814169, 0.04015352949500084, 0.010956116952002048, -0.01138637587428093, -0.018264200538396835, -0.034851618111133575, 0.012565907090902328, -0.01943262666463852, 0.0037424054462462664, 0.010686655528843403, -0.02072516269981861, 0.005379269830882549, -0.011653966270387173, -0.02851145714521408, 0.03259146958589554, -0.04112061858177185, -0.05397334694862366, -0.02962518483400345, 0.005711413454264402, 0.036950912326574326, 0.010350239463150501, 0.031186142936348915, 0.013518832623958588, 0.04061415046453476, 0.058112241327762604, 0.012265614233911037, 0.015467937104403973, -0.012466891668736935, 0.024569619446992874, -0.032447680830955505, -0.011317847296595573, -0.03400103747844696, 0.009741449728608131, 0.015538874082267284, -0.020319797098636627, -0.026568416506052017, 0.06103019416332245, 0.0019878000020980835, -0.01997528038918972, 0.002103396225720644, -0.022564170882105827, -0.0007929620915092528, -0.05835910513997078, 0.03205931931734085, -0.024282798171043396, 0.023592617362737656, 0.04187805950641632, -0.049926795065402985, -0.017326293513178825, 0.022933831438422203, -0.020683247596025467, 0.006653903983533382, 0.027811193838715553, 0.061741914600133896, -0.021202899515628815, 0.08417099714279175, 0.043678924441337585, -0.0024714625906199217, 0.02733222395181656, 0.02552560158073902, -0.0005445638089440763, -0.027545161545276642, 0.01145980041474104, 0.0013586317654699087, 0.017547855153679848, -0.0011465211864560843, -0.06719238311052322, 0.05065577104687691, 0.04098428413271904, -0.0023488791193813086, -0.0038265124894678593, 0.02601170353591442, 0.02496832050383091, -0.05068038031458855, 0.0012717662611976266, 0.06707849353551865, 0.001446648151613772, 0.02604936622083187, -0.0032744554337114096, 0.001291087013669312, -0.057827454060316086, 0.0008841115050017834, -0.029328225180506706, 0.027785155922174454, -0.013338780961930752, -0.024707365781068802, -0.008392010815441608, -0.06520748138427734, -0.01734105311334133, -0.016167448833584785, -0.0004459655028767884, -0.01641784980893135, 0.015808939933776855, 0.028117112815380096, 0.05424613878130913, 0.035709504038095474, -0.03597969561815262, -0.032938141375780106, 0.04147582873702049, 0.053115375339984894, 0.01500256359577179, 0.06552312523126602, 0.06346352398395538, 0.02821502462029457, -0.014003664255142212, 0.014436057768762112, 0.013060178607702255, 0.0028171848971396685, 0.003568857442587614, -0.0073090665973722935, 0.0037348605692386627, -0.031148193404078484, -0.0425211600959301, -0.005425636190921068, 0.04910466820001602, -0.007988477125763893, -0.013125523924827576, -0.0546281635761261, -0.02361919730901718, -0.03902486339211464, -0.0013808071380481124, -0.046622227877378464, 0.024817168712615967, 0.007664532400667667, 0.0006640793289989233, -0.004272168036550283, -0.05584680289030075, 0.2237125188112259, 0.05253530293703079, 0.026961922645568848, 0.0004046694084536284, 0.019159749150276184, 0.041375044733285904, 0.04075564071536064, -0.025199227035045624, 0.04166015237569809, -0.029859324917197227, 0.009823407046496868, -0.02290148288011551, -0.000651259848382324, 0.038649193942546844, -0.03972921520471573, 0.022523676976561546, -0.055309951305389404, -0.033807169646024704, -0.021036716178059578, -0.04702375829219818, -0.04973156750202179, 0.024864960461854935, 0.00986887514591217, 0.03819651901721954, 0.028838612139225006, -0.013561137020587921, 0.022579438984394073, -0.04330303892493248, 0.013394873589277267, 0.006282830610871315, -0.010206595994532108, -0.02782326750457287, 0.016602976247668266, -0.016063202172517776, -0.0004894572193734348, 0.015565184876322746, 0.003177449107170105, -0.06608017534017563, -0.0031947295647114515, 0.05559379234910011, -0.00158096628729254, -0.01058974489569664, 0.044702619314193726, -0.028391839936375618, -0.017785899341106415, 0.06525842845439911, 0.008232326246798038, 0.020352117717266083, 0.0365886315703392, -0.08387687057256699, 0.025278130546212196, -0.060220323503017426, 0.018332339823246002, -0.025658464059233665, -0.05028065666556358, -0.021482063457369804, -0.015002483502030373, -0.00637718103826046, -0.0065529076382517815, 0.03937619552016258, -0.014004821889102459, 0.02999085932970047, 0.0035349130630493164, 0.03554605320096016, -0.048064764589071274, -0.003616355126723647, -0.015051173977553844, -0.0020104728173464537, -0.05036245286464691, -0.022585276514291763, 0.032374124974012375, -0.00776146724820137, -0.004433510359376669, -0.004982786253094673, -0.005247581284493208, 0.04604445397853851, -0.02173890918493271, 0.03374006226658821, 0.0025677175726741552, -0.04728938266634941, 0.009888299740850925, -0.004013804253190756, -0.0020835890900343657, -0.021588273346424103, 0.03031548485159874, 0.0668962374329567, -0.03472309187054634, 0.0020467343274503946, 0.006887142080813646, 0.03394871577620506, 0.02824169583618641, 0.047228746116161346, -0.016159312799572945, -0.002009575255215168, 0.014603883028030396], metadata={}, excluded_embed_metadata_keys=[], excluded_llm_metadata_keys=[], relationships={}, metadata_template='{key}: {value}', metadata_separator='\\n', text_resource=MediaResource(embeddings=None, data=None, text=\"Lu, C. Luk, B. Maher, Y. Pan, C. Puhrsch, M. Reso, M. Saroufim, M. Y. Siraichi, H. Suk, M. Suo, P. Tillet, E. Wang, X. Wang, W. Wen, S. Zhang, X. Zhao, K. Zhou, R. Zou, A. Mathews, G. Chanan, P. Wu, and S. Chintala. Pytorch 2: Faster\\n\\nmachine learning through dynamic python bytecode transformation and graph compilation. In Proceedings of the 29th ACM International Conference on Architectural Support for Programming Languages and Operating Systems, Volume 2 (ASPLOS '24) . ACM, 4 2024. doi: 10.1145/3620665.3640366. URL https://pytorch.org/assets/pytorch2-2.pdf .\\n\\n- [3] C. Auer, M. Dolfi, A. Carvalho, C. B. Ramis, and P. W. Staar. Delivering document conversion as a cloud service with high throughput and responsiveness. In 2022 IEEE 15th International Conference on Cloud Computing (CLOUD) , pages 363-373. IEEE, 2022.\\n- [4] J. Berkenbilt. Qpdf: A content-preserving pdf document transformer, 2024. URL https: //github.com/qpdf/qpdf .\\n- [5] O. R. developers. Onnx runtime. https://onnxruntime.ai/ , 2024. Version: 1.18.1.\\n- [6] IBM. Data Prep Kit: a community project to democratize and accelerate unstructured data preparation for LLM app developers, 2024. URL https://github.com/IBM/ data-prep-kit .\\n- [7] A. S. Inc. PyMuPDF, 2024. URL https://github.com/pymupdf/PyMuPDF .\\n- [8] J. Liu. \", path=None, url=None, mimetype=None), image_resource=None, audio_resource=None, video_resource=None, text_template='{metadata_str}\\n\\n{content}'),\n",
       " Document(id_='201b4f21-1eb0-411e-8bf5-ebe8116732c9', embedding=[0.01999719627201557, -0.008304388262331486, -0.00983511470258236, 0.03590455278754234, -0.012422339990735054, -0.01155118178576231, 0.009512250311672688, 0.019032767042517662, -0.0186009518802166, 0.03602873906493187, 0.02469848096370697, 0.016390223056077957, -0.014020879752933979, -0.04994235187768936, -0.02915557101368904, 0.010938147082924843, -0.011871736496686935, -0.01281354296952486, -0.015877675265073776, 0.021935394033789635, 0.009577123448252678, 0.02430586889386177, -0.10255897790193558, 0.024026378989219666, 0.008323818445205688, 0.041124794632196426, 0.045478083193302155, -0.002156121889129281, 0.09236901998519897, 0.04400760307908058, 0.00961647555232048, -0.017693936824798584, 0.012855669483542442, -0.0051832497119903564, -0.01577594317495823, -0.0371977798640728, 0.03097519464790821, -0.0293167345225811, -0.019384022802114487, -0.048582300543785095, 0.0019256728701293468, -0.003944552503526211, 0.05295153707265854, -0.04202266409993172, -0.04596830531954765, 0.013912180438637733, 0.0005994798266328871, -0.02706640399992466, -0.0036258436739444733, -0.04898004233837128, -0.022295841947197914, 0.004038393963128328, 0.019828008487820625, 0.008006750606000423, 0.012132657691836357, 0.018785471096634865, -0.019515490159392357, 0.011008108034729958, -0.052985526621341705, -0.004978713113814592, 0.02026568166911602, 0.011233116500079632, 0.0046962774358689785, -0.057276830077171326, -0.008121322840452194, 0.008863400667905807, -0.018293000757694244, -0.007090398110449314, 0.011090129613876343, -0.03660832345485687, -0.052498094737529755, 0.004625453148037195, -0.06867597997188568, -0.027887802571058273, -0.057625070214271545, -0.021431857720017433, 0.016022996976971626, -0.0022010833490639925, -0.04282044619321823, 0.06367550790309906, 0.006342550273984671, 0.01285075955092907, 0.012020940892398357, -0.022232376039028168, -0.014960532076656818, -0.04999144375324249, 0.08495306223630905, 0.02764131873846054, 0.03557711839675903, 0.0035681903827935457, -0.029599150642752647, 0.03605540841817856, -0.013782169669866562, -0.004162073601037264, 0.013047654181718826, 0.040937453508377075, 0.011448509991168976, 0.06517527252435684, 0.013812863267958164, 0.023277517408132553, 0.001984064234420657, 0.05671007186174393, -0.03172021731734276, 0.03359942510724068, -0.025382114574313164, 0.005914813373237848, 0.02919899672269821, -0.03199372440576553, 0.01676526479423046, -0.028595779091119766, -0.014506668783724308, -0.011282096616923809, -0.013215684331953526, -0.024845045059919357, -0.022187476977705956, 0.00992030743509531, -0.019228320568799973, 0.01035383716225624, -0.017018301412463188, 0.002090380061417818, -0.020578673109412193, -0.012455059215426445, -0.006035600323230028, -0.0011197939747944474, -0.00837218202650547, -0.04399094730615616, 0.023649612441658974, 0.03827175870537758, -0.05610884726047516, 0.005895553156733513, -0.015373398549854755, -0.0205890703946352, -0.011977765709161758, 0.033438194543123245, -0.0010142717510461807, -0.0023443931713700294, -0.00662275729700923, 0.012752551585435867, 0.019282512366771698, 0.0015235039172694087, 0.013740244321525097, 0.00031309371115639806, 0.014358658343553543, 0.06345967203378677, 0.009104704484343529, 0.027949048206210136, 0.012426410801708698, -0.015084775164723396, -0.058742456138134, 0.039355192333459854, -0.013825989328324795, 0.009484260343015194, 0.037179432809352875, 0.022947434335947037, -0.006346302572637796, 0.009341360069811344, 0.007737393956631422, -0.01623288355767727, 0.007318445481359959, -0.008946141228079796, 0.007265301886945963, 0.01880369335412979, -0.07233753800392151, 0.016141286119818687, -0.022461047396063805, 0.05283244699239731, -0.004184075631201267, -0.003944567404687405, -0.00894156750291586, -0.06229417026042938, 0.005042341537773609, -0.0011608346831053495, -0.018841536715626717, 0.049379389733076096, 0.0413949117064476, 0.054873161017894745, 0.046347200870513916, -0.01440188568085432, 0.04776334762573242, 0.01971653290092945, -0.011746778152883053, -0.016054704785346985, 0.011410512961447239, 0.02418416552245617, 0.004192084539681673, 0.036904554814100266, 0.0034722217824310064, 0.018783103674650192, 0.022259769961237907, -0.015238435007631779, 0.012835012748837471, 0.04727039113640785, 0.010729586705565453, 0.025467978790402412, 0.004557795822620392, 0.018474481999874115, -0.05383383482694626, 0.03156385198235512, 0.004987207241356373, -0.05941780284047127, -0.04382874444127083, 0.05352668836712837, 0.02901778742671013, 0.0118787232786417, -0.05494117736816406, -0.0062741925939917564, -0.0005437625804916024, 0.03630181401968002, -0.056869879364967346, 0.009426971897482872, 0.044483959674835205, 0.049917276948690414, -0.011071874760091305, -0.027802495285868645, 0.013789111748337746, -0.022317051887512207, -0.02083737403154373, 0.004643008578568697, -0.02217784710228443, 0.007639703340828419, -0.022138066589832306, 0.03820738196372986, 0.02133995294570923, -0.015065878629684448, 0.006129463668912649, -0.0017571606440469623, 0.02428998053073883, 0.05788357928395271, 0.005902446806430817, 0.0018249658169224858, 0.0007481280481442809, 0.03333070129156113, 0.007392561994493008, 0.03955899551510811, 0.03774761036038399, -0.002494950545951724, 0.06481306254863739, 0.03682556003332138, 0.023626429960131645, 0.002111769514158368, -0.005795762874186039, 0.020663917064666748, 0.011938252486288548, 0.00657128868624568, 0.02498561516404152, 0.015982428565621376, 0.006550754886120558, -0.008967612870037556, 0.027537254616618156, 0.004755164962261915, -0.035801321268081665, 0.08988770097494125, 0.029432620853185654, 0.01076379045844078, -0.05680616945028305, -0.046647489070892334, 0.017245087772607803, 0.03131723031401634, -0.03271424025297165, -0.04800872132182121, -0.03699256852269173, 0.05794233828783035, 0.012461827136576176, -0.0008267409284599125, -0.011520764790475368, 0.04817790165543556, -0.008024045266211033, 0.008547907695174217, 0.006149961147457361, -0.03234689682722092, -0.03610963374376297, -0.07344789057970047, -0.07781855762004852, 0.008641062304377556, -0.007611915469169617, -0.03451845422387123, -0.04198315367102623, -0.04610377922654152, -0.014775322750210762, -0.0027453384827822447, -0.010856364853680134, 0.02729138545691967, -0.004014998208731413, 0.004154064226895571, -0.0002808483550325036, -0.005858652293682098, -0.023461028933525085, 0.056005027145147324, 0.018418435007333755, 0.04088132828474045, -0.010121464729309082, -0.011292070150375366, -0.04549573361873627, -0.0003506613429635763, -0.004528564866632223, 0.0012433603405952454, -0.008097298443317413, 0.014767144806683064, -0.016718393191695213, -0.03316662833094597, -0.016223473474383354, -0.020366735756397247, 0.0085096824914217, -0.006796937435865402, -0.06826702505350113, 0.027037985622882843, -0.0006642318330705166, -0.018160060048103333, 0.030485406517982483, 0.032540347427129745, -0.013442124240100384, 0.01959160901606083, -0.01054998766630888, 0.024805709719657898, -0.027875542640686035, 0.03335759788751602, 0.01971665397286415, 0.0018881866708397865, -0.03282680734992027, -0.0444830022752285, -0.03855225816369057, 0.0031461031176149845, 0.010838648304343224, -0.03827529773116112, 0.044571250677108765, 0.02048548124730587, 0.0043809674680233, -0.08335485309362411, 0.00022876341245137155, -0.014421257190406322, -0.07627943903207779, -0.027387777343392372, -0.038702405989170074, 0.03611057996749878, 0.0167329553514719, 0.00903699267655611, -0.04265967756509781, -0.018628662452101707, -0.013457446359097958, 0.022962335497140884, 0.01551767997443676, -0.046783898025751114, 0.026921071112155914, 0.049052923917770386, -0.015329984948039055, 0.006803624797612429, 0.0025134305469691753, -0.014010458253324032, -0.015804044902324677, -0.012279630638659, -0.006830289959907532, 0.05087801441550255, 0.04519185051321983, 0.011697018519043922, 0.02285541407763958, 0.043918315321207047, -0.015592302195727825, -0.007603943347930908, 0.03761538863182068, 0.001077080494724214, 0.01115021575242281, 0.03674105927348137, 0.0172784011811018, 0.006013334263116121, -0.024827860295772552, -0.005671399645507336, 0.005773789715021849, 0.005426584277302027, 0.04682951420545578, -0.06991168111562729, 0.054035939276218414, -0.002903161570429802, -0.013695706613361835, 0.01652713119983673, -0.05427871271967888, -0.06221771985292435, 0.033392708748579025, -0.003354758722707629, 0.0469813272356987, -0.04459083452820778, 0.02324061095714569, -0.0031016513239592314, 0.023514876142144203, 0.040046703070402145, -0.005479216575622559, 0.047244612127542496, -0.011770425364375114, -0.025781681761145592, -0.014837437309324741, -0.01143009215593338, -0.011650148779153824, 0.00579412654042244, -0.007547563873231411, 0.0033696372993290424, -0.0428156740963459, -0.04087039455771446, 0.026688700541853905, 0.046238064765930176, 0.018836207687854767, -0.002712512854486704, 0.04287267476320267, 0.0020451024174690247, 0.051211487501859665, 0.031249042600393295, 0.02162613719701767, -0.012709058821201324, -0.04972417652606964, 0.026403723284602165, -0.00018604288925416768, 0.016731826588511467, 0.022146053612232208, -0.0025495337322354317, 0.0034009437076747417, 0.018053853884339333, 0.0013981818920001388, 0.017959993332624435, -0.045265067368745804, -0.00129913620185107, -0.0028612457681447268, 0.03514710068702698, -0.018069753423333168, -0.038106080144643784, -0.02565481700003147, 0.03654758632183075, 0.025776013731956482, -0.06212128326296806, -0.011665359139442444, -0.03523688018321991, 0.031033849343657494, 0.007221749983727932, -0.016523169353604317, -0.022721944376826286, -0.028043009340763092, -0.04565318301320076, -0.06116091087460518, 0.058700233697891235, 0.05339391157031059, -0.018422408029437065, 0.021735576912760735, -0.08256796002388, 0.022401945665478706, -0.023761166259646416, 0.015685075893998146, 0.0033143190667033195, 0.029529307037591934, -0.01077223103493452, -0.001054577762261033, 0.037344738841056824, -0.007653676439076662, -0.03210919350385666, 0.04856378212571144, -0.04562932997941971, 0.0213423240929842, -0.03412364795804024, -0.01443471759557724, -0.013848844915628433, -0.01100256759673357, -0.03992902860045433, -0.007643075659871101, -0.008654268458485603, -0.003190259449183941, -0.003228133777156472, 0.034168507903814316, 0.0023843382950872183, -0.027203360572457314, 0.043116841465234756, 0.0027773180045187473, -0.027239423245191574, 0.027565989643335342, -0.004366982262581587, 0.009596764110028744, 0.009626708924770355, 0.007236824370920658, -0.02198965847492218, -0.01422099769115448, -0.010981690138578415, 0.03330565243959427, -0.0238586887717247, 0.001362159731797874, 0.016468161717057228, 0.00276826205663383, 0.05368116870522499, 0.005412806756794453, -0.017762061208486557, -0.030477214604616165, -0.05154581367969513, -0.012095320038497448, 0.011024708859622478, -0.03681296482682228, 0.008581616915762424, -0.004576449748128653, -0.0275802630931139, 0.04459727182984352, -0.03795286640524864, -0.030122879892587662, -0.02902679331600666, -0.011510778218507767, -0.01862652786076069, 0.024244725704193115, 0.013724036514759064, 0.08296143263578415, -0.007592420093715191, -0.020703325048089027, 0.008813594467937946, 0.00340046314522624, -0.002230786718428135, -0.036081258207559586, -0.01342084351927042, 0.000439376977737993, 0.038951121270656586, -0.02841479331254959, -0.0226003248244524, 0.004315467085689306, -0.02905118837952614, 0.0020658480934798717, -0.018776768818497658, 0.03618020936846733, -0.05345558002591133, -0.042420823127031326, 0.06358940154314041, 0.03644111752510071, -0.08005677908658981, -0.0404106006026268, 0.017936166375875473, 0.0005716293235309422, 0.002049072878435254, 0.03505679592490196, 0.0009148812969215214, -0.02365630492568016, -0.07467570900917053, -0.024029556661844254, -0.035676490515470505, 0.019019311293959618, -0.020007142797112465, -0.018285682424902916, 0.033256419003009796, 0.04392661154270172, 0.05641275271773338, -0.05007059872150421, -0.01930457167327404, -0.07072484493255615, 0.02156582660973072, -0.03314030542969704, -0.02590438909828663, -0.006500495132058859, -0.00921553373336792, 0.005392145365476608, 0.026423947885632515, 0.020062243565917015, 0.006077549420297146, -0.00016152605530805886, -0.03105110116302967, 0.02252291515469551, 0.033040162175893784, -0.020064210519194603, -0.015193781815469265, 0.028645407408475876, 0.016832780092954636, 0.0038434749003499746, -0.020863866433501244, -0.031054001301527023, 0.05833015963435173, -0.05968654900789261, 0.015649892389774323, -0.027089959010481834, -0.005699049215763807, -0.041692331433296204, -0.033693358302116394, 0.04400533437728882, -0.03649619221687317, -0.03108094446361065, -0.026139328256249428, 0.03848310559988022, -0.0006552102277055383, -0.0022638665977865458, -0.040601130574941635, -0.011289660818874836, -0.08637501299381256, -0.02097395993769169, 0.009383970871567726, -0.03228393942117691, 0.007164990995079279, -0.009004098363220692, 0.006962948944419622, 0.03359750285744667, -0.040017519146203995, 0.06251753866672516, 0.058986857533454895, 0.0017648889916017652, 0.010490492917597294, -0.044556282460689545, -0.009076039306819439, -0.010180937126278877, -0.009977233596146107, 0.004416200332343578, 0.013891083188354969, -0.05466381087899208, -0.00716910045593977, -0.04098775237798691, -0.03942403197288513, -0.014425928704440594, 0.0154412891715765, 0.06760893762111664, -0.04768640547990799, 0.03145050257444382, -0.002161553129553795, -0.04163946583867073, -0.04787890613079071, 0.04052736610174179, 0.011486549861729145, 0.01072569377720356, 0.03526687994599342, 0.019863300025463104, -0.012666376307606697, 0.012716610915958881, 0.0020228137727826834, -0.0007504710229113698, -0.008981716819107533, 0.06008217856287956, 0.005579962860792875, -0.03213043883442879, 0.061071496456861496, 0.0463644340634346, -0.023781003430485725, -0.026765532791614532, 0.014894727617502213, 0.0026467780116945505, 0.003827855922281742, -0.014102865010499954, 0.011113891378045082, 0.021383682265877724, -0.018027283251285553, -0.006298730615526438, -0.0013739158166572452, -0.018068205565214157, 0.0016791680827736855, 0.028370730578899384, 0.02295542322099209, 0.0005858868244104087, 0.041361626237630844, 0.07437006384134293, -0.02307634800672531, 0.013177776709198952, -0.01588626205921173, -0.022465422749519348, 0.020405538380146027, -0.005161238368600607, 0.05788733437657356, 0.010085776448249817, 0.008982464671134949, 0.026412932202219963, 0.014828109182417393, 0.01530641969293356, -0.037440553307533264, -0.043014995753765106, 0.03512689843773842, -0.025281133130192757, -0.045485202223062515, -0.019715504720807076, 0.02069058082997799, -0.003431619144976139, 0.03415416181087494, -0.017787203192710876, 0.034511130303144455, 0.03470934182405472, 0.013877548277378082, -0.008177772164344788, -0.03494640067219734, -0.0030768567230552435, -0.04192579537630081, -0.013186508789658546, -0.020526517182588577, -0.03860403224825859, -0.020086880773305893, 0.022607743740081787, 0.010230534709990025, -0.07121939212083817, 0.0031229741871356964, 0.04413430020213127, -0.058260753750801086, 0.01670733653008938, -0.01338612288236618, 0.012254009954631329, -0.058543577790260315, -0.03991033881902695, 0.024511119350790977, -0.006515387445688248, -0.0026707604993134737, 0.007175966631621122, -0.00363472499884665, 0.021281933411955833, 0.01526129525154829, 0.028468867763876915, -0.0040841554291546345, -0.009989174082875252, -0.042943697422742844, -0.010134303011000156, -0.032699134200811386, -0.007204271387308836, -0.05108341947197914, 0.05158847197890282, 0.00046944510540924966, -0.008565906435251236, 0.0035639191046357155, 0.029958996921777725, 0.05056765675544739, -0.0648728534579277, -0.017284618690609932, 0.06354054808616638, -0.023760883137583733, 0.007150930818170309, -0.014972873032093048, -0.0016189594753086567, -0.049721039831638336, -0.025871122255921364, 0.0027801382821053267, 0.017734190449118614, -0.001689060707576573, -0.006821148097515106, 0.03175706788897514, 0.05713905021548271, 0.04430856183171272, 0.0018282635137438774, -0.001446238369680941, 0.033797893673181534, 0.01722659170627594, 0.007019218988716602, 0.02991541288793087, -0.01198809314519167, -0.03696995601058006, -0.034314487129449844, 0.003183678723871708, 0.03371366858482361, -0.014600901864469051, -0.007614027708768845, -0.0018445190507918596, -0.015003522858023643, -0.0632467046380043, 0.020732726901769638, 0.01536185946315527, 0.04526747763156891, 0.04490869492292404, 0.00026093795895576477, -0.03150544315576553, 0.0038332119584083557, -0.02369646169245243, -0.005162769462913275, -0.06250762194395065, -0.03167863190174103, -0.0014415303012356162, -0.00187965901568532, -0.004265465773642063, 0.0053566209971904755, -0.0480741485953331, -0.010186774656176567, 0.025072351098060608, 0.04043762385845184, 0.0026463347021490335, 0.038492508232593536, -0.023365089669823647, 0.012617170810699463, -0.030146781355142593, -0.010371549986302853, -0.00614326074719429, 0.02250516228377819, -0.04642122983932495, -0.04563059285283089, 0.040096644312143326, 0.04355324059724808, 0.009057912044227123, 0.0037300479598343372, -0.041739266365766525, 0.0025724160950630903, 0.005601797252893448, 0.019971180707216263, -0.03632678464055061, 0.03948060795664787, -0.0359775684773922, 0.003160932334139943, -0.02499992586672306, 0.012026299722492695, -0.031613342463970184, 0.028482351452112198, 0.026607347652316093, -0.00635096151381731, -0.029280267655849457, 0.0025845675263553858, 0.040958527475595474, -0.02061709389090538, 0.05178895220160484, 0.020770011469721794, 0.031227130442857742, 0.017137626186013222, -0.0035204023588448763, -0.021727856248617172, 0.01937660202383995, 0.03612707182765007, 0.012885276228189468, -0.018819497898221016, 0.04040366783738136, 0.023289022967219353, -0.00811320636421442, -0.0190704558044672, 0.033910516649484634, -0.02310977131128311, 0.005484212189912796, -0.006655760109424591, -0.014787950552999973, 0.03386184200644493, -0.005287084728479385, -0.01849830336868763, 0.02247983030974865, -0.019701145589351654, -0.008584193885326385, -0.013000388629734516, -0.04265999794006348, 0.04299081489443779, -0.030928287655115128, -0.04174588993191719, -0.050015754997730255, 0.029604323208332062, 0.0341341570019722, 0.0034889723174273968, 0.013004999607801437, 0.0005770427524112165, 0.031020881608128548, 0.05592375248670578, 0.016585566103458405, 0.026603523641824722, 0.002070911694318056, 0.010217960923910141, -0.0253145694732666, -0.0014625434996560216, -0.003222082741558552, 0.016851315274834633, -0.02193777821958065, -0.027041034772992134, -0.031875498592853546, 0.0238468199968338, -0.0043494864366948605, -0.021499168127775192, 0.030971411615610123, -0.03227704018354416, -0.02528798207640648, -0.026969600468873978, 0.010797311551868916, -0.009316694922745228, 0.010818812996149063, 0.05779041722416878, -0.04748891666531563, -0.02754008211195469, 0.015079161152243614, -0.003745732130482793, 0.011890852823853493, 0.041466183960437775, 0.03762819245457649, -0.014684242196381092, 0.06853502243757248, 0.04366501048207283, 0.014774796552956104, 0.0005282647907733917, 0.014756166376173496, 0.01997026801109314, -0.023059573024511337, -0.01710657961666584, 0.007165546528995037, 0.009318016469478607, -0.004631465766578913, -0.06274467706680298, 0.025715887546539307, 0.015358597971498966, 0.004298219457268715, -0.005068189464509487, 0.028192678466439247, 0.010602965950965881, -0.040171924978494644, -0.015507740899920464, 0.03602200746536255, -0.023698095232248306, -0.011568781919777393, 0.00160702015273273, -0.020808620378375053, -0.05948051065206528, 0.011361397802829742, -0.028598986566066742, 0.030157092958688736, -0.00935308262705803, -0.06649518013000488, 0.004758389201015234, -0.03479690104722977, -0.015918930992484093, -0.0004810537793673575, -0.015546048991382122, -0.013396607711911201, 0.039713360369205475, 0.03446224331855774, 0.04156776890158653, -0.003136174287647009, -0.010231465101242065, -0.018516486510634422, 0.012616552412509918, 0.04012509807944298, 0.004060362000018358, 0.08164527267217636, 0.045288845896720886, 0.0306133683770895, 0.0008497367962263525, 0.001766791334375739, 0.011752257123589516, 0.01503999624401331, 0.0012850449420511723, -0.0353889986872673, 0.02021392621099949, -0.03117213025689125, -0.051531944423913956, 0.009241105988621712, 0.03968267887830734, 0.055196866393089294, -0.027324682101607323, -0.024098416790366173, 0.008721073158085346, -0.03902093693614006, -0.0023178537376224995, -0.039508573710918427, 0.06095875799655914, -0.0045785498805344105, -0.015541979111731052, -0.00983086135238409, -0.03892083093523979, 0.24366915225982666, 0.058152586221694946, 0.03199419751763344, 0.014621088281273842, 0.025175588205456734, 0.004560106433928013, 0.02881677635014057, -0.048071473836898804, 0.04514232277870178, -0.016626479104161263, 0.02119636721909046, -0.010730077512562275, -0.006144403479993343, 0.0335107259452343, -0.015301698818802834, 0.0319225937128067, -0.04207636043429375, -0.031751569360494614, -0.012255021370947361, -0.047853969037532806, -0.030971728265285492, 0.003641788847744465, 0.01493352185934782, -0.004561637062579393, 0.027789155021309853, -0.016732290387153625, 0.012999599799513817, -0.03546023741364479, -0.028203817084431648, -0.003389047458767891, 0.04366791248321533, -0.03882822394371033, 0.022359585389494896, -0.0301516130566597, -0.041807256639003754, 0.017245139926671982, -0.019658898934721947, -0.07470262795686722, -0.024720974266529083, 0.042265474796295166, -0.022264154627919197, -0.005208824295550585, 0.011044985614717007, -0.01900937221944332, -0.011866462416946888, 0.06919887661933899, 0.00034432511893101037, 0.045614901930093765, 0.042382802814245224, -0.06803684681653976, -0.005813749972730875, -0.03453083336353302, 0.05372173711657524, -0.04188958555459976, -0.07292038202285767, -0.019009359180927277, -0.008222219534218311, -0.04796341061592102, -0.019730206578969955, 0.045999083667993546, -0.0001693314843578264, -0.004171260166913271, -0.01752571575343609, 0.027000190690159798, -0.03828297555446625, 0.03036395274102688, 0.022807206958532333, -0.01124905701726675, -0.02911679819226265, 0.020469123497605324, 0.012832717038691044, 7.946841651573777e-05, -0.01871613785624504, -0.010588089004158974, -0.018192611634731293, 0.04381614550948143, -0.03545263409614563, 0.021123768761754036, 0.006358289159834385, 0.013484294526278973, -0.02922852523624897, -0.002549066673964262, -0.05585663765668869, -0.021994661539793015, -0.0007474359590560198, 0.056123945862054825, -0.05203215032815933, 0.026960348710417747, -0.00829233042895794, 0.03729597479104996, 0.010437685996294022, 0.0032832433935254812, -0.004384496249258518, -0.03261938691139221, -0.014666999690234661], metadata={}, excluded_embed_metadata_keys=[], excluded_llm_metadata_keys=[], relationships={}, metadata_template='{key}: {value}', metadata_separator='\\n', text_resource=MediaResource(embeddings=None, data=None, text='LlamaIndex, 11 2022. URL https://github.com/jerryjliu/llama\\\\_index .\\n- [9] M. Lysak, A. Nassar, N. Livathinos, C. Auer, and P. Staar. Optimized Table Tokenization for Table Structure Recognition. In Document Analysis and Recognition - ICDAR 2023: 17th International Conference, San Jos´ e, CA, USA, August 21-26, 2023, Proceedings, Part II , pages 37-50, Berlin, Heidelberg, Aug. 2023. Springer-Verlag. ISBN 978-3-031-41678-1. doi: 10. 1007/978-3-031-41679-8 3. URL https://doi.org/10.1007/978-3-031-41679-8\\\\_3 .\\n- [10] L. Mishra, S. Dhibi, Y. Kim, C. Berrospi Ramis, S. Gupta, M. Dolfi, and P. Staar. Statements: Universal information extraction from tables with large language models for ESG KPIs. In D. Stammbach, J. Ni, T. Schimanski, K. Dutia, A. Singh, J. Bingler, C. Christiaen, N. Kushwaha, V. Muccione, S. A. Vaghefi, and M. Leippold, editors, Proceedings of the 1st Workshop on Natural Language Processing Meets Climate Change (ClimateNLP 2024) , pages 193-214, Bangkok, Thailand, Aug. 2024. Association for Computational Linguistics. URL https://aclanthology.org/2024.climatenlp-1.15 .\\n- [11] L. Morin, V. Weber, G. I. Meijer, F. Yu, and P. W. J. Staar. ', path=None, url=None, mimetype=None), image_resource=None, audio_resource=None, video_resource=None, text_template='{metadata_str}\\n\\n{content}')]"
      ]
     },
     "execution_count": 31,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "all_chunks[:10]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f2f54557",
   "metadata": {},
   "source": [
    "## 5. Create Ddrant Collection"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f6682847",
   "metadata": {},
   "outputs": [],
   "source": [
    "# import numpy as np\n",
    "# from qdrant_client.models import VectorParams, Distance\n",
    "# from qdrant_client.models import PointStruct\n",
    "\n",
    "\n",
    "# # Create the collection if it doesn't exist\n",
    "# client.recreate_collection(\n",
    "#     collection_name=collection_name,\n",
    "#     vectors_config=VectorParams(\n",
    "#         size=np.array(all_chunks[0].embedding).shape[0],  # dimension of your embedding\n",
    "#         distance=Distance.COSINE                # or Distance.DOT, Distance.EUCLID\n",
    "#     )\n",
    "# )\n",
    "\n",
    "# points = []\n",
    "# for i, chunk in enumerate(all_chunks):\n",
    "#     if chunk.embedding is not None:\n",
    "#         points.append(\n",
    "#             PointStruct(\n",
    "#                 id=i,\n",
    "#                 vector=chunk.embedding,\n",
    "#                 payload={\"text\": chunk.text}\n",
    "#             )\n",
    "#         )\n",
    "\n",
    "# client.upsert(\n",
    "#     collection_name=collection_name,\n",
    "#     points=points\n",
    "# )"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6189b8fe",
   "metadata": {},
   "source": [
    "# 5. Load the embedding model and index data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "id": "398d3707",
   "metadata": {},
   "outputs": [],
   "source": [
    "from llama_index.vector_stores.qdrant import QdrantVectorStore\n",
    "from llama_index.core import VectorStoreIndex, StorageContext\n",
    "\n",
    "def create_index(documents):\n",
    "\n",
    "    vector_store = QdrantVectorStore(client=client,\n",
    "                                     collection_name=collection_name)\n",
    "    \n",
    "    storage_context = StorageContext.from_defaults(vector_store=vector_store)\n",
    "    \n",
    "    index = VectorStoreIndex.from_documents(documents,\n",
    "                                            storage_context=storage_context)\n",
    "    \n",
    "    return index"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "id": "4cbe8452",
   "metadata": {},
   "outputs": [],
   "source": [
    "from llama_index.core import Settings\n",
    "\n",
    "index = create_index(all_chunks)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "id": "cd5373f1",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "llama_index.core.indices.vector_store.base.VectorStoreIndex"
      ]
     },
     "execution_count": 34,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "type(index)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7c43420f",
   "metadata": {},
   "source": [
    "## 6. Load the LLM"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "id": "0df5efb3",
   "metadata": {},
   "outputs": [],
   "source": [
    "from llama_index.llms.ollama import Ollama\n",
    "from llama_index.core import Settings\n",
    "\n",
    "\n",
    "llm = Ollama(model=\"llama3.2:1b\", request_timeout=120.0, temperature=0.0)\n",
    "\n",
    "Settings.llm = llm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "id": "7dd2a7ea",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(llama_index.core.settings._Settings,\n",
       " Ollama(callback_manager=<llama_index.core.callbacks.base.CallbackManager object at 0x4128020d0>, system_prompt=None, messages_to_prompt=<function messages_to_prompt at 0x127a50c10>, completion_to_prompt=<function default_completion_to_prompt at 0x127ed6040>, output_parser=None, pydantic_program_mode=<PydanticProgramMode.DEFAULT: 'default'>, query_wrapper_prompt=None, base_url='http://localhost:11434', model='llama3.2:1b', temperature=0.0, context_window=-1, request_timeout=120.0, prompt_key='prompt', json_mode=False, additional_kwargs={}, is_function_calling_model=True, keep_alive=None, thinking=None),\n",
       " HuggingFaceEmbedding(model_name='BAAI/bge-large-en-v1.5', embed_batch_size=10, callback_manager=<llama_index.core.callbacks.base.CallbackManager object at 0x4128020d0>, num_workers=None, embeddings_cache=None, max_length=512, normalize=True, query_instruction=None, text_instruction=None, cache_folder=None, show_progress_bar=False))"
      ]
     },
     "execution_count": 68,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "type(Settings), Settings.llm, Settings.embed_model"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7b2f8e3f",
   "metadata": {},
   "source": [
    "# 7. Define the prompt template"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "id": "89ac4eb5",
   "metadata": {},
   "outputs": [],
   "source": [
    "from llama_index.core import PromptTemplate\n",
    "\n",
    "template = \"\"\"Context information is below:\n",
    "              ---------------------\n",
    "              {context_str}\n",
    "              ---------------------\n",
    "              Given the context information above I want you to think\n",
    "              step by step to answer the query in a crisp manner,\n",
    "              incase you don't know the answer say 'I don't know!'\n",
    "            \n",
    "              Query: {query_str}\n",
    "        \n",
    "              Answer:\"\"\"\n",
    "\n",
    "qa_prompt_tmpl = PromptTemplate(template)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1f4e4672",
   "metadata": {},
   "outputs": [],
   "source": [
    "## 8. Query Qdrant directly with your own embedding\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "24aeb544",
   "metadata": {},
   "outputs": [],
   "source": [
    "# query = \"What exactly is DSPy?\"\n",
    "\n",
    "# # Use the same embedding model as for chunking\n",
    "# query_embedding = semantic_chunker.chunk([query])[0]\n",
    "\n",
    "# search_result = client.search(\n",
    "#     collection_name=collection_name,\n",
    "#     query_vector=query_embedding.tolist(),\n",
    "#     limit=5\n",
    "# )\n",
    "\n",
    "# # Gather the top results' texts\n",
    "# top_chunks = [hit.payload[\"text\"] for hit in search_result]\n",
    "\n",
    "# # Optionally, synthesize an answer using your LLM\n",
    "# context_str = \"\\n\\n\".join(top_chunks)\n",
    "# prompt = template.format(context_str=context_str, query_str=query)\n",
    "\n",
    "# response = llm.complete(prompt)\n",
    "# print(response)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7c6c1af9",
   "metadata": {},
   "source": [
    "# 8. Reranking"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "eee93a22",
   "metadata": {},
   "source": [
    "Here, we use a cross-encoder to re-rank the document chunks. Also, we limit the output to the top 3 most relevant chunks based on the model’s scoring."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 70,
   "id": "a04ba8a9",
   "metadata": {},
   "outputs": [],
   "source": [
    "from llama_index.core.postprocessor import SentenceTransformerRerank\n",
    "\n",
    "rerank = SentenceTransformerRerank(\n",
    "    model=\"cross-encoder/ms-marco-MiniLM-L-2-v2\", \n",
    "    top_n=3\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 71,
   "id": "0739096d",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "SentenceTransformerRerank(callback_manager=<llama_index.core.callbacks.base.CallbackManager object at 0x3e5c21df0>, model='cross-encoder/ms-marco-MiniLM-L-2-v2', top_n=3, device='mps', keep_retrieval_score=False, trust_remote_code=False)"
      ]
     },
     "execution_count": 71,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "rerank"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bb166d08",
   "metadata": {},
   "source": [
    "# 9. Query the document"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 92,
   "id": "ad1dcfe2",
   "metadata": {},
   "outputs": [],
   "source": [
    "query_engine = index.as_query_engine(similarity_top_k=10,\n",
    "                                     node_postprocessors=[rerank])\n",
    "\n",
    "query_engine.update_prompts(\n",
    "    {\"response_synthesizer:text_qa_template\": qa_prompt_tmpl}\n",
    ")\n",
    "\n",
    "# response = query_engine.query(\"What exactly is DSPy?\")\n",
    "# response = query_engine.query(\"How is DSPy pronounced?\")\n",
    "# response = query_engine.query(\"What is the github repo for docling?\")\n",
    "response = query_engine.query(\"What is the TTS for docling with pypdfium backend, when running on an Apple M3 Max using 4 threads for the test dataset of 225 pages?\")\n",
    "# response = query_engine.query(\"Which is the RationalAI product that addresses customer care?\")\n",
    "# response = query_engine.query(\"What is the point with the Rational control room?\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 93,
   "id": "2d1a01d1",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/markdown": [
       "To determine the Time-to-Solution (TTS) for Docling with the pypdfium backend, when running on an Apple M3 Max using 4 threads for the test dataset of 225 pages, we need to follow these steps:\n",
       "\n",
       "1. First, let's establish the reference numbers for the processing speed of Docling and the resource budget it requires.\n",
       "2. We know that all tests in this section are run with default options on our standard test set distributed with Docling, which consists of three papers from arXiv and two IBM Redbooks, with a total of 225 pages.\n",
       "3. Measurements were taken using both available PDF backends on two different hardware systems: one MacBook Pro M3 Max, and one bare-metal server running Ubuntu 20.04 LTS on an Intel Xeon E5-2690 CPU.\n",
       "\n",
       "Given that we fixed the thread budget (through setting OMP NUM THREADS environment variable) once to 4 (Docling default) and once to 16 (equal to full core count on the test hardware), let's assume we are running with 4 threads for this specific test case.\n",
       "\n",
       "Now, let's look at Table 1:\n",
       "\n",
       "| CPU                         | Thread budget   | native backend   | native backend   | native backend   | native backend   | pypdfium backend   | pypdfium backend   | pypdfium backend   |\n",
       "|-----------------------------|-----------------|------------------|------------------|------------------|--------------------|--------------------|--------------------|\n",
       "|                             |                 | TTS              | Pages/s          | Mem              | TTS                | Pages/s            | Mem                |\n",
       "| Apple M3 Max                | 4               | 177 s 167 s      | 1.27 1.34        | 6.20 GB          | 103 s 92 s         | 2.18 2.45          | 2.56 GB            |\n",
       "\n",
       "Since we are running on an Apple M3 Max with 4 threads, the TTS value for pypdfium backend is 177 s 167 s.\n",
       "\n",
       "Therefore, the Time-to-Solution (TTS) for Docling with the pypdfium backend, when running on an Apple M3 Max using 4 threads for the test dataset of 225 pages, is 177 seconds."
      ],
      "text/plain": [
       "<IPython.core.display.Markdown object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "from IPython.display import Markdown, display\n",
    "\n",
    "display(Markdown(str(response)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 94,
   "id": "8275c694",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'68e1a3c1-4255-4a33-b373-b52195725c43': {},\n",
       " 'bdf910fb-7668-4faa-8ab6-57ad870626c0': {},\n",
       " 'd5b4aeb1-4793-43af-88fc-7d86d4e79506': {}}"
      ]
     },
     "execution_count": 94,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "response.metadata"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 95,
   "id": "deb85e2c",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "\"To determine the Time-to-Solution (TTS) for Docling with the pypdfium backend, when running on an Apple M3 Max using 4 threads for the test dataset of 225 pages, we need to follow these steps:\\n\\n1. First, let's establish the reference numbers for the processing speed of Docling and the resource budget it requires.\\n2. We know that all tests in this section are run with default options on our standard test set distributed with Docling, which consists of three papers from arXiv and two IBM Redbooks, with a total of 225 pages.\\n3. Measurements were taken using both available PDF backends on two different hardware systems: one MacBook Pro M3 Max, and one bare-metal server running Ubuntu 20.04 LTS on an Intel Xeon E5-2690 CPU.\\n\\nGiven that we fixed the thread budget (through setting OMP NUM THREADS environment variable) once to 4 (Docling default) and once to 16 (equal to full core count on the test hardware), let's assume we are running with 4 threads for this specific test case.\\n\\nNow, let's look at Table 1:\\n\\n| CPU                         | Thread budget   | native backend   | native backend   | native backend   | native backend   | pypdfium backend   | pypdfium backend   | pypdfium backend   |\\n|-----------------------------|-----------------|------------------|------------------|------------------|--------------------|--------------------|--------------------|\\n|                             |                 | TTS              | Pages/s          | Mem              | TTS                | Pages/s            | Mem                |\\n| Apple M3 Max                | 4               | 177 s 167 s      | 1.27 1.34        | 6.20 GB          | 103 s 92 s         | 2.18 2.45          | 2.56 GB            |\\n\\nSince we are running on an Apple M3 Max with 4 threads, the TTS value for pypdfium backend is 177 s 167 s.\\n\\nTherefore, the Time-to-Solution (TTS) for Docling with the pypdfium backend, when running on an Apple M3 Max using 4 threads for the test dataset of 225 pages, is 177 seconds.\""
      ]
     },
     "execution_count": 95,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "response.response"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "41bfbad3",
   "metadata": {},
   "source": [
    "## Debug"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 96,
   "id": "ee97f830",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'<!-- image -->\\n\\n## Docling Technical Report\\n\\nVersion 1.0\\n\\nChristoph Auer Maksym Lysak Ahmed Nassar Michele Dolfi Nikolaos Livathinos Panos Vagenas Cesar Berrospi Ramis Matteo Omenetti Fabian Lindlbauer Kasper Dinkla Lokesh Mishra Yusik Kim Shubham Gupta Rafael Teixeira de Lima Valery Weber Lucas Morin Ingmar Meijer Viktor Kuropiatnyk Peter W. J. Staar\\n\\nAI4K Group, IBM Research R¨ uschlikon, Switzerland\\n\\n## Abstract\\n\\nThis technical report introduces Docling , an easy to use, self-contained, MITlicensed open-source package for PDF document conversion. It is powered by state-of-the-art specialized AI models for layout analysis (DocLayNet) and table structure recognition (TableFormer), and runs efficiently on commodity hardware in a small resource budget. The code interface allows for easy extensibility and addition of new features and models.\\n\\n## 1 Introduction\\n\\nConverting PDF documents back into a machine-processable format has been a major challenge for decades due to their huge variability in formats, weak standardization and printing-optimized characteristic, which discards most structural features and metadata. With the advent of LLMs and popular application patterns such as retrieval-augmented generation (RAG), leveraging the rich content embedded in PDFs has become ever more relevant. In the past decade, several powerful document understanding solutions have emerged on the market, most of which are commercial software, cloud offerings [3] and most recently, multi-modal vision-language models. As of today, only a handful of open-source tools cover PDF conversion, leaving a significant feature and quality gap to proprietary solutions.\\n\\nWith Docling , we open-source a very capable and efficient document conversion tool which builds on the powerful, specialized AI models and datasets for layout analysis and table structure recognition we developed and presented in the recent past [12, 13, 9]. Docling is designed as a simple, self-contained python library with permissive license, running entirely locally on commodity hardware. Its code architecture allows for easy extensibility and addition of new features and models.\\n\\nHere is what Docling delivers today:\\n\\n- · Converts PDF documents to JSON or Markdown format, stable and lightning fast\\n- · Understands detailed page layout, reading order, locates figures and recovers table structures\\n- · Extracts metadata from the document, such as title, authors, references and language\\n- · Optionally applies OCR, e.g. for scanned PDFs\\n- · Can be configured to be optimal for batch-mode (i.e high throughput, low time-to-solution) or interactive mode (compromise on efficiency, low time-to-solution)\\n- · Can leverage different accelerators (GPU, MPS, etc).\\n\\n## 2 Getting Started\\n\\nTo use Docling, you can simply install the docling package from PyPI. Documentation and examples are available in our GitHub repository at github.com/DS4SD/docling. All required model assets 1 are downloaded to a local huggingface datasets cache on first use, unless you choose to pre-install the model assets in advance.\\n\\nDocling provides an easy code interface to convert PDF documents from file system, URLs or binary streams, and retrieve the output in either JSON or Markdown format. For convenience, separate methods are offered to convert single documents or batches of documents. A basic usage example is illustrated below. Further examples are available in the Doclign code repository.\\n\\n```\\nfrom docling.document_converter import DocumentConverter Large\\n```\\n\\n```\\nsource = \"https://arxiv.org/pdf/2206.01062\" # PDF path or URL converter = DocumentConverter() result = converter.convert_single(source) print(result.render_as_markdown()) # output: \"## DocLayNet: A Human -Annotated Dataset for Document -Layout Analysis [...]\"\\n```\\n\\nOptionally, you can configure custom pipeline features and runtime options, such as turning on or off features (e.g. OCR, table structure recognition), enforcing limits on the input document size, and defining the budget of CPU threads. Advanced usage examples and options are documented in the README file. Docling also provides a Dockerfile to demonstrate how to install and run it inside a container.\\n\\n## 3 Processing pipeline\\n\\nDocling implements a linear pipeline of operations, which execute sequentially on each given document (see Fig. 1). Each document is first parsed by a PDF backend, which retrieves the programmatic text tokens, consisting of string content and its coordinates on the page, and also renders a bitmap image of each page to support downstream operations. Then, the standard model pipeline applies a sequence of AI models independently on every page in the document to extract features and content, such as layout and table structures. Finally, the results from all pages are aggregated and passed through a post-processing stage, which augments metadata, detects the document language, infers reading-order and eventually assembles a typed document object which can be serialized to JSON or Markdown.\\n\\n## 3.1 PDF backends\\n\\nTwo basic requirements to process PDF documents in our pipeline are a) to retrieve all text content and their geometric coordinates on each page and b) to render the visual representation of each page as it would appear in a PDF viewer. Both these requirements are encapsulated in Docling\\'s PDF backend interface. While there are several open-source PDF parsing libraries available for python, we faced major obstacles with all of them for different reasons, among which were restrictive\\n\\n1 see huggingface.co/ds4sd/docling-models/\\n\\nFigure 1: Sketch of Docling\\'s default processing pipeline. The inner part of the model pipeline is easily customizable and extensible.\\n\\n<!-- image -->\\n\\nlicensing (e.g. pymupdf [7]), poor speed or unrecoverable quality issues, such as merged text cells across far-apart text tokens or table columns (pypdfium, PyPDF) [15, 14].\\n\\nWe therefore decided to provide multiple backend choices, and additionally open-source a custombuilt PDF parser, which is based on the low-level qpdf [4] library. It is made available in a separate package named docling-parse and powers the default PDF backend in Docling. As an alternative, we provide a PDF backend relying on pypdfium , which may be a safe backup choice in certain cases, e.g. if issues are seen with particular font encodings.\\n\\n## 3.2 AI models\\n\\nAs part of Docling, we initially release two highly capable AI models to the open-source community, which have been developed and published recently by our team. The first model is a layout analysis model, an accurate object-detector for page elements [13]. The second model is TableFormer [12, 9], a state-of-the-art table structure recognition model. We provide the pre-trained weights (hosted on huggingface) and a separate package for the inference code as docling-ibm-models . Both models are also powering the open-access deepsearch-experience, our cloud-native service for knowledge exploration tasks.\\n\\n## Layout Analysis Model\\n\\nOur layout analysis model is an object-detector which predicts the bounding-boxes and classes of various elements on the image of a given page. Its architecture is derived from RT-DETR [16] and re-trained on DocLayNet [13], our popular human-annotated dataset for document-layout analysis, among other proprietary datasets. For inference, our implementation relies on the onnxruntime [5].\\n\\nThe Docling pipeline feeds page images at 72 dpi resolution, which can be processed on a single CPU with sub-second latency. All predicted bounding-box proposals for document elements are post-processed to remove overlapping proposals based on confidence and size, and then intersected with the text tokens in the PDF to group them into meaningful and complete units such as paragraphs, section titles, list items, captions, figures or tables.\\n\\n## Table Structure Recognition\\n\\nThe TableFormer model [12], first published in 2022 and since refined with a custom structure token language [9], is a vision-transformer model for table structure recovery. It can predict the logical row and column structure of a given table based on an input image, and determine which table cells belong to column headers, row headers or the table body. Compared to earlier approaches, TableFormer handles many characteristics of tables, such as partial or no borderlines, empty cells, rows or columns, cell spans and hierarchy both on column-heading or row-heading level, tables with inconsistent indentation or alignment and other complexities. For inference, our implementation relies on PyTorch [2].\\n\\nThe Docling pipeline feeds all table objects detected in the layout analysis to the TableFormer model, by providing an image-crop of the table and the included text cells. TableFormer structure predictions are matched back to the PDF cells in post-processing to avoid expensive re-transcription text in the table image. Typical tables require between 2 and 6 seconds to be processed on a standard CPU, strongly depending on the amount of included table cells.\\n\\n## OCR\\n\\nDocling provides optional support for OCR, for example to cover scanned PDFs or content in bitmaps images embedded on a page. In our initial release, we rely on EasyOCR [1], a popular thirdparty OCR library with support for many languages. Docling, by default, feeds a high-resolution page image (216 dpi) to the OCR engine, to allow capturing small print detail in decent quality. While EasyOCR delivers reasonable transcription quality, we observe that it runs fairly slow on CPU (upwards of 30 seconds per page).\\n\\nWe are actively seeking collaboration from the open-source community to extend Docling with additional OCR backends and speed improvements.\\n\\n## 3.3 Assembly\\n\\nIn the final pipeline stage, Docling assembles all prediction results produced on each page into a well-defined datatype that encapsulates a converted document, as defined in the auxiliary package docling-core . The generated document object is passed through a post-processing model which leverages several algorithms to augment features, such as detection of the document language, correcting the reading order, matching figures with captions and labelling metadata such as title, authors and references. The final output can then be serialized to JSON or transformed into a Markdown representation at the users request.\\n\\n## 3.4 Extensibility\\n\\nDocling provides a straight-forward interface to extend its capabilities, namely the model pipeline. A model pipeline constitutes the central part in the processing, following initial document parsing and preceding output assembly, and can be fully customized by sub-classing from an abstract baseclass ( BaseModelPipeline ) or cloning the default model pipeline. This effectively allows to fully customize the chain of models, add or replace models, and introduce additional pipeline configuration parameters. To use a custom model pipeline, the custom pipeline class to instantiate can be provided as an argument to the main document conversion methods. We invite everyone in the community to propose additional or alternative models and improvements.\\n\\nImplementations of model classes must satisfy the python Callable interface. The \\\\_\\\\_call\\\\_\\\\_ method must accept an iterator over page objects, and produce another iterator over the page objects which were augmented with the additional features predicted by the model, by extending the provided PagePredictions data model accordingly.\\n\\n## 4 Performance\\n\\nIn this section, we establish some reference numbers for the processing speed of Docling and the resource budget it requires. All tests in this section are run with default options on our standard test set distributed with Docling, which consists of three papers from arXiv and two IBM Redbooks, with a total of 225 pages. Measurements were taken using both available PDF backends on two different hardware systems: one MacBook Pro M3 Max, and one bare-metal server running Ubuntu 20.04 LTS on an Intel Xeon E5-2690 CPU. For reproducibility, we fixed the thread budget (through setting OMP NUM THREADS environment variable ) once to 4 (Docling default) and once to 16 (equal to full core count on the test hardware). All results are shown in Table 1.\\n\\nIf you need to run Docling in very low-resource environments, please consider configuring the pypdfium backend. While it is faster and more memory efficient than the default docling-parse backend, it will come at the expense of worse quality results, especially in table structure recovery.\\n\\nEstablishing GPU acceleration support for the AI models is currently work-in-progress and largely untested, but may work implicitly when CUDA is available and discovered by the onnxruntime and\\n\\ntorch runtimes backing the Docling pipeline. We will deliver updates on this topic at in a future version of this report.\\n\\nTable 1: Runtime characteristics of Docling with the standard model pipeline and settings, on our test dataset of 225 pages, on two different systems. OCR is disabled. We show the time-to-solution (TTS), computed throughput in pages per second, and the peak memory used (resident set size) for both the Docling-native PDF backend and for the pypdfium backend, using 4 and 16 threads.\\n\\n| CPU                         | Thread budget   | native backend   | native backend   | native backend   | pypdfium backend   | pypdfium backend   | pypdfium backend   |\\n|-----------------------------|-----------------|------------------|------------------|------------------|--------------------|--------------------|--------------------|\\n|                             |                 | TTS              | Pages/s          | Mem              | TTS                | Pages/s            | Mem                |\\n| Apple M3 Max                | 4               | 177 s 167 s      | 1.27 1.34        | 6.20 GB          | 103 s 92 s         | 2.18 2.45          | 2.56 GB            |\\n| (16 cores) Intel(R) E5-2690 | 16 4 16         | 375 s 244 s      | 0.60 0.92        | 6.16 GB          | 239 s 143 s        | 0.94 1.57          | 2.42 GB            |\\n\\n## 5 Applications\\n\\nThanks to the high-quality, richly structured document conversion achieved by Docling, its output qualifies for numerous downstream applications. For example, Docling can provide a base for detailed enterprise document search, passage retrieval or classification use-cases, or support knowledge extraction pipelines, allowing specific treatment of different structures in the document, such as tables, figures, section structure or references. For popular generative AI application patterns, such as retrieval-augmented generation (RAG), we provide quackling , an open-source package which capitalizes on Docling\\'s feature-rich document output to enable document-native optimized vector embedding and chunking. It plugs in seamlessly with LLM frameworks such as LlamaIndex [8]. Since Docling is fast, stable and cheap to run, it also makes for an excellent choice to build document-derived datasets. With its powerful table structure recognition, it provides significant benefit to automated knowledge-base construction [11, 10]. Docling is also integrated within the open IBM data prep kit [6], which implements scalable data transforms to build large-scale multi-modal training datasets.\\n\\n## 6 Future work and contributions\\n\\nDocling is designed to allow easy extension of the model library and pipelines. In the future, we plan to extend Docling with several more models, such as a figure-classifier model, an equationrecognition model, a code-recognition model and more. This will help improve the quality of conversion for specific types of content, as well as augment extracted document metadata with additional information. Further investment into testing and optimizing GPU acceleration as well as improving the Docling-native PDF backend are on our roadmap, too.\\n\\nWe encourage everyone to propose or implement additional features and models, and will gladly take your inputs and contributions under review . The codebase of Docling is open for use and contribution, under the MIT license agreement and in alignment with our contributing guidelines included in the Docling repository. If you use Docling in your projects, please consider citing this technical report.\\n\\n## References\\n\\n- [1] J. AI. Easyocr: Ready-to-use ocr with 80+ supported languages. https://github.com/ JaidedAI/EasyOCR , 2024. Version: 1.7.0.\\n- [2] J. Ansel, E. Yang, H. He, N. Gimelshein, A. Jain, M. Voznesensky, B. Bao, P. Bell, D. Berard, E. Burovski, G. Chauhan, A. Chourdia, W. Constable, A. Desmaison, Z. DeVito, E. Ellison, W. Feng, J. Gong, M. Gschwind, B. Hirsh, S. Huang, K. Kalambarkar, L. Kirsch, M. Lazos, M. Lezcano, Y. Liang, J. Liang, Y. Lu, C. Luk, B. Maher, Y. Pan, C. Puhrsch, M. Reso, M. Saroufim, M. Y. Siraichi, H. Suk, M. Suo, P. Tillet, E. Wang, X. Wang, W. Wen, S. Zhang, X. Zhao, K. Zhou, R. Zou, A. Mathews, G. Chanan, P. Wu, and S. Chintala. Pytorch 2: Faster\\n\\nmachine learning through dynamic python bytecode transformation and graph compilation. In Proceedings of the 29th ACM International Conference on Architectural Support for Programming Languages and Operating Systems, Volume 2 (ASPLOS \\'24) . ACM, 4 2024. doi: 10.1145/3620665.3640366. URL https://pytorch.org/assets/pytorch2-2.pdf .\\n\\n- [3] C. Auer, M. Dolfi, A. Carvalho, C. B. Ramis, and P. W. Staar. Delivering document conversion as a cloud service with high throughput and responsiveness. In 2022 IEEE 15th International Conference on Cloud Computing (CLOUD) , pages 363-373. IEEE, 2022.\\n- [4] J. Berkenbilt. Qpdf: A content-preserving pdf document transformer, 2024. URL https: //github.com/qpdf/qpdf .\\n- [5] O. R. developers. Onnx runtime. https://onnxruntime.ai/ , 2024. Version: 1.18.1.\\n- [6] IBM. Data Prep Kit: a community project to democratize and accelerate unstructured data preparation for LLM app developers, 2024. URL https://github.com/IBM/ data-prep-kit .\\n- [7] A. S. Inc. PyMuPDF, 2024. URL https://github.com/pymupdf/PyMuPDF .\\n- [8] J. Liu. LlamaIndex, 11 2022. URL https://github.com/jerryjliu/llama\\\\_index .\\n- [9] M. Lysak, A. Nassar, N. Livathinos, C. Auer, and P. Staar. Optimized Table Tokenization for Table Structure Recognition. In Document Analysis and Recognition - ICDAR 2023: 17th International Conference, San Jos´ e, CA, USA, August 21-26, 2023, Proceedings, Part II , pages 37-50, Berlin, Heidelberg, Aug. 2023. Springer-Verlag. ISBN 978-3-031-41678-1. doi: 10. 1007/978-3-031-41679-8 3. URL https://doi.org/10.1007/978-3-031-41679-8\\\\_3 .\\n- [10] L. Mishra, S. Dhibi, Y. Kim, C. Berrospi Ramis, S. Gupta, M. Dolfi, and P. Staar. Statements: Universal information extraction from tables with large language models for ESG KPIs. In D. Stammbach, J. Ni, T. Schimanski, K. Dutia, A. Singh, J. Bingler, C. Christiaen, N. Kushwaha, V. Muccione, S. A. Vaghefi, and M. Leippold, editors, Proceedings of the 1st Workshop on Natural Language Processing Meets Climate Change (ClimateNLP 2024) , pages 193-214, Bangkok, Thailand, Aug. 2024. Association for Computational Linguistics. URL https://aclanthology.org/2024.climatenlp-1.15 .\\n- [11] L. Morin, V. Weber, G. I. Meijer, F. Yu, and P. W. J. Staar. Patcid: an open-access dataset of chemical structures in patent documents. Nature Communications , 15(1):6532, August 2024. ISSN 2041-1723. doi: 10.1038/s41467-024-50779-y. URL https://doi.org/10.1038/ s41467-024-50779-y .\\n- [12] A. Nassar, N. Livathinos, M. Lysak, and P. Staar. Tableformer: Table structure understanding with transformers. In Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition , pages 4614-4623, 2022.\\n- [13] B. Pfitzmann, C. Auer, M. Dolfi, A. S. Nassar, and P. Staar. Doclaynet: a large humanannotated dataset for document-layout segmentation. pages 3743-3751, 2022.\\n- [14] pypdf Maintainers. pypdf: A Pure-Python PDF Library, 2024. URL https://github.com/ py-pdf/pypdf .\\n- [15] P. Team. PyPDFium2: Python bindings for PDFium, 2024. URL https://github.com/ pypdfium2-team/pypdfium2 .\\n- [16] Y. Zhao, W. Lv, S. Xu, J. Wei, G. Wang, Q. Dang, Y. Liu, and J. Chen. Detrs beat yolos on real-time object detection, 2023.\\n\\n## Appendix\\n\\nIn this section, we illustrate a few examples of Docling\\'s output in Markdown and JSON.\\n\\n## DocLayNet: A Large Human-Annotated Dataset for Document-Layout Analysis\\n\\n## DocLayNet: A Large Human-Annotated Dataset for Document-Layout Analysis\\n\\nBirgit Pfitzmann IBM Research Rueschlikon, Switzerland bpf@zurich.ibm.com\\n\\nChristoph Auer IBM Research Rueschlikon, Switzerland cau@zurich.ibm.com\\n\\nMichele Dolfi IBM Research Rueschlikon, Switzerland dol@zurich.ibm.com\\n\\nAhmed S. Nassar IBM Research Rueschlikon, Switzerland ahn@zurich.ibm.com\\n\\nPeter Staar IBM Research Rueschlikon, Switzerland taa@zurich.ibm.com\\n\\n## ABSTRACT\\n\\nAccurate document layout analysis is a key requirement for highquality PDF document conversion. With the recent availability of public, large ground-truth datasets such as PubLayNet and DocBank, deep-learning models have proven to be very effective at layout detection and segmentation. While these datasets are of adequate size to train such models, they severely lack in layout variability since they are sourced from scientific article repositories such as PubMed and arXiv only. Consequently, the accuracy of the layout segmentation drops significantly when these models are applied on more challenging and diverse layouts. In this paper, we present DocLayNet , a new, publicly available, document-layout annotation dataset in COCO format. It contains 80863 manually annotated pages from diverse data sources to represent a wide variability in layouts. For each PDF page, the layout annotations provide labelled bounding-boxes with a choice of 11 distinct classes. DocLayNet also provides a subset of double- and triple-annotated pages to determine the inter-annotator agreement. In multiple experiments, we provide baseline accuracy scores (in mAP) for a set of popular object detection models. We also demonstrate that these models fall approximately 10% behind the inter-annotator agreement. Furthermore, we provide evidence that DocLayNet is of sufficient size. Lastly, we compare models trained on PubLayNet, DocBank and DocLayNet, showing that layout predictions of the DocLayNettrained models are more robust and thus the preferred choice for general-purpose document-layout analysis.\\n\\n## CCS CONCEPTS\\n\\n· Informationsystems → Documentstructure ; · Appliedcomputing → Document analysis ; · Computing methodologies → Machine learning Computer vision ; ; Object detection ;\\n\\nPermission to make digital or hard copies of part or all of this work for personal or classroom use is granted without fee provided that copies are not made or distributed for profit or commercial advantage and that copies bear this notice and the full citation on the first page. Copyrights for third-party components of this work must be honored. For all other uses, contact the owner/author(s). KDD \\'22, August 14-18, 2022, Washington, DC, USA © 2022 Copyright held by the owner/author(s). ACM ISBN 978-1-4503-9385-0/22/08. https://doi.org/10.1145/3534678.3539043\\n\\nBirgit Pfitzmann IBM Research Rueschlikon, Switzerland bpf@zurich.ibm.com\\n\\nChristoph Auer IBM Research Rueschlikon, Switzerland cau@zurich.ibm.com\\n\\nMichele Dolfi IBM Research Rueschlikon, Switzerland dol@zurich.ibm.com\\n\\nAhmed S. Nassar IBM Research Rueschlikon, Switzerland ahn@zurich.ibm.com\\n\\nPeter Staar IBM Research Rueschlikon, Switzerland taa@zurich.ibm.com\\n\\n## ABSTRACT\\n\\nAccurate document layout analysis is a key requirement for highquality PDF document conversion. With the recent availability of public, large groundtruth datasets such as PubLayNet and DocBank, deep-learning models have proven to be very effective at layout detection and segmentation. While these datasets are of adequate size to train such models, they severely lack in layout variability since they are sourced from scientific article repositories such as PubMed and arXiv only. Consequently, the accuracy of the layout segmentation drops significantly when these models are applied on more challenging and diverse layouts. In this paper, we present DocLayNet , a new, publicly available, document-layout annotation dataset in COCO format. It contains 80863 manually annotated pages from diverse data sources to represent a wide variability in layouts. For each PDF page, the layout annotations provide labelled bounding-boxes with a choice of 11 distinct classes. DocLayNet also provides a subset of double- and triple-annotated pages to determine the inter-annotator agreement. In multiple experiments, we provide baseline accuracy scores (in mAP) for a set of popular object detection models. We also demonstrate that these models fall approximately 10% behind the inter-annotator agreement. Furthermore, we provide evidence that DocLayNet is of sufficient size. Lastly, we compare models trained on PubLayNet, DocBank and DocLayNet, showing that layout predictions of the DocLayNettrained models are more robust and thus the preferred choice for general-purpose document-layout analysis.\\n\\n## CCS CONCEPTS\\n\\nÆ Information systems → Document structure ; Æ Applied computing → Document analysis ; Æ Computing methodologies → Machine learning ; Computer vision ; Object detection ;\\n\\nPermission to make digital or hard copies of part or all of this work for personal or classroom use is granted without fee provided that copies are not made or distributed for profit or commercial advantage and that copies bear this notice and the full citation on the first page. Copyrights for third-party components of this work must be honored. For all other uses, contact the owner/author(s).\\n\\nKDD \\'22, August 14-18, 2022, Washington, DC, USA \\' 2022 Copyright held by the owner/author(s). ACM ISBN 978-1-4503-9385-0/22/08. https://doi.org/10.1145/3534678.3539043\\n\\nFigure 1: Four examples of complex page layouts across different document categories\\n\\n## KEYWORDS\\n\\nPDF document conversion, layout segmentation, object-detection, data set, Machine Learning\\n\\n## ACM Reference Format:\\n\\nBirgit Pfitzmann, Christoph Auer, Michele Dolfi, Ahmed S. Nassar, and Peter Staar. 2022. DocLayNet: A Large Human-Annotated Dataset for DocumentLayout Analysis. In Proceedings of the 28th ACM SIGKDD Conference on Knowledge Discovery and Data Mining (KDD \\'22), August 14-18, 2022, Washington, DC, USA. ACM, New York, NY, USA, 9 pages. https://doi.org/10.1145/ 3534678.3539043\\n\\n<!-- image -->\\n\\n<!-- image -->\\n\\nAGL Energy Limited  ABN 74 1\\n\\n5 061 375\\n\\n<!-- image -->\\n\\n<!-- image -->\\n\\nFigure 1: Four examples of complex page layouts across different document categories\\n\\n## KEYWORDS\\n\\nPDF document conversion, layout segmentation, object-detection, data set, Machine Learning\\n\\n## ACMReference Format:\\n\\nBirgit Pfitzmann, Christoph Auer, Michele Dolfi, Ahmed S. Nassar, and Peter Staar. 2022. DocLayNet: A Large Human-Annotated Dataset for DocumentLayout Analysis. In Proceedings of the 28th ACM SIGKDD Conference on Knowledge Discovery and Data Mining (KDD \\'22), August 14-18, 2022, Washington, DC, USA. ACM, New York, NY, USA, 9 pages. https://doi.org/10.1145/ 3534678.3539043\\n\\n1 INTRODUCTION\\n\\nDespite the substantial improvements achieved with machine-learning (ML) approaches and deep neural networks in recent years, document conversion remains a challenging problem, as demonstrated by the numerous public competitions held on this topic [1-4]. The challenge originates from the huge variability in PDF documents regarding layout, language and formats (scanned, programmatic or a combination of both). Engineering a single ML model that can be applied on all types of documents and provides high-quality layout segmentation remains to this day extremely challenging [5]. To highlight the variability in document layouts, we show a few example documents from the DocLayNet dataset in Figure 1. Figure 2: Title page of the DocLayNet paper (arxiv.org/pdf/2206.01062) - left PDF, right rendered Markdown. If recognized, metadata such as authors are appearing first under the title. Text content inside figures is currently dropped, the caption is retained and linked to the figure in the JSON representation (not shown).\\n\\nKDD \\'22, August 14-18, 2022, Washington, DC, USA Birgit Pfitzmann, Christoph Auer, Michele Dolfi, Ahmed S. Nassar, and Peter Staar\\n\\nTable 2: Prediction performance (mAP@0.5-0.95) of object detection networks on DocLayNet test set. The MRCNN (Mask R-CNN) and FRCNN (Faster R-CNN) models with ResNet-50 or ResNet-101 backbone were trained based on the network architectures from the detectron2 model zoo (Mask R-CNN R50, R101-FPN 3x, Faster R-CNN R101-FPN 3x), with default configurations. The YOLO implementation utilized was YOLOv5x6 [13]. All models were initialised using pre-trained weights from the COCO 2017 dataset.\\n\\n|                                                                                                        | human                                                                   | MRCNN R50 R101                                                                                                          | FRCNN R101                                                  | YOLO v5x6                                                   |\\n|--------------------------------------------------------------------------------------------------------|-------------------------------------------------------------------------|-------------------------------------------------------------------------------------------------------------------------|-------------------------------------------------------------|-------------------------------------------------------------|\\n| Caption Footnote Formula List-item Page-footer Page-header Picture Section-header Table Text Title All | 84-89 83-91 83-85 87-88 93-94 85-89 69-71 83-84 77-81 84-86 60-72 82-83 | 68.4 71.5 70.9 71.8 60.1 63.4 81.2 80.8 61.6 59.3 71.9 70.0 71.7 72.7 67.6 69.3 82.2 82.9 84.6 85.8 76.7 80.4 72.4 73.5 | 70.1 73.7 63.5 81.0 58.9 72.0 72.0 68.4 82.2 85.4 79.9 73.4 | 77.7 77.2 66.2 86.2 61.1 67.9 77.1 74.6 86.3 88.1 82.7 76.8 |\\n\\nto avoid this at any cost in order to have clear, unbiased baseline numbers for human document-layout annotation. Third, we introduced the feature of snapping boxes around text segments to obtain a pixel-accurate annotation and again reduce time and effort. The CCS annotation tool automatically shrinks every user-drawn box to the minimum bounding-box around the enclosed text-cells for all purely text-based segments, which excludes only Table and Picture . For the latter, we instructed annotation staff to minimise inclusion of surrounding whitespace while including all graphical lines. A downside of snapping boxes to enclosed text cells is that some wrongly parsed PDF pages cannot be annotated correctly and need to be skipped. Fourth, we established a way to flag pages as rejected for cases where no valid annotation according to the label guidelines could be achieved. Example cases for this would be PDF pages that render incorrectly or contain layouts that are impossible to capture with non-overlapping rectangles. Such rejected pages are not contained in the final dataset. With all these measures in place, experienced annotation staff managed to annotate a single page in a typical timeframe of 20s to 60s, depending on its complexity.\\n\\n## 5 EXPERIMENTS\\n\\nThe primary goal of DocLayNet is to obtain high-quality ML models capable of accurate document-layout analysis on a wide variety of challenging layouts. As discussed in Section 2, object detection models are currently the easiest to use, due to the standardisation of ground-truth data in COCO format [16] and the availability of general frameworks such as detectron2 [17]. Furthermore, baseline numbers in PubLayNet and DocBank were obtained using standard object detection models such as Mask R-CNN and Faster R-CNN. As such, we will relate to these object detection methods in this\\n\\n<!-- image -->\\n\\nThird, achienec\\n\\n## EXPERIMENTS\\n\\nchalenongayouls ground-vuth dawa such WC\\n\\n<!-- image -->\\n\\nFigure 5: Prediction performance (mAP@0.5-0.95) of a Mask R-CNNnetworkwithResNet50backbonetrainedonincreasing fractions of the DocLayNet dataset. The learning curve flattens around the 80% mark, indicating that increasing the size of the DocLayNet dataset with similar data will not yield significantly better predictions.\\n\\npaper and leave the detailed evaluation of more recent methods mentioned in Section 2 for future work.\\n\\nIn this section, we will present several aspects related to the performance of object detection models on DocLayNet. Similarly as in PubLayNet, we will evaluate the quality of their predictions using mean average precision (mAP) with 10 overlaps that range from 0.5 to 0.95 in steps of 0.05 (mAP@0.5-0.95). These scores are computed by leveraging the evaluation code provided by the COCO API [16].\\n\\n## Baselines for Object Detection\\n\\nIn Table 2, we present baseline experiments (given in mAP) on Mask R-CNN [12], Faster R-CNN [11], and YOLOv5 [13]. Both training and evaluation were performed on RGB images with dimensions of 1025 × 1025 pixels. For training, we only used one annotation in case of redundantly annotated pages. As one can observe, the variation in mAP between the models is rather low, but overall between 6 and 10% lower than the mAP computed from the pairwise human annotations on triple-annotated pages. This gives a good indication that the DocLayNet dataset poses a worthwhile challenge for the research community to close the gap between human recognition and ML approaches. It is interesting to see that Mask R-CNN and Faster R-CNN produce very comparable mAP scores, indicating that pixel-based image segmentation derived from bounding-boxes does not help to obtain better predictions. On the other hand, the more recent Yolov5x model does very well and even out-performs humans on selected labels such as Text , Table and Picture . This is not entirely surprising, as Text , Table and Picture are abundant and the most visually distinctive in a document.\\n\\ncoioct dcochon modols\\n\\n## Baselines for Object Detection\\n\\nmak enbrel\\n\\nFigure 3: Page 6 of the DocLayNet paper. If recognized, metadata such as authors are appearing first under the title. Elements recognized as page headers or footers are suppressed in Markdown to deliver uninterrupted content in reading order. Tables are inserted in reading order. The paragraph in \\'5. Experiments\\' wrapping over the column end is broken up in two and interrupted by the table.\\n\\nKDD \\'22, August 14-18, 2022, Washington, DC, USA\\n\\nBirgit Pfitzmann, Christoph Auer, Michele Dolfi, Ahmed S. Nassar, and Peter Staar\\n\\nTable 1: DocLayNet dataset overview. Along with the frequency of each class label, we present the relative occurrence (as %\\n\\nbetween pairwise annotations from the triple-annotated pages, from which we obtain accuracy ranges.\\n\\nof row \\'Total\\') in the train, test and validation sets. The inter-annotator agreement is computed as the mAP@0.5-0.95 metric\\n\\n<!-- image -->\\n\\n<!-- image -->\\n\\n| class label    | Count   | % of Total   | % of Total   | % of Total   | triple inter-annotator mAP @0.5-0.95 (%)   | triple inter-annotator mAP @0.5-0.95 (%)   | triple inter-annotator mAP @0.5-0.95 (%)   | triple inter-annotator mAP @0.5-0.95 (%)   | triple inter-annotator mAP @0.5-0.95 (%)   | triple inter-annotator mAP @0.5-0.95 (%)   | triple inter-annotator mAP @0.5-0.95 (%)   |\\n|----------------|---------|--------------|--------------|--------------|--------------------------------------------|--------------------------------------------|--------------------------------------------|--------------------------------------------|--------------------------------------------|--------------------------------------------|--------------------------------------------|\\n| class label    | Count   | Train        | Test         | Val          | All                                        | Fin                                        | Man                                        | Sci                                        | Law                                        | Pat                                        | Ten                                        |\\n| Caption        | 22524   | 2.04         | 1.77         | 2.32         | 84-89                                      | 40-61                                      | 86-92                                      | 94-99                                      | 95-99                                      | 69-78                                      | n/a                                        |\\n| Footnote       | 6318    | 0.60         | 0.31         | 0.58         | 83-91                                      | n/a                                        | 100                                        | 62-88                                      | 85-94                                      | n/a                                        | 82-97                                      |\\n| Formula        | 25027   | 2.25         | 1.90         | 2.96         | 83-85                                      | n/a                                        | n/a                                        | 84-87                                      | 86-96                                      | n/a                                        | n/a                                        |\\n| List-item      | 185660  | 17.19        | 13.34        | 15.82        | 87-88                                      | 74-83                                      | 90-92                                      | 97-97                                      | 81-85                                      | 75-88                                      | 93-95                                      |\\n| Page-footer    | 70878   | 6.51         | 5.58         | 6.00         | 93-94                                      | 88-90                                      | 95-96                                      | 100                                        | 92-97                                      | 100                                        | 96-98                                      |\\n| Page-header    | 58022   | 5.10         | 6.70         | 5.06         | 85-89                                      | 66-76                                      | 90-94                                      | 98-100                                     | 91-92                                      | 97-99                                      | 81-86                                      |\\n| Picture        | 45976   | 4.21         | 2.78         | 5.31         | 69-71                                      | 56-59                                      | 82-86                                      | 69-82                                      | 80-95                                      | 66-71                                      | 59-76                                      |\\n| Section-header | 142884  | 12.60        | 15.77        | 12.85        | 83-84                                      | 76-81                                      | 90-92                                      | 94-95                                      | 87-94                                      | 69-73                                      | 78-86                                      |\\n| Table          | 34733   | 3.20         | 2.27         | 3.60         | 77-81                                      | 75-80                                      | 83-86                                      | 98-99                                      | 58-80                                      | 79-84                                      | 70-85                                      |\\n| Text           | 510377  | 45.82        | 49.28        | 45.00        | 84-86                                      | 81-86                                      | 88-93                                      | 89-93                                      | 87-92                                      | 71-79                                      | 87-95                                      |\\n| Title          | 5071    | 0.47         | 0.30         | 0.50         | 60-72                                      | 24-63                                      | 50-63                                      | 94-100                                     | 82-96                                      | 68-79                                      | 24-56                                      |\\n| Total          | 1107470 | 941123       | 99816        | 66531        | 82-83                                      | 71-74                                      | 79-81                                      | 89-94                                      | 86-91                                      | 71-76                                      | 68-85                                      |\\n\\n<!-- image -->\\n\\ninclude publication repositories such as arXiv\\n\\nTable 1: DocLayNet dataset overview. Along with the frequency of each class label, we present the relative occurrence (as % of row \"Total\") in the train, test and validation sets. The inter-annotator agreement is computed as the mAP@0.5-0.95 metric between pairwise annotations from the triple-\\n\\nannotated pages, from which we obtain accuracy ranges.\\n\\n<!-- image -->\\n\\n|                       |         | %of Total   | %of Total   | %of Total   | triple inter- annotator mAP @ 0.5-0.95 (%)   | triple inter- annotator mAP @ 0.5-0.95 (%)   | triple inter- annotator mAP @ 0.5-0.95 (%)   | triple inter- annotator mAP @ 0.5-0.95 (%)   | triple inter- annotator mAP @ 0.5-0.95 (%)   | triple inter- annotator mAP @ 0.5-0.95 (%)   | triple inter- annotator mAP @ 0.5-0.95 (%)   |\\n|-----------------------|---------|-------------|-------------|-------------|----------------------------------------------|----------------------------------------------|----------------------------------------------|----------------------------------------------|----------------------------------------------|----------------------------------------------|----------------------------------------------|\\n| class label           | Count   | Train       | Test        | Val         | All                                          | Fin                                          | Man                                          | Sci                                          | Law                                          | Pat                                          | Ten                                          |\\n| Caption               | 22524   | 2.04        | 1.77        | 2.32        | 84-89                                        | 40-61                                        | 86-92                                        | 94-99                                        | 95-99                                        | 69-78                                        | n/a                                          |\\n| Footnote              | 6318    | 0.60        | 0.31        | 0.58        | 83-91                                        | n/a                                          | 100                                          | 62-88                                        | 85-94                                        | n/a                                          | 82-97                                        |\\n| Formula               | 25027   | 2.25        | 1.90        | 2.96        | 83-85                                        | n/a                                          | n/a                                          | 84-87                                        | 86-96                                        | n/a                                          | n/a                                          |\\n| List-item             | 185660  | 17.19       | 13.34       | 15.82       | 87-88                                        | 74-83                                        | 90-92                                        | 97-97                                        | 81-85                                        | 75-88                                        | 93-95                                        |\\n| Page- footer          | 70878   | 6.51        | 5.58        | 6.00        | 93-94                                        | 88-90                                        | 95-96                                        | 100                                          | 92-97                                        | 100                                          | 96-98                                        |\\n| Page- header offices, | 58022   | 5.10        | 6.70        | 5.06        | 85-89                                        | 66-76                                        | 90-94                                        | 98-100                                       | 91-92                                        | 97-99                                        | 81-86                                        |\\n| Picture               | 45976   | 4.21        | 2.78        | 5.31        | 69-71                                        | 56-59                                        | 82-86                                        | 69-82                                        | 80-95                                        | 66-71                                        | 59-76                                        |\\n| Section- header not   | 142884  | 12.60       | 15.77       | 12.85       | 83-84                                        | 76-81                                        | 90-92                                        | 94-95                                        | 87-94                                        | 69-73                                        | 78-86                                        |\\n| Table                 | 34733   | 3.20        | 2.27        | 3.60        | 77-81                                        | 75-80                                        | 83-86                                        | 98-99                                        | 58-80                                        | 79-84                                        | 70-85                                        |\\n| Text                  | 510377  | 45.82       | 49.28       | 45.00       | 84-86                                        | 81-86                                        | 88-93                                        | 89-93                                        | 87-92                                        | 71-79                                        | 87-95                                        |\\n| Title [22], a         | 5071    | 0.47        | 0.30        | 0.50        | 60-72                                        | 24-63                                        | 50-63                                        | 94-100                                       | 82-96                                        | 68-79                                        | 24-56                                        |\\n| Total in-             | 1107470 | 941123      | 99816       | 66531       | 82-83                                        | 71-74                                        | 79-81                                        | 89-94                                        | 86-91                                        | 71-76                                        | 68-85                                        |\\n\\n3\\n\\n,\\n\\ngovernment offices,\\n\\nWe reviewed the col-\\n\\n,\\n\\nPage-\\n\\nTitle and\\n\\n.\\n\\npage. Specificity ensures that the choice of label is not ambiguous,\\n\\n<!-- image -->\\n\\nwe distributed the annotation workload and performed continuous be annotated. We refrained from class labels that are very specific\\n\\nonly. For phases three and four, a group of 40 dedicated annotators while coverage ensures that all meaningful items on a page can\\n\\nquality controls. Phase one and two required a small team of experts to a document category, such as\\n\\nAbstract in the\\n\\nScientific Articles were assembled and supervised.\\n\\ncategory. We also avoided class labels that are tightly linked to the\\n\\nPhase 1: Data selection and preparation.\\n\\nOur inclusion cri-\\n\\nAuthor\\n\\nAffiliation\\n\\nteria for documents were described in Section 3. A large effort went into ensuring that all documents are free to use. The data sources in DocBank, are often only distinguishable by discriminating on 3 https://arxiv.org/ Figure 4: Table 1 from the DocLayNet paper in the original PDF (A), as rendered Markdown (B) and in JSON representation (C). Spanning table cells, such as the multi-column header \\'triple interannotator mAP@0.5-0.95 (%)\\', is repeated for each column in the Markdown representation (B), which guarantees that every data point can be traced back to row and column headings only by its grid coordinates in the table. In the JSON representation, the span information is reflected in the fields of each table cell (C).\\n\\nsemantics of the text. Labels such as and\\n\\n,\\n\\nas seen'"
      ]
     },
     "execution_count": 96,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "docs[0].text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 97,
   "id": "479a1b99",
   "metadata": {},
   "outputs": [],
   "source": [
    "from IPython.display import Markdown, display\n",
    "import re\n",
    "\n",
    "def highlight(text, query, color):\n",
    "        # Case-insensitive highlight\n",
    "        pattern = re.compile(re.escape(query), re.IGNORECASE)\n",
    "        return pattern.sub(f\"<mark style='background-color:{color};'>{query}</mark>\", text)\n",
    "\n",
    "def display_sources_with_highlight(response, docs, query, highlight_color=\"#ffff00\"):\n",
    "    \"\"\"\n",
    "    Display source documents for the response, highlighting the query in the text.\n",
    "    \"\"\"\n",
    "    \n",
    "    for source in response.metadata.values():\n",
    "        source_document = source.get(\"file_name\")\n",
    "        source_page = source.get(\"page_label\")\n",
    "        if source_page:\n",
    "            # We need also to filter per document file_name\n",
    "            d = next((doc for doc in docs if doc.metadata.get(\"file_name\") == source_document and doc.metadata.get(\"page_label\") == source_page), None)\n",
    "            if d:\n",
    "                highlighted = highlight(d.text, query, highlight_color)\n",
    "                display(Markdown(f\"### Source Document (page_label: {source_page})\\n\\n{highlighted}\"))\n",
    "\n",
    "# Example usage:\n",
    "word = \"TTS\"\n",
    "display_sources_with_highlight(response, docs, word)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "11d1f031",
   "metadata": {},
   "source": [
    "Let's try retrieving the docs containing the query instead"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 83,
   "id": "ac2e58dd",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Write an inline function to find the first document containing a specific text\n",
    "def find_document_with_text(text):\n",
    "    for i, doc in enumerate(docs):\n",
    "        if text in doc.text:\n",
    "            return i, doc\n",
    "    return None, None\n",
    "\n",
    "i, doc = find_document_with_text(word)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "404a282b",
   "metadata": {},
   "outputs": [],
   "source": [
    "doc"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8ad8817b",
   "metadata": {},
   "outputs": [],
   "source": [
    "Markdown(highlight(doc.text, word, \"#ffff00\"))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
