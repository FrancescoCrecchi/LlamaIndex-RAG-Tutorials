{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "d497e57e",
   "metadata": {},
   "source": [
    "# 1. Setup Asyncio"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "0b7c49a4",
   "metadata": {},
   "outputs": [],
   "source": [
    "import nest_asyncio\n",
    "\n",
    "nest_asyncio.apply()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8d74b235",
   "metadata": {},
   "source": [
    "# 2. Setup the Qdrant vector database"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3108311b",
   "metadata": {},
   "source": [
    "Let's now connect to out qdrant database to store the collection of documents we will use for RAG. \n",
    "We will use the `qdrant_client` library to interact with the Qdrant database."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "bc42de2b",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/fc/experiments/rag-project/.venv/lib/python3.9/site-packages/urllib3/__init__.py:35: NotOpenSSLWarning: urllib3 v2 only supports OpenSSL 1.1.1+, currently the 'ssl' module is compiled with 'LibreSSL 2.8.3'. See: https://github.com/urllib3/urllib3/issues/3020\n",
      "  warnings.warn(\n"
     ]
    }
   ],
   "source": [
    "import qdrant_client\n",
    "\n",
    "collection_name = \"chat_with_docs_docling\"\n",
    "\n",
    "client = qdrant_client.QdrantClient(\n",
    "    host=\"localhost\",\n",
    "    port=6333,\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "08dcc9f8",
   "metadata": {},
   "source": [
    "# 3. Read the documents"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2f2b58ad",
   "metadata": {},
   "source": [
    "We are now reading the documents using the docling library. For each document in the `docs` folder, we extract images and tables, in addition to its text."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "82745a5d",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/fc/experiments/rag-project/.venv/lib/python3.9/site-packages/tqdm/auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n",
      "/Users/fc/experiments/rag-project/.venv/lib/python3.9/site-packages/torch/utils/data/dataloader.py:683: UserWarning: 'pin_memory' argument is set as true but not supported on MPS now, then device pinned memory won't be used.\n",
      "  warnings.warn(warn_msg)\n",
      "/Users/fc/experiments/rag-project/.venv/lib/python3.9/site-packages/torch/utils/data/dataloader.py:683: UserWarning: 'pin_memory' argument is set as true but not supported on MPS now, then device pinned memory won't be used.\n",
      "  warnings.warn(warn_msg)\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "from glob import glob\n",
    "from pathlib import Path\n",
    "\n",
    "from docling.datamodel.base_models import InputFormat\n",
    "from docling.document_converter import DocumentConverter, PdfFormatOption\n",
    "from docling.datamodel.pipeline_options import PdfPipelineOptions, TableFormerMode\n",
    "from docling_core.types.doc import ImageRefMode, PictureItem, TableItem\n",
    "\n",
    "\n",
    "pdf_dir = \"./docs\"\n",
    "pdf_files = glob(os.path.join(pdf_dir, \"*.pdf\"))\n",
    "md_files = []\n",
    "\n",
    "IMAGE_RESOLUTION_SCALE = 2.0\n",
    "output_dir = Path(\"processed_docs\")\n",
    "output_dir.mkdir(parents=True, exist_ok=True)\n",
    "\n",
    "# Pipeline options for PDF conversion\n",
    "pipeline_options = PdfPipelineOptions(do_table_structure=True)\n",
    "# Table options\n",
    "pipeline_options.table_structure_options.mode = TableFormerMode.ACCURATE  # use more accurate TableFormer model\n",
    "# Image options\n",
    "pipeline_options.images_scale = IMAGE_RESOLUTION_SCALE\n",
    "pipeline_options.generate_picture_images = True\n",
    "pipeline_options.generate_page_images = True        # This is needed to generate table images\n",
    "\n",
    "converter = DocumentConverter(\n",
    "    format_options={\n",
    "        InputFormat.PDF: PdfFormatOption(pipeline_options=pipeline_options)\n",
    "    }\n",
    ")\n",
    "\n",
    "def replace_artifact_images(md_path, doc_filename):\n",
    "    import re\n",
    "\n",
    "    \"\"\"\n",
    "    Replace image links to _artifacts with links to your own saved images.\n",
    "    \"\"\"\n",
    "    md_text = Path(md_path).read_text()\n",
    "    # Replace artifact image links with your own pictures/picture-{n}.png\n",
    "    # Example: ![](docling_artifacts/image_000000_abc123.png) -> ![](docling/pictures/picture-1.png)\n",
    "    artifact_img_regex = re.compile(r'!\\[.*?\\]\\((?:.*?_artifacts/)?image_\\d+_[a-f0-9]+\\.png\\)')\n",
    "    picture_idx = 1\n",
    "\n",
    "    def replacer(match):\n",
    "        nonlocal picture_idx\n",
    "        new_link = f\"![]({doc_filename}/pictures/picture-{picture_idx}.png)\"\n",
    "        picture_idx += 1\n",
    "        return new_link\n",
    "\n",
    "    return artifact_img_regex.sub(replacer, md_text)\n",
    "\n",
    "# Convert each PDF to Markdown using Docling's Python API\n",
    "ddocs = []\n",
    "for pdf_path in pdf_files:\n",
    "    result = converter.convert(pdf_path)\n",
    "    ddocs.append(result.document)\n",
    "\n",
    "    # Save images of figures and tables\n",
    "    doc_filename = result.input.file.stem\n",
    "    table_counter = 0\n",
    "    picture_counter = 0\n",
    "\n",
    "    # Need to create document directories for images and tables\n",
    "    (output_dir / doc_filename / \"tables\").mkdir(parents=True, exist_ok=True)\n",
    "    (output_dir / doc_filename / \"pictures\").mkdir(parents=True, exist_ok=True)\n",
    "\n",
    "    for element, _level in result.document.iterate_items():\n",
    "        if isinstance(element, TableItem):\n",
    "            table_counter += 1\n",
    "            element_image_filename = (\n",
    "                output_dir / doc_filename / \"tables\" / f\"table-{table_counter}.png\"\n",
    "            )\n",
    "            with element_image_filename.open(\"wb\") as fp:\n",
    "                element.get_image(result.document).save(fp, \"PNG\")\n",
    "\n",
    "        if isinstance(element, PictureItem):\n",
    "            picture_counter += 1\n",
    "            element_image_filename = (\n",
    "                output_dir / doc_filename / \"pictures\" / f\"picture-{picture_counter}.png\"\n",
    "            )\n",
    "            with element_image_filename.open(\"wb\") as fp:\n",
    "                element.get_image(result.document).save(fp, \"PNG\")\n",
    "\n",
    "    # Save markdown with externally referenced pictures\n",
    "    md_filename = output_dir / f\"{doc_filename}.md\"\n",
    "    result.document.save_as_markdown(md_filename, image_mode=ImageRefMode.REFERENCED)\n",
    "\n",
    "    # Replace artifact image links with your own pictures/picture-{n}.png\n",
    "    md_text = replace_artifact_images(md_filename, doc_filename)\n",
    "    md_filename.write_text(md_text)\n",
    "\n",
    "    # Cleanup up artifacts directory if it exists\n",
    "    artifacts_dir = output_dir / f\"{doc_filename}_artifacts\"\n",
    "    if artifacts_dir.exists():\n",
    "        import shutil\n",
    "        shutil.rmtree(artifacts_dir)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "32c3bc55",
   "metadata": {},
   "source": [
    "## 3.1 Inspecting Docling Documents"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "id": "88d09d50",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'docling.pdf'"
      ]
     },
     "execution_count": 66,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "ddoc = ddocs[0]  # Use the first document for indexing\n",
    "ddoc.origin.filename"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ba129a4d",
   "metadata": {},
   "source": [
    "Does this document has pictures in it?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "id": "5dac38cd",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "13"
      ]
     },
     "execution_count": 67,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(ddoc.pictures)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1df9c28e",
   "metadata": {},
   "source": [
    "Let's inspect one to find its provenance wrt the original document"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "id": "ccb2a44c",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/var/folders/8w/7z6k0z3n1jd9jqhxwh808hbm0000gn/T/ipykernel_88970/3201237041.py:3: PydanticDeprecatedSince20: The `dict` method is deprecated; use `model_dump` instead. Deprecated in Pydantic V2.0 to be removed in V3.0. See Pydantic V2 Migration Guide at https://errors.pydantic.dev/2.11/migration/\n",
      "  provenance.dict()\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "{'page_no': 3,\n",
       " 'bbox': {'l': 110.07231140136719,\n",
       "  't': 719.2913360595703,\n",
       "  'r': 500.7577209472656,\n",
       "  'b': 581.2926177978516,\n",
       "  'coord_origin': <CoordOrigin.BOTTOMLEFT: 'BOTTOMLEFT'>},\n",
       " 'charspan': (0, 0)}"
      ]
     },
     "execution_count": 41,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "image = ddoc.pictures[1]\n",
    "provenance = image.prov[0]\n",
    "provenance.dict()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7ee4bd0a",
   "metadata": {},
   "source": [
    "## 3.2 Bonus: Extracting images using their provenance"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "id": "e25ac1c3",
   "metadata": {},
   "outputs": [],
   "source": [
    "import fitz  # PyMuPDF\n",
    "\n",
    "def save_image_from_provenance(pdf_path, provenance, output_path):\n",
    "    \"\"\"\n",
    "    Extracts and saves an image from a PDF using provenance info.\n",
    "    provenance should have 'page_ix' (0-based) and 'bbox' ([x0, y0, x1, y1])\n",
    "    \"\"\"\n",
    "    page_ix = provenance.page_no-1  # Convert to 0-based index\n",
    "    doc = fitz.open(pdf_path)\n",
    "    page = doc[page_ix]\n",
    "\n",
    "    _bbox = provenance.bbox.to_top_left_origin(page_height=page.rect.height)\n",
    "    bbox = [_bbox.l, _bbox.t, _bbox.r, _bbox.b]  # Ensure bbox is a list of floats\n",
    "\n",
    "    clip = fitz.Rect(*bbox)\n",
    "    pix = page.get_pixmap(clip=clip)\n",
    "    pix.save(output_path)\n",
    "    doc.close()\n",
    "\n",
    "\n",
    "\n",
    "# Example usage:\n",
    "save_image_from_provenance(pdf_path, provenance, \"img_from_provenance.png\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ccc70c33",
   "metadata": {},
   "source": [
    "# 4. Performing RAG\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d5e146a0",
   "metadata": {
    "vscode": {
     "languageId": "markdown"
    }
   },
   "source": [
    "## 4.1 Loading documents\n",
    "\n",
    "We are now using the markdown version of the documents obtained through docling stored in the `processed_docs` dir"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 74,
   "id": "942f91c5",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "2"
      ]
     },
     "execution_count": 74,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from llama_index.core import SimpleDirectoryReader\n",
    "\n",
    "input_dir_path = \"./processed_docs\"\n",
    "\n",
    "loader = SimpleDirectoryReader(\n",
    "    input_dir=input_dir_path,\n",
    "    required_exts=[\".md\"],\n",
    "    recursive=True\n",
    ")\n",
    "\n",
    "docs = loader.load_data()\n",
    "len(docs)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "182a5dcc",
   "metadata": {},
   "source": [
    "Let's have a peek into the content retrieved from the PDF files"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 73,
   "id": "cb6b7707",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "======== Document: docling.md ========\n",
      "\n",
      "Content: ![](docling/pictures/picture-1.png)\n",
      "\n",
      "## Docling Technical Report\n",
      "\n",
      "Version 1.0\n",
      "\n",
      "Christoph Auer Maksym\n",
      "Metadata: {'file_path': '/Users/fc/experiments/rag-project/processed_docs/docling.md', 'file_name': 'docling.md', 'file_size': 49283, 'creation_date': '2025-07-22', 'last_modified_date': '2025-07-22'}\n",
      "\n",
      "\n",
      "======== Document: dspy.md ========\n",
      "\n",
      "Content: ## DSPY: COMPILING DECLARATIVE LANGUAGE MODEL CALLS INTO SELF-IMPROVING PIPELINES\n",
      "\n",
      "Omar Khattab, 1 A\n",
      "Metadata: {'file_path': '/Users/fc/experiments/rag-project/processed_docs/dspy.md', 'file_name': 'dspy.md', 'file_size': 115012, 'creation_date': '2025-07-22', 'last_modified_date': '2025-07-22'}\n",
      "\n",
      "\n"
     ]
    }
   ],
   "source": [
    "for doc in docs:\n",
    "    print(f\"======== Document: {doc.metadata.get('file_name')} ========\\n\")\n",
    "    print(f\"Content: {doc.get_content()[:100]}\") # Print first 100 characters of content\n",
    "    print(f\"Metadata: {doc.metadata}\")\n",
    "    print(\"\\n\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f9028b52",
   "metadata": {},
   "source": [
    "## 4.2 Use Chonkie to chunk the documents"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "98559f22",
   "metadata": {},
   "outputs": [],
   "source": [
    "from chonkie import SemanticChunker\n",
    "from llama_index.core.schema import Document\n",
    "from llama_index.core import Settings\n",
    "from llama_index.embeddings.huggingface import HuggingFaceEmbedding\n",
    "\n",
    "\n",
    "semantic_chunker = SemanticChunker(\n",
    "    embedding_model=\"BAAI/bge-large-en-v1.5\",\n",
    "    threshold=0.5,\n",
    "    chunk_size=512,\n",
    "    min_sentences=1\n",
    ")\n",
    "\n",
    "embed_model = HuggingFaceEmbedding(model_name=\"BAAI/bge-large-en-v1.5\",\n",
    "                                   trust_remote_code=True)\n",
    "\n",
    "Settings.embed_model = embed_model\n",
    "\n",
    "all_chunks = []\n",
    "for doc in docs:\n",
    "    chunks = semantic_chunker.chunk(doc.text)\n",
    "    for chunk in chunks:\n",
    "        # Use LlamaIndex's embedding model to embed the chunk text\n",
    "        chunk_embedding = Settings.embed_model.get_text_embedding(chunk.text)\n",
    "        all_chunks.append(\n",
    "            Document(\n",
    "                text=chunk.text,\n",
    "                metadata=doc.metadata,\n",
    "                embedding=chunk_embedding\n",
    "            )\n",
    "        )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "a53758e8",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "90"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(all_chunks)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "id": "37ddb4e7",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "llama_index.core.schema.Document"
      ]
     },
     "execution_count": 51,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "type(all_chunks[0])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ce92f5ba",
   "metadata": {},
   "source": [
    "Chunks are llama_index `Document`s with their own metadata and embeddings. Those are the actual documents we will index in Qdrant. "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6189b8fe",
   "metadata": {},
   "source": [
    "## 4.3 Create Qdrant collection and index data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "398d3707",
   "metadata": {},
   "outputs": [],
   "source": [
    "from llama_index.vector_stores.qdrant import QdrantVectorStore\n",
    "from llama_index.core import VectorStoreIndex, StorageContext\n",
    "\n",
    "def create_index(documents):\n",
    "\n",
    "    vector_store = QdrantVectorStore(client=client,\n",
    "                                     collection_name=collection_name)\n",
    "    \n",
    "    storage_context = StorageContext.from_defaults(vector_store=vector_store)\n",
    "    \n",
    "    index = VectorStoreIndex.from_documents(documents,\n",
    "                                            storage_context=storage_context)\n",
    "    \n",
    "    return index"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "4cbe8452",
   "metadata": {},
   "outputs": [],
   "source": [
    "from llama_index.core import Settings\n",
    "\n",
    "index = create_index(all_chunks)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "cd5373f1",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "llama_index.core.indices.vector_store.base.VectorStoreIndex"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "type(index)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7c43420f",
   "metadata": {},
   "source": [
    "## 4.4 Load the LLM"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f6a81cb2",
   "metadata": {},
   "source": [
    "Now, it's time to define the LLM model we will use for querying the index. We are using Ollama as the LLM provider, but you can replace it with any other LLM provider supported by LlamaIndex.\n",
    "\n",
    "Please, make sure to have available the intended model locally. To do so, you can use the pull command. In a separate terminal, execute:\n",
    "```bash\n",
    "ollama pull llama3.2\n",
    "```\n",
    "and wait for the model to download. Once ready, continue with the next cell!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "id": "0df5efb3",
   "metadata": {},
   "outputs": [],
   "source": [
    "from llama_index.llms.ollama import Ollama\n",
    "from llama_index.core import Settings\n",
    "\n",
    "\n",
    "llm = Ollama(model=\"llama3.2\", request_timeout=120.0, temperature=0.0)\n",
    "\n",
    "Settings.llm = llm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "id": "7dd2a7ea",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(llama_index.core.settings._Settings,\n",
       " Ollama(callback_manager=<llama_index.core.callbacks.base.CallbackManager object at 0x43010ff70>, system_prompt=None, messages_to_prompt=<function messages_to_prompt at 0x4352e40d0>, completion_to_prompt=<function default_completion_to_prompt at 0x3f8043e50>, output_parser=None, pydantic_program_mode=<PydanticProgramMode.DEFAULT: 'default'>, query_wrapper_prompt=None, base_url='http://localhost:11434', model='llama3.2', temperature=0.0, context_window=-1, request_timeout=120.0, prompt_key='prompt', json_mode=False, additional_kwargs={}, is_function_calling_model=True, keep_alive=None, thinking=None),\n",
       " HuggingFaceEmbedding(model_name='BAAI/bge-large-en-v1.5', embed_batch_size=10, callback_manager=<llama_index.core.callbacks.base.CallbackManager object at 0x43010ff70>, num_workers=None, embeddings_cache=None, max_length=512, normalize=True, query_instruction=None, text_instruction=None, cache_folder=None, show_progress_bar=False))"
      ]
     },
     "execution_count": 54,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "type(Settings), Settings.llm, Settings.embed_model"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7b2f8e3f",
   "metadata": {},
   "source": [
    "## 4.5 Define the prompt template"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "89ac4eb5",
   "metadata": {},
   "outputs": [],
   "source": [
    "from llama_index.core import PromptTemplate\n",
    "\n",
    "template = \"\"\"Context information is below:\n",
    "              ---------------------\n",
    "              {context_str}\n",
    "              ---------------------\n",
    "              Given the context information above I want you to think\n",
    "              step by step to answer the query in a crisp manner,\n",
    "              incase you don't know the answer say 'I don't know!'\n",
    "            \n",
    "              Query: {query_str}\n",
    "        \n",
    "              Answer:\"\"\"\n",
    "\n",
    "qa_prompt_tmpl = PromptTemplate(template)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7c6c1af9",
   "metadata": {},
   "source": [
    "## 4.6 Define Reranker"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "eee93a22",
   "metadata": {},
   "source": [
    "Here, we use a cross-encoder to re-rank the document chunks. Also, we limit the output to the top 3 most relevant chunks based on the model’s scoring."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "a04ba8a9",
   "metadata": {},
   "outputs": [],
   "source": [
    "from llama_index.core.postprocessor import SentenceTransformerRerank\n",
    "\n",
    "rerank = SentenceTransformerRerank(\n",
    "    model=\"cross-encoder/ms-marco-MiniLM-L-2-v2\", \n",
    "    top_n=3\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "0739096d",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "SentenceTransformerRerank(callback_manager=<llama_index.core.callbacks.base.CallbackManager object at 0x3ecc84eb0>, model='cross-encoder/ms-marco-MiniLM-L-2-v2', top_n=3, device='mps', keep_retrieval_score=False, trust_remote_code=False)"
      ]
     },
     "execution_count": 30,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "rerank"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bb166d08",
   "metadata": {},
   "source": [
    "## 4.7 Query the index"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c51e5a18",
   "metadata": {},
   "source": [
    "It's time to query the index! Let's ask a question about the content of the documents we indexed."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "ad1dcfe2",
   "metadata": {},
   "outputs": [],
   "source": [
    "query_engine = index.as_query_engine(similarity_top_k=10, node_postprocessors=[rerank])\n",
    "\n",
    "query_engine.update_prompts(\n",
    "    {\"response_synthesizer:text_qa_template\": qa_prompt_tmpl}\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f9c03f15",
   "metadata": {},
   "source": [
    "Question:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "id": "31a1806d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Example queries\n",
    "# response = query_engine.query(\"What exactly is DSPy?\")\n",
    "# response = query_engine.query(\"How is DSPy pronounced?\")\n",
    "# response = query_engine.query(\"What is the github repo for docling?\")\n",
    "response = query_engine.query(\"What is the TTS for docling with pypdfium backend, when running on an Apple M3 Max using 4 threads for the test dataset of 225 pages?\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6c0924bb",
   "metadata": {},
   "source": [
    "Answer:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "id": "2d1a01d1",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/markdown": [
       "According to Table 1 in the document, the TTS (time-to-solution) for Docling with the pypdfium backend, when running on an Apple M3 Max using 4 threads for the test dataset of 225 pages is 103 seconds."
      ],
      "text/plain": [
       "<IPython.core.display.Markdown object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "from IPython.display import Markdown, display\n",
    "\n",
    "display(Markdown(str(response)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "id": "8275c694",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'37adfe9d-ffb7-46f4-9a1e-f231866acee8': {'file_path': '/Users/fc/experiments/rag-project/processed_docs/docling.md',\n",
       "  'file_name': 'docling.md',\n",
       "  'file_size': 49283,\n",
       "  'creation_date': '2025-07-22',\n",
       "  'last_modified_date': '2025-07-22'},\n",
       " 'd5f9a120-de15-4d3a-939f-0a0e40b9c2d8': {'file_path': '/Users/fc/experiments/rag-project/processed_docs/docling.md',\n",
       "  'file_name': 'docling.md',\n",
       "  'file_size': 49283,\n",
       "  'creation_date': '2025-07-22',\n",
       "  'last_modified_date': '2025-07-22'},\n",
       " '6de32255-ea98-4f3a-8f4d-0196f693f4ba': {'file_path': '/Users/fc/experiments/rag-project/processed_docs/docling.md',\n",
       "  'file_name': 'docling.md',\n",
       "  'file_size': 49283,\n",
       "  'creation_date': '2025-07-22',\n",
       "  'last_modified_date': '2025-07-22'}}"
      ]
     },
     "execution_count": 57,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "response.metadata"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "41bfbad3",
   "metadata": {},
   "source": [
    "## Bonus: visualize relevant text in documents"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f742917e",
   "metadata": {},
   "source": [
    "We are now defining an helper function to visualize a query text in the documents under RAG.\n",
    "\n",
    "This can help us understand how the model is interpreting the content and which (parts of) the documents are being exploited in the LLM response."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "id": "479a1b99",
   "metadata": {},
   "outputs": [],
   "source": [
    "from IPython.display import Markdown, display\n",
    "import re\n",
    "\n",
    "def highlight(text, query, color):\n",
    "        # Case-insensitive highlight\n",
    "        pattern = re.compile(re.escape(query), re.IGNORECASE)\n",
    "        return pattern.sub(f\"<mark style='background-color:{color};'>{query}</mark>\", text)\n",
    "\n",
    "def display_sources_with_highlight(response, docs, query, highlight_color=\"#ffff00\"):\n",
    "    \"\"\"\n",
    "    Display source documents for the response, highlighting the query in the text.\n",
    "    Avoid displaying the same document multiple times.\n",
    "    \"\"\"\n",
    "    shown = set()\n",
    "    for source in response.metadata.values():\n",
    "        source_document = source.get(\"file_name\")\n",
    "        if source_document and source_document not in shown:\n",
    "            d = next((doc for doc in docs if doc.metadata.get(\"file_name\") == source_document), None)\n",
    "            if d:\n",
    "                highlighted = highlight(d.text, query, highlight_color)\n",
    "                display(Markdown(f\"# ==================== \\n\\n{highlighted}\\n\\n\"))\n",
    "                shown.add(source_document)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "41fbf0a2",
   "metadata": {},
   "source": [
    "Usage:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d9c42196",
   "metadata": {},
   "outputs": [],
   "source": [
    "word = \"TTS\"\n",
    "display_sources_with_highlight(response, docs, word)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
